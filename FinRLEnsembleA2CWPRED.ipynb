{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrds in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.26.4)\n",
      "Requirement already satisfied: packaging<23.3 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (23.2)\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.2.2)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.9.9)\n",
      "Requirement already satisfied: scipy<1.13,>=1.12 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.0.29)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (4.2.1)\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n"
     ]
    }
   ],
   "source": [
    "# ## install finrl library\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent, DRLEnsembleAgentA2C\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_5_TICKER = [\n",
    "    \"AXP\",\n",
    "    \"AMGN\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "    \"CAT\",\n",
    "]\n",
    "INDEX_5_TICKER = [\n",
    "    \"^DJI\", \n",
    "    \"^IXIC\", \n",
    "    \"^NYA\", \n",
    "    \"^RUT\", \n",
    "    \"^GSPC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "\n",
    "# # TRAIN_START_DATE = '2009-04-01'\n",
    "# # TRAIN_END_DATE = '2021-01-01'\n",
    "# # TEST_START_DATE = '2021-01-01'\n",
    "# # TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "\n",
    "# TRAIN_START_DATE = '2009-06-01'\n",
    "# #TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "# dfexport = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = DOW_30_TICKER).fetch_data()\n",
    "\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfexport.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data export\n",
    "# import pickle\n",
    "# datasetName = \"dailydata\"\n",
    "# datasetDir = \"./datasets\"\n",
    "\n",
    "# os.makedirs(datasetDir, exist_ok=True)\n",
    "# datasetPath = os.path.join(datasetDir, datasetName) + \".pkl\"\n",
    "\n",
    "\n",
    "# with open(datasetPath, 'wb') as file:\n",
    "#     pickle.dump(dfexport, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (16555, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "#TRAIN_START_DATE = '2000-01-01'\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2017-10-01'\n",
    "TEST_START_DATE = '2017-10-01'\n",
    "TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = INDEX_5_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Andrew Martin - UNCOMMENT BELOW TO ADD PREDICTION INDICATOR\n",
    "# import pickle\n",
    "# with open(\"./datasets/index_5_predictor_2.pkl\", 'rb') as file:\n",
    "#   df_prob = pickle.load(file)\n",
    "# df6 = df_prob.copy()\n",
    "# df6 = df6.loc[:, ~df6.columns.duplicated(keep='first')]\n",
    "# df6[\"date\"] = df6[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "# df2 = processed.merge(df6[['tic', 'date', 'Predicted_Target']], on=['tic', 'date'], how='left')\n",
    "# processed = df2.copy()\n",
    "# INDICATORS.append(\"Predicted_Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 5, State Space: 51\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgentA2C(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2017-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 347          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.13        |\n",
      "|    explained_variance | 0.00659      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -9.21        |\n",
      "|    reward             | -0.120393455 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 2.47         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 347       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.975    |\n",
      "|    reward             | 0.6864112 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.812     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 331        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.29      |\n",
      "|    reward             | -1.5019037 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 335        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 9.07       |\n",
      "|    reward             | 0.58748484 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 336        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -2.39      |\n",
      "|    reward             | 0.18395512 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.229      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 336         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | -0.0995     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 5.38        |\n",
      "|    reward             | -0.21655715 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.763       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -1.73      |\n",
      "|    reward             | 0.81580395 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.459      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -34.9     |\n",
      "|    reward             | 3.5856323 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 32.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 340       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -15.5     |\n",
      "|    reward             | 1.9903429 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.9       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 9.72      |\n",
      "|    reward             | 0.6748313 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.65      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 342        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -14.3      |\n",
      "|    reward             | -0.5688653 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.53       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 342        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 5.19       |\n",
      "|    reward             | 0.72910357 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 343        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -17.2      |\n",
      "|    reward             | -0.7622614 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 14.7      |\n",
      "|    reward             | 0.8564676 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 5.3       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 344         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 9.48        |\n",
      "|    reward             | -0.10627148 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.22        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 344        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 4.49       |\n",
      "|    reward             | -1.8958486 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.664      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 345         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 5.57        |\n",
      "|    reward             | -0.09956161 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.715       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 346        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | -3.6802657 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 7.5        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 348         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 1.77        |\n",
      "|    reward             | -0.44893828 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 2.05        |\n",
      "---------------------------------------\n",
      "day: 1949, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2553919.97\n",
      "total_reward: 1553919.97\n",
      "total_cost: 62977.99\n",
      "total_trades: 6149\n",
      "Sharpe: 0.808\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 349         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 5.32        |\n",
      "|    reward             | -0.16550295 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 0.937       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2017-10-02 to  2018-01-02\n",
      "A2C Sharpe Ratio:  0.33120497645382474\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_10\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 510        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | -0.3405752 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 322        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 12         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 9.203693   |\n",
      "|    clip_fraction        | 0.99       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -12.6      |\n",
      "|    explained_variance   | -0.0877    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.24       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.297      |\n",
      "|    reward               | 0.63798374 |\n",
      "|    std                  | 14.2       |\n",
      "|    value_loss           | 5.35       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034627873 |\n",
      "|    clip_fraction        | 0.354       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.3       |\n",
      "|    explained_variance   | -0.00516    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.886       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0241      |\n",
      "|    reward               | -0.5518644  |\n",
      "|    std                  | 14.3        |\n",
      "|    value_loss           | 3.44        |\n",
      "-----------------------------------------\n",
      "day: 1949, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 596013.65\n",
      "total_reward: -403986.35\n",
      "total_cost: 824883.76\n",
      "total_trades: 7192\n",
      "Sharpe: -0.337\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 281         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031440783 |\n",
      "|    clip_fraction        | 0.338       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.4       |\n",
      "|    explained_variance   | 9.97e-05    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.67        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0129      |\n",
      "|    reward               | -0.83017695 |\n",
      "|    std                  | 14.2        |\n",
      "|    value_loss           | 3.23        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032335155 |\n",
      "|    clip_fraction        | 0.325       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.3       |\n",
      "|    explained_variance   | -6.37e-05   |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0205      |\n",
      "|    reward               | -1.1088924  |\n",
      "|    std                  | 14.3        |\n",
      "|    value_loss           | 3.39        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2017-10-02 to  2018-01-02\n",
      "PPO Sharpe Ratio:  -0.031368025201507076\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_8\n",
      "day: 1949, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2128598.69\n",
      "total_reward: 1128598.69\n",
      "total_cost: 998.59\n",
      "total_trades: 3898\n",
      "Sharpe: 0.761\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 186        |\n",
      "|    time_elapsed    | 41         |\n",
      "|    total_timesteps | 7800       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 321        |\n",
      "|    critic_loss     | 5.86       |\n",
      "|    learning_rate   | 0.01       |\n",
      "|    n_updates       | 7699       |\n",
      "|    reward          | 0.23197193 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2017-10-02 to  2018-01-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-01-02\n",
      "======Trading from:  2018-01-02 to  2018-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-01-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_8\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -0.0203    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -6.61      |\n",
      "|    reward             | 0.14794135 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.93     |\n",
      "|    reward             | 0.5318524 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 0.737     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 344        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -5.42      |\n",
      "|    reward             | -1.6431407 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 347        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.6       |\n",
      "|    reward             | 0.19057433 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 0.382      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 347        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | -0.0474    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 37.1       |\n",
      "|    reward             | 0.11513885 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 30.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 347         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 14.3        |\n",
      "|    reward             | 0.030807897 |\n",
      "|    std                | 0.981       |\n",
      "|    value_loss         | 5.04        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 347         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 24.4        |\n",
      "|    reward             | -0.73081946 |\n",
      "|    std                | 0.982       |\n",
      "|    value_loss         | 15.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 344        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | -0.587     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -13.4      |\n",
      "|    reward             | 0.04789392 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 4.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -19.2     |\n",
      "|    reward             | -2.185103 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 10.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 341       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -22.6     |\n",
      "|    reward             | 0.2904238 |\n",
      "|    std                | 0.974     |\n",
      "|    value_loss         | 11        |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 337         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.92       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 18.1        |\n",
      "|    reward             | -0.44390002 |\n",
      "|    std                | 0.966       |\n",
      "|    value_loss         | 8.27        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 336      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.89    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 30.3     |\n",
      "|    reward             | 2.048018 |\n",
      "|    std                | 0.96     |\n",
      "|    value_loss         | 20.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.89     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 23.6      |\n",
      "|    reward             | -2.481104 |\n",
      "|    std                | 0.961     |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 336         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.88       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 16.8        |\n",
      "|    reward             | -0.13732155 |\n",
      "|    std                | 0.958       |\n",
      "|    value_loss         | 10.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.86     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 15.9      |\n",
      "|    reward             | 3.3979688 |\n",
      "|    std                | 0.955     |\n",
      "|    value_loss         | 5.35      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 336          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 23           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.86        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | 3.37         |\n",
      "|    reward             | -0.014927381 |\n",
      "|    std                | 0.954        |\n",
      "|    value_loss         | 1.27         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 336        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.88      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 8.3        |\n",
      "|    reward             | 0.94508886 |\n",
      "|    std                | 0.958      |\n",
      "|    value_loss         | 8.11       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 337         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.88       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 6.45        |\n",
      "|    reward             | -0.74205714 |\n",
      "|    std                | 0.957       |\n",
      "|    value_loss         | 1.46        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.85     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -58       |\n",
      "|    reward             | 3.2027912 |\n",
      "|    std                | 0.953     |\n",
      "|    value_loss         | 90.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 337        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.88      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -11.3      |\n",
      "|    reward             | 0.04141084 |\n",
      "|    std                | 0.957      |\n",
      "|    value_loss         | 2.27       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-01-02 to  2018-04-04\n",
      "A2C Sharpe Ratio:  -0.03335984988545557\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_8\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 384       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.7282878 |\n",
      "----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 253        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 10.78769   |\n",
      "|    clip_fraction        | 0.993      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -14        |\n",
      "|    explained_variance   | -0.0291    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.95       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.298      |\n",
      "|    reward               | 0.19895428 |\n",
      "|    std                  | 24.3       |\n",
      "|    value_loss           | 4.79       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021859776 |\n",
      "|    clip_fraction        | 0.286       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.6       |\n",
      "|    explained_variance   | 0.000375    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0173      |\n",
      "|    reward               | -3.7161512  |\n",
      "|    std                  | 23.9        |\n",
      "|    value_loss           | 3.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 219         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023667723 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.6       |\n",
      "|    explained_variance   | -0.00779    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.618       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0111      |\n",
      "|    reward               | 1.1058555   |\n",
      "|    std                  | 24.1        |\n",
      "|    value_loss           | 3.44        |\n",
      "-----------------------------------------\n",
      "day: 2012, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 549596.54\n",
      "total_reward: -450403.46\n",
      "total_cost: 1102481.53\n",
      "total_trades: 7521\n",
      "Sharpe: -0.437\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025080297 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.6       |\n",
      "|    explained_variance   | 0.000942    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.966       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0123      |\n",
      "|    reward               | -0.13831711 |\n",
      "|    std                  | 24.1        |\n",
      "|    value_loss           | 3.56        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-01-02 to  2018-04-04\n",
      "PPO Sharpe Ratio:  -0.06847602947584366\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_8\n",
      "day: 2012, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1902133.69\n",
      "total_reward: 902133.69\n",
      "total_cost: 998.41\n",
      "total_trades: 6036\n",
      "Sharpe: 0.619\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 135        |\n",
      "|    time_elapsed    | 59         |\n",
      "|    total_timesteps | 8052       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 76.6       |\n",
      "|    critic_loss     | 0.281      |\n",
      "|    learning_rate   | 0.01       |\n",
      "|    n_updates       | 7951       |\n",
      "|    reward          | -0.7396157 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-01-02 to  2018-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-04-04\n",
      "======Trading from:  2018-04-04 to  2018-07-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_8\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 214        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -13.4      |\n",
      "|    reward             | 0.49450755 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 4.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 217       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 2.52      |\n",
      "|    reward             | 1.611228  |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 3.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 221        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -0.0153    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -16.4      |\n",
      "|    reward             | -1.9537466 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 9.09       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 222         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -27.1       |\n",
      "|    reward             | -0.52860785 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 11          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 225        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 24.1       |\n",
      "|    reward             | -2.1003504 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 20         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.315     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -10.9      |\n",
      "|    reward             | 0.39993098 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.34       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 231       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -110      |\n",
      "|    reward             | -1.58257  |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 258       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 233         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -23.6       |\n",
      "|    reward             | -0.26599175 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 13.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 234       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -8.47     |\n",
      "|    reward             | -0.865225 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 235       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 8.88      |\n",
      "|    reward             | 1.4261006 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.76      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 236         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -15.5       |\n",
      "|    reward             | -0.06106904 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.45        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 237         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 0.481       |\n",
      "|    reward             | 0.010011914 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.215       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 238       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.742    |\n",
      "|    reward             | 1.9488057 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.529     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 238        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 5.88       |\n",
      "|    reward             | 0.63478595 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.89       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 237         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -23.7       |\n",
      "|    reward             | -0.71555185 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 15.8        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 236      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -2.67    |\n",
      "|    reward             | 0.3104   |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.263    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 235        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 5.61       |\n",
      "|    reward             | 0.73303616 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.17       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 234         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -8.84       |\n",
      "|    reward             | -0.07338462 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.09        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 234       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -7.89     |\n",
      "|    reward             | 1.9065553 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.13      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 233       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 12.9      |\n",
      "|    reward             | 0.9987282 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.56      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2018-04-04 to  2018-07-03\n",
      "A2C Sharpe Ratio:  0.22071149706616902\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_8\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 311        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 6          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.16265677 |\n",
      "-----------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 195       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 20        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 7.127221  |\n",
      "|    clip_fraction        | 0.985     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -10.9     |\n",
      "|    explained_variance   | -0.114    |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 1.03      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.315     |\n",
      "|    reward               | 3.9729266 |\n",
      "|    std                  | 6.11      |\n",
      "|    value_loss           | 3.63      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 172        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 35         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05633961 |\n",
      "|    clip_fraction        | 0.563      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -16.2      |\n",
      "|    explained_variance   | -0.000171  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 9.53       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.0663     |\n",
      "|    reward               | 0.24876338 |\n",
      "|    std                  | 6.26       |\n",
      "|    value_loss           | 18.7       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 161        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 50         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07109901 |\n",
      "|    clip_fraction        | 0.497      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -16.3      |\n",
      "|    explained_variance   | -0.000959  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 12.3       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0478     |\n",
      "|    reward               | 0.902012   |\n",
      "|    std                  | 6.26       |\n",
      "|    value_loss           | 17.1       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 66         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07149218 |\n",
      "|    clip_fraction        | 0.544      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -16.4      |\n",
      "|    explained_variance   | -0.00131   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 15.2       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0537     |\n",
      "|    reward               | 0.74316794 |\n",
      "|    std                  | 6.62       |\n",
      "|    value_loss           | 15.8       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2018-04-04 to  2018-07-03\n",
      "PPO Sharpe Ratio:  0.24163551971740613\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_8\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 134       |\n",
      "|    time_elapsed    | 61        |\n",
      "|    total_timesteps | 8304      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.09e+03 |\n",
      "|    critic_loss     | 152       |\n",
      "|    learning_rate   | 0.01      |\n",
      "|    n_updates       | 8203      |\n",
      "|    reward          | 3.0293658 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2018-04-04 to  2018-07-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-07-03\n",
      "======Trading from:  2018-07-03 to  2018-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-07-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_8\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 227           |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -7.17         |\n",
      "|    explained_variance | -0.0187       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -4.35         |\n",
      "|    reward             | 2.8047853e-06 |\n",
      "|    std                | 1.01          |\n",
      "|    value_loss         | 0.777         |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.19     |\n",
      "|    explained_variance | -0.0347   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.448    |\n",
      "|    reward             | 0.5444592 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 226        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.54      |\n",
      "|    reward             | -1.8855442 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2.05      |\n",
      "|    reward             | 0.6106655 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -10.2     |\n",
      "|    reward             | 0.7984099 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.19     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 0.0556    |\n",
      "|    reward             | 1.1869601 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.158     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -18.5       |\n",
      "|    reward             | -0.18487383 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.9         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 227       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -13.8     |\n",
      "|    reward             | 0.9195631 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 6.84      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.19     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.63     |\n",
      "|    reward             | 1.5883595 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 228        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -17.8      |\n",
      "|    reward             | -1.6507714 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 14.2       |\n",
      "|    reward             | 0.54713905 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 228        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -7.67      |\n",
      "|    reward             | -0.7615382 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -20.5      |\n",
      "|    reward             | -2.4441743 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 228        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -3.98      |\n",
      "|    reward             | 0.79244035 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.881      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 0.2743415 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 229       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 5.94      |\n",
      "|    reward             | 0.5497427 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 229         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -16.6       |\n",
      "|    reward             | -0.65364504 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 229       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 5.01      |\n",
      "|    reward             | 1.8091451 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.06      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | -0.00973   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -7.22      |\n",
      "|    reward             | -1.2870716 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -24.4      |\n",
      "|    reward             | 0.19928339 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.4       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-07-03 to  2018-10-02\n",
      "A2C Sharpe Ratio:  0.37277149731375814\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_8\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 306          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 6            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.072130375 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 188         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.290756    |\n",
      "|    clip_fraction        | 0.987       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.9       |\n",
      "|    explained_variance   | -0.0296     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 3.05        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.294       |\n",
      "|    reward               | 0.028758323 |\n",
      "|    std                  | 11          |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035494875 |\n",
      "|    clip_fraction        | 0.428       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.7       |\n",
      "|    explained_variance   | 0.0019      |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.478       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0344      |\n",
      "|    reward               | 0.23725723  |\n",
      "|    std                  | 10.7        |\n",
      "|    value_loss           | 5.3         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 155         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.042320304 |\n",
      "|    clip_fraction        | 0.39        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.7       |\n",
      "|    explained_variance   | -0.00101    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0268      |\n",
      "|    reward               | -0.34375378 |\n",
      "|    std                  | 11          |\n",
      "|    value_loss           | 8.23        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 150        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 67         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04637553 |\n",
      "|    clip_fraction        | 0.416      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -18.8      |\n",
      "|    explained_variance   | -0.000783  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 3.11       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0281     |\n",
      "|    reward               | 1.4825817  |\n",
      "|    std                  | 11.1       |\n",
      "|    value_loss           | 7.81       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2018-07-03 to  2018-10-02\n",
      "PPO Sharpe Ratio:  -0.00184409908201683\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_8\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    episodes        | 4            |\n",
      "|    fps             | 138          |\n",
      "|    time_elapsed    | 61           |\n",
      "|    total_timesteps | 8556         |\n",
      "| train/             |              |\n",
      "|    actor_loss      | -415         |\n",
      "|    critic_loss     | 1.42e+03     |\n",
      "|    learning_rate   | 0.01         |\n",
      "|    n_updates       | 8455         |\n",
      "|    reward          | 0.0026650878 |\n",
      "-------------------------------------\n",
      "======DDPG Validation from:  2018-07-03 to  2018-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-10-02\n",
      "======Trading from:  2018-10-02 to  2019-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_378_8\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 240         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.15       |\n",
      "|    explained_variance | -0.345      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -12.8       |\n",
      "|    reward             | -0.09646194 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.71        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 234       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.597    |\n",
      "|    reward             | 0.5515357 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.844     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 232        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -5.25      |\n",
      "|    reward             | -1.4499495 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 233        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.16      |\n",
      "|    reward             | 0.27491307 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.237      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 234        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 8.64       |\n",
      "|    reward             | -2.4604502 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 231        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 3.03       |\n",
      "|    reward             | 0.21824348 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.14       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 231         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | -0.168      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 2.64        |\n",
      "|    reward             | -0.67911166 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.225       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 232         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 5.49        |\n",
      "|    reward             | -0.16454618 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 232       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0.00812   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.3      |\n",
      "|    reward             | 1.1323644 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.16      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 230         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -23.4       |\n",
      "|    reward             | -0.41454396 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.000243   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -18.8      |\n",
      "|    reward             | -1.5360297 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.111      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -11.2      |\n",
      "|    reward             | 0.39338964 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -0.0417    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -17.6      |\n",
      "|    reward             | 0.50014454 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.74       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 3.65       |\n",
      "|    reward             | -2.4138653 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 0.908      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 3.34       |\n",
      "|    reward             | -0.4198682 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.461      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0.053       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 16.7        |\n",
      "|    reward             | -0.17972207 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.96        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | 0.32430828 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.4        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 226          |\n",
      "|    iterations         | 1800         |\n",
      "|    time_elapsed       | 39           |\n",
      "|    total_timesteps    | 9000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.09        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1799         |\n",
      "|    policy_loss        | -1.11        |\n",
      "|    reward             | -0.071538694 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 0.588        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 224        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -4.45      |\n",
      "|    reward             | 0.21432689 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 224        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -0.936     |\n",
      "|    reward             | -0.3863871 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.602      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-10-02 to  2019-01-03\n",
      "A2C Sharpe Ratio:  -0.28941131015405575\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_378_8\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 287         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.021145178 |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 178       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 22        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.9063005 |\n",
      "|    clip_fraction        | 0.977     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -8.65     |\n",
      "|    explained_variance   | -0.0959   |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 3.25      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.249     |\n",
      "|    reward               | 0.200735  |\n",
      "|    std                  | 1.6       |\n",
      "|    value_loss           | 3.69      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.8845863   |\n",
      "|    clip_fraction        | 0.995       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.2       |\n",
      "|    explained_variance   | 0.000221    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 5.51        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.299       |\n",
      "|    reward               | -0.73676896 |\n",
      "|    std                  | 15.6        |\n",
      "|    value_loss           | 17          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 152         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.039198965 |\n",
      "|    clip_fraction        | 0.359       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | -0.00294    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 7.36        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0229      |\n",
      "|    reward               | 1.1965309   |\n",
      "|    std                  | 15.8        |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030445147 |\n",
      "|    clip_fraction        | 0.326       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -20.7       |\n",
      "|    explained_variance   | 0.000507    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.8         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0206      |\n",
      "|    reward               | 0.63674337  |\n",
      "|    std                  | 15.5        |\n",
      "|    value_loss           | 14.5        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-10-02 to  2019-01-03\n",
      "PPO Sharpe Ratio:  -0.35911742562027305\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_8\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 139        |\n",
      "|    time_elapsed    | 63         |\n",
      "|    total_timesteps | 8808       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -646       |\n",
      "|    critic_loss     | 40.1       |\n",
      "|    learning_rate   | 0.01       |\n",
      "|    n_updates       | 8707       |\n",
      "|    reward          | 0.62536705 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-10-02 to  2019-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-01-03\n",
      "======Trading from:  2019-01-03 to  2019-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_441_8\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 222        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0.232      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -9.17      |\n",
      "|    reward             | -0.1185565 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 2.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.0053    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.269      |\n",
      "|    reward             | 0.50350267 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.768      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 231        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0.0507     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.65      |\n",
      "|    reward             | -1.5272661 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 230       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -12.6     |\n",
      "|    reward             | 0.4871359 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 11.3      |\n",
      "|    reward             | 0.1366709 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.71      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 223         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.114      |\n",
      "|    reward             | -0.18896505 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.489       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 222        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 2.06       |\n",
      "|    reward             | -0.6351658 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.205      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.0194     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 4          |\n",
      "|    reward             | 0.36462158 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.531      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | -0.0349    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 4.44       |\n",
      "|    reward             | 0.60608584 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 220       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.176     |\n",
      "|    reward             | 2.3577724 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.25      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 220       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.618    |\n",
      "|    reward             | 1.4934182 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.194     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 219        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.0289    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 13         |\n",
      "|    reward             | -0.4447566 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 219        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 8.66       |\n",
      "|    reward             | -0.7441825 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 218        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -8.68      |\n",
      "|    reward             | 0.05048782 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.53       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 218          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 34           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.1         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | -14.9        |\n",
      "|    reward             | -0.076766156 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 5.07         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 218       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -16.8     |\n",
      "|    reward             | -0.230273 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.52      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 218        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -6.8       |\n",
      "|    reward             | -1.8836868 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 217        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -6.6       |\n",
      "|    reward             | -1.6960391 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 217        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -29.3      |\n",
      "|    reward             | -2.2701118 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 28.5       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 217      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -7.39    |\n",
      "|    reward             | -0.65856 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.04     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2019-01-03 to  2019-04-04\n",
      "A2C Sharpe Ratio:  0.6046302283951033\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_441_8\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 301          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 6            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.039102487 |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 197        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 5.1920223  |\n",
      "|    clip_fraction        | 0.987      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -12        |\n",
      "|    explained_variance   | -0.0317    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 3.29       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.301      |\n",
      "|    reward               | -0.7672539 |\n",
      "|    std                  | 5.98       |\n",
      "|    value_loss           | 3.19       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 175         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.083513245 |\n",
      "|    clip_fraction        | 0.558       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16         |\n",
      "|    explained_variance   | 0.000213    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0634      |\n",
      "|    reward               | 0.00967835  |\n",
      "|    std                  | 6.11        |\n",
      "|    value_loss           | 5.97        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 174         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04998701  |\n",
      "|    clip_fraction        | 0.504       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.2       |\n",
      "|    explained_variance   | -0.00767    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.14        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0463      |\n",
      "|    reward               | -0.70607364 |\n",
      "|    std                  | 6.03        |\n",
      "|    value_loss           | 4.09        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 176        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 58         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.07662887 |\n",
      "|    clip_fraction        | 0.521      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -16.2      |\n",
      "|    explained_variance   | -0.000106  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 2.06       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.049      |\n",
      "|    reward               | 0.05738245 |\n",
      "|    std                  | 6.12       |\n",
      "|    value_loss           | 3.45       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2019-01-03 to  2019-04-04\n",
      "PPO Sharpe Ratio:  0.4821036350926581\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_8\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 164       |\n",
      "|    time_elapsed    | 54        |\n",
      "|    total_timesteps | 9060      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 175       |\n",
      "|    critic_loss     | 250       |\n",
      "|    learning_rate   | 0.01      |\n",
      "|    n_updates       | 8959      |\n",
      "|    reward          | 0.1243125 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-01-03 to  2019-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-04-04\n",
      "======Trading from:  2019-04-04 to  2019-07-05\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_504_8\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 269          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.09        |\n",
      "|    explained_variance | 0.286        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -10.1        |\n",
      "|    reward             | -0.006185161 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 3.02         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 275        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.6       |\n",
      "|    reward             | 0.66007805 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 277        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.8       |\n",
      "|    reward             | -2.0873353 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 278        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -6.05      |\n",
      "|    reward             | 0.23734297 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 0.848      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 276        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.379      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 12.4       |\n",
      "|    reward             | 0.29907137 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.34       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 274       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -3.94     |\n",
      "|    reward             | 0.5664798 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.665     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 277       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 2.69      |\n",
      "|    reward             | 1.1229705 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.968     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 279        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -4.2       |\n",
      "|    reward             | -1.4959953 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.951      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 278       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0.0121    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -12.5     |\n",
      "|    reward             | 0.6141276 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.34      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 279         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0.00893     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -3.63       |\n",
      "|    reward             | -0.39944425 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.937       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 282          |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 19           |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.14        |\n",
      "|    explained_variance | 0.0328       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | 4.65         |\n",
      "|    reward             | -0.056457423 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.695        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 282        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | 0.19859652 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.2        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 9.18       |\n",
      "|    reward             | 0.13989581 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 286        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0.123      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -7.85      |\n",
      "|    reward             | 0.31001243 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | -0.0255    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 3.8        |\n",
      "|    reward             | 0.13273375 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.677      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 286          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 27           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.13        |\n",
      "|    explained_variance | -0.0138      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -0.246       |\n",
      "|    reward             | -0.119655155 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.0563       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 288        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -0.0109    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -14.6      |\n",
      "|    reward             | 0.64447993 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 289       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0.0741    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.57      |\n",
      "|    reward             | 0.9869707 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.365     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 289       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 8.39      |\n",
      "|    reward             | 0.7458285 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 288        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 2.18       |\n",
      "|    reward             | -1.1121985 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-04-04 to  2019-07-05\n",
      "A2C Sharpe Ratio:  0.1268402078739833\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_504_8\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 370        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 5          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.08918706 |\n",
      "-----------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 235        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 17         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 10.951534  |\n",
      "|    clip_fraction        | 0.974      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.67      |\n",
      "|    explained_variance   | -0.078     |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 3.9        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.411      |\n",
      "|    reward               | 0.36744374 |\n",
      "|    std                  | 1.54       |\n",
      "|    value_loss           | 3.18       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 210       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 29        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 10.719083 |\n",
      "|    clip_fraction        | 0.986     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13       |\n",
      "|    explained_variance   | -1.43e-05 |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 7.93      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.33      |\n",
      "|    reward               | 1.5543281 |\n",
      "|    std                  | 7.67      |\n",
      "|    value_loss           | 19.4      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.060607374 |\n",
      "|    clip_fraction        | 0.534       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.000253    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 5.84        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0544      |\n",
      "|    reward               | 1.9396675   |\n",
      "|    std                  | 7.79        |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 193        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 53         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06331804 |\n",
      "|    clip_fraction        | 0.507      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17.1      |\n",
      "|    explained_variance   | -0.000526  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 3.8        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.046      |\n",
      "|    reward               | 0.75887644 |\n",
      "|    std                  | 7.93       |\n",
      "|    value_loss           | 12.4       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2019-04-04 to  2019-07-05\n",
      "PPO Sharpe Ratio:  0.08538538013361091\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_9\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 161       |\n",
      "|    time_elapsed    | 57        |\n",
      "|    total_timesteps | 9312      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 0.847     |\n",
      "|    critic_loss     | 0.906     |\n",
      "|    learning_rate   | 0.01      |\n",
      "|    n_updates       | 9211      |\n",
      "|    reward          | 0.6305978 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-04-04 to  2019-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-07-05\n",
      "======Trading from:  2019-07-05 to  2019-10-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_567_9\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 301        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.1      |\n",
      "|    reward             | 0.00903418 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 2.68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 314        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.98      |\n",
      "|    reward             | 0.79718816 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 319        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -11.1      |\n",
      "|    reward             | -2.5224664 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 3.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 321       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -5.59     |\n",
      "|    reward             | 0.3333175 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.941     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 314       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 9.63      |\n",
      "|    reward             | 2.7329628 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 3.18      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 309         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 0.313       |\n",
      "|    reward             | 0.012153272 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.526       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 7.38       |\n",
      "|    reward             | 0.12082211 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 302       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -9.22     |\n",
      "|    reward             | 1.2496223 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 15        |\n",
      "|    reward             | 1.7294554 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 14.3       |\n",
      "|    reward             | -1.2495828 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 297        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 7.9        |\n",
      "|    reward             | -1.5643908 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 28.6       |\n",
      "|    reward             | -0.6226318 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 22.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 5.4        |\n",
      "|    reward             | -1.0539727 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 292        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 39.2       |\n",
      "|    reward             | -13.371701 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 62.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -7.39     |\n",
      "|    reward             | 2.5047657 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 2.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 290       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 20.6      |\n",
      "|    reward             | -1.666477 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 6.94      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 290         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 0.931       |\n",
      "|    reward             | -0.29702726 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 1.19        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 290       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 10.3      |\n",
      "|    reward             | 0.6521295 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 3.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 288        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -10.3      |\n",
      "|    reward             | 0.33173555 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 7.39       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 288       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.19     |\n",
      "|    reward             | 2.3222988 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 2.09      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2019-07-05 to  2019-10-03\n",
      "A2C Sharpe Ratio:  -0.16783229446833836\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_567_9\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 387        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 5          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.04953202 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 240         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 11.360136   |\n",
      "|    clip_fraction        | 0.995       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -13.6       |\n",
      "|    explained_variance   | -0.0474     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.302       |\n",
      "|    reward               | -0.58936137 |\n",
      "|    std                  | 28          |\n",
      "|    value_loss           | 4.39        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 210         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014523269 |\n",
      "|    clip_fraction        | 0.297       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.3       |\n",
      "|    explained_variance   | -0.00047    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0149      |\n",
      "|    reward               | 0.18677223  |\n",
      "|    std                  | 28.1        |\n",
      "|    value_loss           | 3.39        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 202          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.015766408  |\n",
      "|    clip_fraction        | 0.245        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -23.3        |\n",
      "|    explained_variance   | -0.000107    |\n",
      "|    learning_rate        | 0.01         |\n",
      "|    loss                 | 1.67         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | 0.0122       |\n",
      "|    reward               | -0.060477484 |\n",
      "|    std                  | 28.4         |\n",
      "|    value_loss           | 3.8          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 195         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025779773 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -23.3       |\n",
      "|    explained_variance   | 0.000517    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.516       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.00841     |\n",
      "|    reward               | 0.17595704  |\n",
      "|    std                  | 27.8        |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-07-05 to  2019-10-03\n",
      "PPO Sharpe Ratio:  -0.26332521453653845\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_9\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 152       |\n",
      "|    time_elapsed    | 62        |\n",
      "|    total_timesteps | 9564      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -64.8     |\n",
      "|    critic_loss     | 6.79      |\n",
      "|    learning_rate   | 0.01      |\n",
      "|    n_updates       | 9463      |\n",
      "|    reward          | 2.1583161 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-07-05 to  2019-10-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-10-03\n",
      "======Trading from:  2019-10-03 to  2020-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-10-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_630_9\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 271       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -0.319    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -10.1     |\n",
      "|    reward             | 0.4670661 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.73      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 262      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 5.86     |\n",
      "|    reward             | 1.557675 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.48     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 269        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -16        |\n",
      "|    reward             | -1.9190079 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 267        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -0.0961    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -22.1      |\n",
      "|    reward             | -0.5207959 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 267        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.021      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 12.5       |\n",
      "|    reward             | 0.35082462 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 268        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 0.0964     |\n",
      "|    reward             | 0.18598227 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.512      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 266        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.0612    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 14.4       |\n",
      "|    reward             | -1.0829782 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.82       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 267       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0.0394    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 20.4      |\n",
      "|    reward             | 1.1949857 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 16.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 267        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -0.342     |\n",
      "|    reward             | -1.0818063 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 269       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0.0245    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.678     |\n",
      "|    reward             | 0.1558457 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 269         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.148      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 1.2         |\n",
      "|    reward             | -0.47341815 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.305       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 271         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 0.49        |\n",
      "|    reward             | -0.52950794 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.58        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 271       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -13.7     |\n",
      "|    reward             | 1.7117207 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.81      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 270      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 4.72     |\n",
      "|    reward             | 4.600352 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 10.7     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 271      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 8.94     |\n",
      "|    reward             | 2.532192 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.82     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 272         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -10.4       |\n",
      "|    reward             | -0.49809727 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 272       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.246     |\n",
      "|    reward             | 1.0769625 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.7       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 270       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 10        |\n",
      "|    reward             | 0.7624758 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 25.5      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 268      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 35       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -63      |\n",
      "|    reward             | 2.479254 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 79.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 266        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | -0.4960549 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.26       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-10-03 to  2020-01-03\n",
      "A2C Sharpe Ratio:  0.7351561589484097\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_630_9\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 340        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 6          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.24054472 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 14.605696   |\n",
      "|    clip_fraction        | 0.995       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.3       |\n",
      "|    explained_variance   | -0.123      |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.294       |\n",
      "|    reward               | -0.22482021 |\n",
      "|    std                  | 53.4        |\n",
      "|    value_loss           | 3.22        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011971088 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.5       |\n",
      "|    explained_variance   | -0.000502   |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.00627     |\n",
      "|    reward               | -0.52114505 |\n",
      "|    std                  | 52.1        |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01552148 |\n",
      "|    clip_fraction        | 0.168      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -26.5      |\n",
      "|    explained_variance   | -0.00137   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 0.393      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0055     |\n",
      "|    reward               | 0.8928302  |\n",
      "|    std                  | 53.3       |\n",
      "|    value_loss           | 3.26       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 51          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013025972 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -26.5       |\n",
      "|    explained_variance   | 0.000192    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.829       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.00846     |\n",
      "|    reward               | -0.6482628  |\n",
      "|    std                  | 53.2        |\n",
      "|    value_loss           | 3.52        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-10-03 to  2020-01-03\n",
      "PPO Sharpe Ratio:  0.4959128502742255\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_9\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 165        |\n",
      "|    time_elapsed    | 59         |\n",
      "|    total_timesteps | 9816       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -1.85e+03  |\n",
      "|    critic_loss     | 281        |\n",
      "|    learning_rate   | 0.01       |\n",
      "|    n_updates       | 9715       |\n",
      "|    reward          | -4.4957957 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2019-10-03 to  2020-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-01-03\n",
      "======Trading from:  2020-01-03 to  2020-04-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_693_9\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 291      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 1        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | -0.00644 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -11.7    |\n",
      "|    reward             | 0.339898 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.74     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0.106     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.18      |\n",
      "|    reward             | 0.4154622 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.23      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 287        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.0759    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.8       |\n",
      "|    reward             | -1.1591042 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 3.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 282        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.28      |\n",
      "|    reward             | 0.09964226 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 0.153      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 277         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -0.0216     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -2.72       |\n",
      "|    reward             | -0.16119905 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.299       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 275          |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 10           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.09        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | -20.7        |\n",
      "|    reward             | -0.116977744 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 15.2         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 273        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0.127      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -2.73      |\n",
      "|    reward             | 0.15251592 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 0.293      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 271         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -2.1        |\n",
      "|    reward             | -0.21766533 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.34        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 271       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -3.35     |\n",
      "|    reward             | 0.771346  |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.228     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 272          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 18           |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 3.58         |\n",
      "|    reward             | -0.023903906 |\n",
      "|    std                | 0.997        |\n",
      "|    value_loss         | 0.599        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 274       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -0.0231   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -5.51     |\n",
      "|    reward             | 0.5118826 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 5.5       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 275       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 2.05      |\n",
      "|    reward             | 0.6112772 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.203     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 275        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.289     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 6.57       |\n",
      "|    reward             | -0.5346745 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 276        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -1.03      |\n",
      "|    reward             | 0.27769923 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.0442     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 276       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 12.2      |\n",
      "|    reward             | 1.0977932 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.18      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 276         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 18.5        |\n",
      "|    reward             | -0.47069415 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 277       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -8.06     |\n",
      "|    reward             | 2.6854234 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 278         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 32          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -17.1       |\n",
      "|    reward             | -0.15798905 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 10.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 278        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 1.22       |\n",
      "|    reward             | 0.40596798 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.086      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 278         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -2.2        |\n",
      "|    reward             | -0.12103457 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.15        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2020-01-03 to  2020-04-03\n",
      "A2C Sharpe Ratio:  -0.10165377264757294\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_693_9\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 388        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 5          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.22052304 |\n",
      "-----------------------------------\n",
      "day: 2516, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 650816.66\n",
      "total_reward: -349183.34\n",
      "total_cost: 1122015.31\n",
      "total_trades: 9721\n",
      "Sharpe: -0.228\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 251       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 16        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 4.0483966 |\n",
      "|    clip_fraction        | 0.976     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -10.5     |\n",
      "|    explained_variance   | -0.0438   |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 1.6       |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.323     |\n",
      "|    reward               | 1.7928234 |\n",
      "|    std                  | 3.4       |\n",
      "|    value_loss           | 3.91      |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 222          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.24034849   |\n",
      "|    clip_fraction        | 0.721        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -13.5        |\n",
      "|    explained_variance   | 3.95e-05     |\n",
      "|    learning_rate        | 0.01         |\n",
      "|    loss                 | 3.12         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | 0.125        |\n",
      "|    reward               | -0.118971445 |\n",
      "|    std                  | 3.67         |\n",
      "|    value_loss           | 8.63         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 217        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 37         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.10027759 |\n",
      "|    clip_fraction        | 0.62       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.7      |\n",
      "|    explained_variance   | 0.00289    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 2.54       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0771     |\n",
      "|    reward               | -0.191177  |\n",
      "|    std                  | 3.71       |\n",
      "|    value_loss           | 6.84       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 48         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0869338  |\n",
      "|    clip_fraction        | 0.645      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.8      |\n",
      "|    explained_variance   | 0.00181    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.77       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0828     |\n",
      "|    reward               | 0.10285601 |\n",
      "|    std                  | 3.8        |\n",
      "|    value_loss           | 5.08       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2020-01-03 to  2020-04-03\n",
      "PPO Sharpe Ratio:  -0.20661802225581585\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_10\n",
      "day: 2516, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2019519.75\n",
      "total_reward: 1019519.75\n",
      "total_cost: 40272.06\n",
      "total_trades: 5242\n",
      "Sharpe: 0.531\n",
      "=================================\n",
      "======DDPG Validation from:  2020-01-03 to  2020-04-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-04-03\n",
      "======Trading from:  2020-04-03 to  2020-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-04-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_756_10\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 269         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -0.00438    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -9.64       |\n",
      "|    reward             | -0.12050998 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 2.05        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 267        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -0.668     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 1.14       |\n",
      "|    reward             | 0.57166094 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 263        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0.0256     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.22      |\n",
      "|    reward             | -1.5777053 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.7        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 242        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0.143      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -1.48      |\n",
      "|    reward             | 0.37589929 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.375      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 246         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0.00844     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -5.34       |\n",
      "|    reward             | -0.26275042 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 0.743       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 251        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 3.67       |\n",
      "|    reward             | -1.1282929 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.76       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 251         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.02       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -0.861      |\n",
      "|    reward             | 0.091771275 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 0.568       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 252       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.53     |\n",
      "|    reward             | -1.202624 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 0.0928    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 253         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -0.00234    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -18.4       |\n",
      "|    reward             | -0.36247188 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 4.92        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 253        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 9.31       |\n",
      "|    reward             | 0.48543575 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 5.13       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 252       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -0.0247   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -4.83     |\n",
      "|    reward             | 0.9615874 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 0.751     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 251        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -0.118     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 9.06       |\n",
      "|    reward             | 0.80546564 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 1.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 251       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 5.18      |\n",
      "|    reward             | 1.2143096 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 0.773     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 251         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -6.81       |\n",
      "|    reward             | -0.13895957 |\n",
      "|    std                | 0.983       |\n",
      "|    value_loss         | 1.55        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 252       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 2.96      |\n",
      "|    reward             | 1.0662291 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 0.518     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 252          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 31           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.01        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -1.35        |\n",
      "|    reward             | -0.048277345 |\n",
      "|    std                | 0.985        |\n",
      "|    value_loss         | 0.267        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 250       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 12.6      |\n",
      "|    reward             | 1.0485862 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 6.09      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 248       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.28      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 1.63      |\n",
      "|    reward             | -1.160616 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 0.196     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 248        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.384     |\n",
      "|    reward             | 0.22093779 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 0.0681     |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 248      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.09    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -36.3    |\n",
      "|    reward             | 6.732588 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 25.1     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-04-03 to  2020-07-06\n",
      "A2C Sharpe Ratio:  0.31906345213703036\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_756_10\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 314          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 6            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.048807226 |\n",
      "-------------------------------------\n",
      "day: 2579, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 376801.06\n",
      "total_reward: -623198.94\n",
      "total_cost: 1091061.31\n",
      "total_trades: 9761\n",
      "Sharpe: -0.480\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 204        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 4.961308   |\n",
      "|    clip_fraction        | 0.954      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.9       |\n",
      "|    explained_variance   | -0.146     |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.4        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.24       |\n",
      "|    reward               | 0.22981471 |\n",
      "|    std                  | 1.83       |\n",
      "|    value_loss           | 3.28       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 177        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 3.8006978  |\n",
      "|    clip_fraction        | 0.98       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.3      |\n",
      "|    explained_variance   | -0.000785  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 3.2        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.28       |\n",
      "|    reward               | 0.35244784 |\n",
      "|    std                  | 5.3        |\n",
      "|    value_loss           | 9.39       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 164         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.086575516 |\n",
      "|    clip_fraction        | 0.62        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.6       |\n",
      "|    explained_variance   | -0.000497   |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.87        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0739      |\n",
      "|    reward               | 1.9016523   |\n",
      "|    std                  | 5.57        |\n",
      "|    value_loss           | 6.56        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.071767956 |\n",
      "|    clip_fraction        | 0.519       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.6       |\n",
      "|    explained_variance   | 0.000696    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 9.69        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0525      |\n",
      "|    reward               | -0.21887887 |\n",
      "|    std                  | 5.86        |\n",
      "|    value_loss           | 6.43        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-04-03 to  2020-07-06\n",
      "PPO Sharpe Ratio:  0.445509404408488\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_10\n",
      "day: 2579, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2517783.23\n",
      "total_reward: 1517783.23\n",
      "total_cost: 8817.00\n",
      "total_trades: 5179\n",
      "Sharpe: 0.602\n",
      "=================================\n",
      "======DDPG Validation from:  2020-04-03 to  2020-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-07-06\n",
      "======Trading from:  2020-07-06 to  2020-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_819_10\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 219        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0.00393    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -14.6      |\n",
      "|    reward             | 0.20021646 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 220       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0.229     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.774     |\n",
      "|    reward             | 0.7638591 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.907     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 219        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.52      |\n",
      "|    reward             | -2.1946774 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.78       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 219         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -21.2       |\n",
      "|    reward             | -0.39811993 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 10.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -5.82e-05  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -13.2      |\n",
      "|    reward             | -1.0644451 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 221        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -1.15      |\n",
      "|    reward             | -1.7306323 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.96       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -9.94      |\n",
      "|    reward             | -1.5076199 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 220       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 8.35      |\n",
      "|    reward             | 1.3216795 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.88      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 222       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -7.76     |\n",
      "|    reward             | 1.5032953 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.5       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 224       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -0.00769  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -33.9     |\n",
      "|    reward             | 2.3514967 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 29.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 225        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -3.88      |\n",
      "|    reward             | -0.3500402 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.358      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 226        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0.000998   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -18.2      |\n",
      "|    reward             | -1.2698307 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.00495    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 6.47        |\n",
      "|    reward             | -0.28329992 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 227       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -6.4      |\n",
      "|    reward             | 0.6958652 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 228        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -79.4      |\n",
      "|    reward             | -2.9450743 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 144        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 1.17       |\n",
      "|    reward             | -1.1894175 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.174      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 229       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 2.78      |\n",
      "|    reward             | -2.01572  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.395     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 7.05      |\n",
      "|    reward             | -1.552858 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -6.33     |\n",
      "|    reward             | 1.0082009 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.832     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 228        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -0.00172   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -9.82      |\n",
      "|    reward             | -3.2351449 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.89       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-07-06 to  2020-10-02\n",
      "A2C Sharpe Ratio:  0.20635865493391406\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_819_10\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 299        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 6          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.16605389 |\n",
      "-----------------------------------\n",
      "day: 2642, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 314129.89\n",
      "total_reward: -685870.11\n",
      "total_cost: 1067752.60\n",
      "total_trades: 9932\n",
      "Sharpe: -0.527\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 188        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 7.700732   |\n",
      "|    clip_fraction        | 0.992      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -12.6      |\n",
      "|    explained_variance   | -0.0305    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.88       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.296      |\n",
      "|    reward               | 0.09505313 |\n",
      "|    std                  | 10         |\n",
      "|    value_loss           | 3.34       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.033834763 |\n",
      "|    clip_fraction        | 0.42        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.7       |\n",
      "|    explained_variance   | -0.00509    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.032       |\n",
      "|    reward               | 0.23349433  |\n",
      "|    std                  | 10.2        |\n",
      "|    value_loss           | 4.37        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 53         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03092214 |\n",
      "|    clip_fraction        | 0.377      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -18.7      |\n",
      "|    explained_variance   | -0.00133   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 0.722      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0243     |\n",
      "|    reward               | -1.271275  |\n",
      "|    std                  | 10.4       |\n",
      "|    value_loss           | 4.49       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 69          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035868958 |\n",
      "|    clip_fraction        | 0.389       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.8       |\n",
      "|    explained_variance   | -0.00739    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.313       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.023       |\n",
      "|    reward               | 0.12992029  |\n",
      "|    std                  | 10.5        |\n",
      "|    value_loss           | 2.11        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-06 to  2020-10-02\n",
      "PPO Sharpe Ratio:  -0.09626816943293738\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_10\n",
      "day: 2642, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2186529.46\n",
      "total_reward: 1186529.46\n",
      "total_cost: 998.33\n",
      "total_trades: 7926\n",
      "Sharpe: 0.509\n",
      "=================================\n",
      "======DDPG Validation from:  2020-07-06 to  2020-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-10-02\n",
      "======Trading from:  2020-10-02 to  2021-01-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_882_10\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 242          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 0.0769       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -10.2        |\n",
      "|    reward             | -0.059537694 |\n",
      "|    std                | 0.997        |\n",
      "|    value_loss         | 2.96         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 242        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.0651     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.27      |\n",
      "|    reward             | 0.65885293 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 237        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -11.8      |\n",
      "|    reward             | -2.3403218 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 4.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 231       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -4.04     |\n",
      "|    reward             | 0.4536605 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.66      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 230        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -9.41      |\n",
      "|    reward             | -0.6039102 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.8        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 225         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 3.92        |\n",
      "|    reward             | -0.61400354 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 4.12        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 223        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -5.9       |\n",
      "|    reward             | 0.43986684 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 222        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 14.6       |\n",
      "|    reward             | 0.09445433 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 222       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 10.7      |\n",
      "|    reward             | 0.560904  |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.36      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 221         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -10.2       |\n",
      "|    reward             | -0.26271835 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 3.69        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -23.1      |\n",
      "|    reward             | 0.12169733 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 7.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 11.6       |\n",
      "|    reward             | -1.1051613 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 220       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.524    |\n",
      "|    reward             | -0.045325 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.112     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 219       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 9.01      |\n",
      "|    reward             | 0.1253747 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.09      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 220        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 3.44e-05   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 10         |\n",
      "|    reward             | -1.9421134 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.42       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 220         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -0.00087    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 97.3        |\n",
      "|    reward             | -0.03734655 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 186         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 219        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 21         |\n",
      "|    reward             | -1.5162438 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 12.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 218        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 4.01       |\n",
      "|    reward             | 0.76380545 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 217       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -2.69     |\n",
      "|    reward             | 0.5550875 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 216       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 2.11      |\n",
      "|    reward             | 0.2503495 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.435     |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-10-02 to  2021-01-04\n",
      "A2C Sharpe Ratio:  0.321045637647246\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_882_10\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 270        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 7          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.34965244 |\n",
      "-----------------------------------\n",
      "day: 2705, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 615067.73\n",
      "total_reward: -384932.27\n",
      "total_cost: 1196959.27\n",
      "total_trades: 10532\n",
      "Sharpe: -0.161\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 173       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 23        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 6.4674077 |\n",
      "|    clip_fraction        | 0.989     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -12.2     |\n",
      "|    explained_variance   | -0.0213   |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 1.78      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.313     |\n",
      "|    reward               | 0.8276442 |\n",
      "|    std                  | 7.79      |\n",
      "|    value_loss           | 4.5       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 153        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 40         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06277282 |\n",
      "|    clip_fraction        | 0.503      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17.4      |\n",
      "|    explained_variance   | 0.000527   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 3.71       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.0502     |\n",
      "|    reward               | 0.9959818  |\n",
      "|    std                  | 8.03       |\n",
      "|    value_loss           | 8.65       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 146        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 55         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03518412 |\n",
      "|    clip_fraction        | 0.412      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17.6      |\n",
      "|    explained_variance   | 0.000597   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 5.27       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0272     |\n",
      "|    reward               | 0.2991359  |\n",
      "|    std                  | 8.44       |\n",
      "|    value_loss           | 12.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 142         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.07015721  |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.8       |\n",
      "|    explained_variance   | -0.000683   |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0332      |\n",
      "|    reward               | 0.030923868 |\n",
      "|    std                  | 8.64        |\n",
      "|    value_loss           | 7.62        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-10-02 to  2021-01-04\n",
      "PPO Sharpe Ratio:  0.37864672663207355\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_10\n",
      "day: 2705, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2480344.87\n",
      "total_reward: 1480344.87\n",
      "total_cost: 997.55\n",
      "total_trades: 5410\n",
      "Sharpe: 0.564\n",
      "=================================\n",
      "======DDPG Validation from:  2020-10-02 to  2021-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-01-04\n",
      "======Trading from:  2021-01-04 to  2021-04-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_945_10\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 194         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.5       |\n",
      "|    reward             | 0.029800948 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.8         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.323     |\n",
      "|    reward             | 0.7225787 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -3.89     |\n",
      "|    reward             | -2.270363 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 2.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 3.26      |\n",
      "|    reward             | 1.0660193 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 2.69      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 200         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -20.1       |\n",
      "|    reward             | -0.88791347 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 7.34        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 7.51      |\n",
      "|    reward             | 1.1311709 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 9.09      |\n",
      "|    reward             | 1.0041214 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 3.06      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 5.73      |\n",
      "|    reward             | 0.4733566 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 32        |\n",
      "|    reward             | 0.7563258 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 31.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 17.3      |\n",
      "|    reward             | -8.364809 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 35.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -128       |\n",
      "|    reward             | 0.17461172 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 320        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -1.76      |\n",
      "|    reward             | -3.3923078 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 2.59       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 17.5       |\n",
      "|    reward             | 0.53480697 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 9.13       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 16.1      |\n",
      "|    reward             | 4.8972807 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 7.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 6.24       |\n",
      "|    reward             | 0.02417506 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -20.5     |\n",
      "|    reward             | 4.3626184 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 22.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -4.67      |\n",
      "|    reward             | 0.79708755 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.528      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -22.5      |\n",
      "|    reward             | 0.28448743 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 8.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -18.8      |\n",
      "|    reward             | -0.5445003 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 8.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 50         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 12.4       |\n",
      "|    reward             | -1.4757499 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 7.82       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  0.17446436686481323\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_945_10\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 258         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.03736942 |\n",
      "------------------------------------\n",
      "day: 2768, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 725416.33\n",
      "total_reward: -274583.67\n",
      "total_cost: 1255794.02\n",
      "total_trades: 10469\n",
      "Sharpe: -0.074\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 166         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 5.7587833   |\n",
      "|    clip_fraction        | 0.991       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -11.5       |\n",
      "|    explained_variance   | -0.0651     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.95        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | 0.289       |\n",
      "|    reward               | -0.14036733 |\n",
      "|    std                  | 6.69        |\n",
      "|    value_loss           | 4.26        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 155        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 39         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0741983  |\n",
      "|    clip_fraction        | 0.551      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -16.7      |\n",
      "|    explained_variance   | -0.00179   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 4.32       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.0526     |\n",
      "|    reward               | -1.7144132 |\n",
      "|    std                  | 6.91       |\n",
      "|    value_loss           | 5.75       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 150         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.095193475 |\n",
      "|    clip_fraction        | 0.506       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -16.8       |\n",
      "|    explained_variance   | -0.00261    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 3.15        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0431      |\n",
      "|    reward               | 0.45834166  |\n",
      "|    std                  | 7.15        |\n",
      "|    value_loss           | 5.39        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 147          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.045331627  |\n",
      "|    clip_fraction        | 0.486        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -16.9        |\n",
      "|    explained_variance   | 2.84e-05     |\n",
      "|    learning_rate        | 0.01         |\n",
      "|    loss                 | 2.29         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.0433       |\n",
      "|    reward               | -0.123282805 |\n",
      "|    std                  | 7.37         |\n",
      "|    value_loss           | 3.51         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  0.18276558301328802\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_10\n",
      "day: 2768, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4942027.53\n",
      "total_reward: 3942027.53\n",
      "total_cost: 998.92\n",
      "total_trades: 5536\n",
      "Sharpe: 0.835\n",
      "=================================\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1008_10\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.00359   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | 0.47171497 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.24       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 5.58     |\n",
      "|    reward             | 1.5222   |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.29     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -19.6      |\n",
      "|    reward             | -1.9298959 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.98       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -23.5       |\n",
      "|    reward             | -0.52648395 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 10.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 196         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -15.1       |\n",
      "|    reward             | -0.62390214 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 7.9         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -11.5     |\n",
      "|    reward             | 1.0029552 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 4.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 10.8       |\n",
      "|    reward             | 0.12726812 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.92       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 18.3      |\n",
      "|    reward             | 0.7279316 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 8.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -0.0329    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 4.81       |\n",
      "|    reward             | 0.26560456 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0.0377    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | 1.0747824 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 3.35      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.01    |\n",
      "|    explained_variance | 0.0129   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 19.5     |\n",
      "|    reward             | 2.508786 |\n",
      "|    std                | 0.984    |\n",
      "|    value_loss         | 6.67     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 18.1      |\n",
      "|    reward             | -0.567933 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 6.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 200        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 9.06       |\n",
      "|    reward             | -1.2751951 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 2.33       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 201         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -1.05       |\n",
      "|    reward             | -0.43232074 |\n",
      "|    std                | 0.982       |\n",
      "|    value_loss         | 1.3         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 202       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -7.36     |\n",
      "|    reward             | 1.7667061 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 0.989     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 203       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -11.5     |\n",
      "|    reward             | 0.6847785 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 4.45      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 204         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 29.4        |\n",
      "|    reward             | 0.108056754 |\n",
      "|    std                | 0.978       |\n",
      "|    value_loss         | 16.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 205       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | -0.00149  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 15.3      |\n",
      "|    reward             | 1.6968659 |\n",
      "|    std                | 0.97      |\n",
      "|    value_loss         | 7.17      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 205        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 12         |\n",
      "|    reward             | 0.24306172 |\n",
      "|    std                | 0.971      |\n",
      "|    value_loss         | 5.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 206       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 48        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 16.8      |\n",
      "|    reward             | 1.7433295 |\n",
      "|    std                | 0.971     |\n",
      "|    value_loss         | 7.17      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.20019096199177783\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1008_10\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 297          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 6            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.115826815 |\n",
      "-------------------------------------\n",
      "day: 2831, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1092144.80\n",
      "total_reward: 92144.80\n",
      "total_cost: 990920.53\n",
      "total_trades: 11080\n",
      "Sharpe: 0.132\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 187        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 21         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 6.096242   |\n",
      "|    clip_fraction        | 0.97       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -8.6       |\n",
      "|    explained_variance   | -0.0998    |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.81       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.252      |\n",
      "|    reward               | -1.1999649 |\n",
      "|    std                  | 1.48       |\n",
      "|    value_loss           | 4.22       |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 172       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 35        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 5.6277742 |\n",
      "|    clip_fraction        | 0.992     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13.6     |\n",
      "|    explained_variance   | 7e-05     |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 5.52      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | 0.298     |\n",
      "|    reward               | 3.8564854 |\n",
      "|    std                  | 9.87      |\n",
      "|    value_loss           | 10.1      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 49         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03685629 |\n",
      "|    clip_fraction        | 0.429      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -18.5      |\n",
      "|    explained_variance   | -0.000466  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 18.3       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0278     |\n",
      "|    reward               | -0.6527547 |\n",
      "|    std                  | 10.7       |\n",
      "|    value_loss           | 39.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 158         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029225295 |\n",
      "|    clip_fraction        | 0.451       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -18.9       |\n",
      "|    explained_variance   | -0.00655    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 3.16        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0328      |\n",
      "|    reward               | 0.03676815  |\n",
      "|    std                  | 10.8        |\n",
      "|    value_loss           | 5.38        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  -0.007299037876349856\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1008_10\n",
      "day: 2831, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3568818.33\n",
      "total_reward: 2568818.33\n",
      "total_cost: 998.35\n",
      "total_trades: 2831\n",
      "Sharpe: 0.737\n",
      "=================================\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1071_10\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 214        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | 0.41858315 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.94       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 216      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 5.46     |\n",
      "|    reward             | 1.337261 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.05     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 216        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -2.01      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -22.2      |\n",
      "|    reward             | -1.7486664 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 15.6       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 217         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -23.7       |\n",
      "|    reward             | -0.34193107 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 13.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 215        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0.115      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -9.85      |\n",
      "|    reward             | -0.8506461 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 4.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 214        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 12.7       |\n",
      "|    reward             | -3.5721414 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 2.6        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 213       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.985    |\n",
      "|    reward             | 1.4876559 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 4.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 214       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -0.179    |\n",
      "|    reward             | 2.308325  |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 4.74      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 214        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -11.9      |\n",
      "|    reward             | -1.1841546 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 3.15       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 214      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.04    |\n",
      "|    explained_variance | 0.00596  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 30       |\n",
      "|    reward             | -0.1158  |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 14.5     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 214        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -37        |\n",
      "|    reward             | -1.2284911 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 58.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 214       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 4.25      |\n",
      "|    reward             | 2.1171994 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 0.569     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 215        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -1.64      |\n",
      "|    reward             | 0.43764853 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.692      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 215       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -19.5     |\n",
      "|    reward             | 1.9341184 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 11.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 215        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -8.86      |\n",
      "|    reward             | 0.67229766 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 2.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 215       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0.00734   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -15.9     |\n",
      "|    reward             | 2.4704874 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 6.05      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 216       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0.00266   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 24.9      |\n",
      "|    reward             | 1.0479732 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 18.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 217       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.28      |\n",
      "|    reward             | 0.2214502 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 0.556     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 217        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -4.31      |\n",
      "|    reward             | 0.65837544 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.421      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 219        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 0.324      |\n",
      "|    reward             | -0.3712092 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.882      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.04547494480900616\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1071_10\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 328         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 6           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.008002374 |\n",
      "------------------------------------\n",
      "day: 2894, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 918423.88\n",
      "total_reward: -81576.12\n",
      "total_cost: 1202368.57\n",
      "total_trades: 11066\n",
      "Sharpe: 0.053\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 198       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 20        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.690062  |\n",
      "|    clip_fraction        | 0.973     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.96     |\n",
      "|    explained_variance   | -0.00183  |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 2.67      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.526     |\n",
      "|    reward               | -2.270943 |\n",
      "|    std                  | 3.02      |\n",
      "|    value_loss           | 4.23      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 168        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.15524764 |\n",
      "|    clip_fraction        | 0.781      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -12.9      |\n",
      "|    explained_variance   | -0.000688  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 2.7        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.165      |\n",
      "|    reward               | 1.6094092  |\n",
      "|    std                  | 3.03       |\n",
      "|    value_loss           | 9.8        |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 154        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 53         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.14212167 |\n",
      "|    clip_fraction        | 0.665      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -12.7      |\n",
      "|    explained_variance   | -0.000727  |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 31.9       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0885     |\n",
      "|    reward               | 0.4246443  |\n",
      "|    std                  | 2.99       |\n",
      "|    value_loss           | 33.5       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 147        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.2773712  |\n",
      "|    clip_fraction        | 0.74       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -12.8      |\n",
      "|    explained_variance   | -0.00844   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 13.4       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.128      |\n",
      "|    reward               | 0.68851405 |\n",
      "|    std                  | 3.12       |\n",
      "|    value_loss           | 17.8       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  0.03430832281795505\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1071_10\n",
      "day: 2894, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2625099.51\n",
      "total_reward: 1625099.51\n",
      "total_cost: 998.81\n",
      "total_trades: 8682\n",
      "Sharpe: 0.558\n",
      "=================================\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1134_10\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 216        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0.0584     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | 0.25930268 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 4.58       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 219       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | -1.13     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 6.87      |\n",
      "|    reward             | 1.3639964 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 4.98      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 215        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.14      |\n",
      "|    reward             | -2.1428325 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 3.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 222        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -19.4      |\n",
      "|    reward             | 0.11628911 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 7.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 222         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -9.4        |\n",
      "|    reward             | -0.70010614 |\n",
      "|    std                | 0.986       |\n",
      "|    value_loss         | 3.97        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 224       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.51     |\n",
      "|    reward             | 1.3100785 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 0.275     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 226        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -0.334     |\n",
      "|    reward             | 0.57854444 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 0.166      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 227         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 13.3        |\n",
      "|    reward             | -0.29866353 |\n",
      "|    std                | 0.978       |\n",
      "|    value_loss         | 4.44        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -24.6     |\n",
      "|    reward             | 0.5125774 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 21.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 227       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -36.7     |\n",
      "|    reward             | 1.1486608 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 78.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 225       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 11.2      |\n",
      "|    reward             | 3.9279168 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 7.84      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 224       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -13.5     |\n",
      "|    reward             | -1.023692 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 5.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 223        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -7.32      |\n",
      "|    reward             | 0.79192734 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 223        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 13.2       |\n",
      "|    reward             | -3.1004188 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 5.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 221        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 22.2       |\n",
      "|    reward             | 0.33227167 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 11.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 219       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -8.81     |\n",
      "|    reward             | 2.114233  |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 12        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 218       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 38        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 6.21e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -42.2     |\n",
      "|    reward             | 2.6701756 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 93.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 216        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -13.3      |\n",
      "|    reward             | 0.19982305 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 4.57       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 215       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 11.7      |\n",
      "|    reward             | 0.9025464 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 6.38      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 214      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 46       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.01    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 9.38     |\n",
      "|    reward             | 1.247914 |\n",
      "|    std                | 0.983    |\n",
      "|    value_loss         | 2.14     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  0.21345124185766925\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1134_10\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 263         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.06605527 |\n",
      "------------------------------------\n",
      "day: 2957, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 451915.54\n",
      "total_reward: -548084.46\n",
      "total_cost: 1269539.29\n",
      "total_trades: 10977\n",
      "Sharpe: -0.304\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 166       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 24        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 10.997925 |\n",
      "|    clip_fraction        | 0.996     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -13.9     |\n",
      "|    explained_variance   | 0.011     |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 1.49      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.3       |\n",
      "|    reward               | 0.3728459 |\n",
      "|    std                  | 22.2      |\n",
      "|    value_loss           | 4.3       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020268498 |\n",
      "|    clip_fraction        | 0.298       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.5       |\n",
      "|    explained_variance   | -0.00133    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 0.878       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0144      |\n",
      "|    reward               | -0.24670711 |\n",
      "|    std                  | 22.6        |\n",
      "|    value_loss           | 4.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 138         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019965295 |\n",
      "|    clip_fraction        | 0.264       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.6       |\n",
      "|    explained_variance   | -0.0158     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 2.59        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0119      |\n",
      "|    reward               | 0.39135405  |\n",
      "|    std                  | 23.4        |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 135         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015625969 |\n",
      "|    clip_fraction        | 0.249       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -22.8       |\n",
      "|    explained_variance   | -0.0108     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.09        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0106      |\n",
      "|    reward               | -0.08888935 |\n",
      "|    std                  | 23.7        |\n",
      "|    value_loss           | 2.04        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
      "PPO Sharpe Ratio:  0.17071064433947983\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1134_10\n",
      "day: 2957, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3230583.06\n",
      "total_reward: 2230583.06\n",
      "total_cost: 1122.68\n",
      "total_trades: 2959\n",
      "Sharpe: 0.671\n",
      "=================================\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
      "======Trading from:  2022-01-03 to  2022-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1197_10\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 183        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.4      |\n",
      "|    reward             | 0.22116783 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.41       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 185       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.768     |\n",
      "|    reward             | 0.8394936 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -3.89     |\n",
      "|    reward             | -2.369369 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.26      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 186      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -6.14    |\n",
      "|    reward             | 0.827698 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 1.21     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 187         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -10.7       |\n",
      "|    reward             | -0.73981637 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 6.03        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 187      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -3.31    |\n",
      "|    reward             | 6.553904 |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 3.8      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -24.4      |\n",
      "|    reward             | -0.5056434 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 9.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 11.1      |\n",
      "|    reward             | 1.8454558 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 4.97      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 188         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -13         |\n",
      "|    reward             | -0.07527179 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 5.03        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 188         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 4.29        |\n",
      "|    reward             | -0.17366749 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.458       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 189       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -4.02     |\n",
      "|    reward             | 0.8312124 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.717     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 189         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 4.02        |\n",
      "|    reward             | -0.30105633 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.41        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 189      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -6.35    |\n",
      "|    reward             | 1.432489 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.33     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 189        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 12.8       |\n",
      "|    reward             | -0.9011328 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 4.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 189         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0.0241      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 7.78        |\n",
      "|    reward             | -0.40751767 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 1.94        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 189         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 5.25        |\n",
      "|    reward             | -0.31601503 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 1.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -30.8     |\n",
      "|    reward             | 1.3619097 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 28.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 17.1      |\n",
      "|    reward             | -1.721061 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 190      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 49       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -4.32    |\n",
      "|    reward             | 1.371454 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 1.61     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 189         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 52          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 5.82        |\n",
      "|    reward             | -0.15488464 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 0.836       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-01-03 to  2022-04-04\n",
      "A2C Sharpe Ratio:  -0.15978117050468482\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1197_10\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 249        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 8          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.06665199 |\n",
      "-----------------------------------\n",
      "day: 3020, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 612521.84\n",
      "total_reward: -387478.16\n",
      "total_cost: 1129170.18\n",
      "total_trades: 11127\n",
      "Sharpe: -0.162\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 164        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 24         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 3.9036074  |\n",
      "|    clip_fraction        | 0.918      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.77      |\n",
      "|    explained_variance   | -0.226     |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 1.8        |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | 0.196      |\n",
      "|    reward               | -1.2129683 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 3.32       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 149        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 40         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 4.892263   |\n",
      "|    clip_fraction        | 0.989      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -11.7      |\n",
      "|    explained_variance   | -0.00142   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 2.22       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.325      |\n",
      "|    reward               | -3.0674574 |\n",
      "|    std                  | 5.48       |\n",
      "|    value_loss           | 5.53       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 143         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.121096164 |\n",
      "|    clip_fraction        | 0.575       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -15.6       |\n",
      "|    explained_variance   | -0.0011     |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 8.76        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.0675      |\n",
      "|    reward               | 1.0178661   |\n",
      "|    std                  | 5.47        |\n",
      "|    value_loss           | 28.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 140        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 72         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.08952953 |\n",
      "|    clip_fraction        | 0.561      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -15.6      |\n",
      "|    explained_variance   | 0.000386   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 20         |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | 0.0582     |\n",
      "|    reward               | 0.2856969  |\n",
      "|    std                  | 5.72       |\n",
      "|    value_loss           | 14.1       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-01-03 to  2022-04-04\n",
      "PPO Sharpe Ratio:  -0.27499658455430576\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1197_10\n",
      "day: 3020, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5938132.01\n",
      "total_reward: 4938132.01\n",
      "total_cost: 998.92\n",
      "total_trades: 6040\n",
      "Sharpe: 0.856\n",
      "=================================\n",
      "======DDPG Validation from:  2022-01-03 to  2022-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
      "======Trading from:  2022-04-04 to  2022-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1260_10\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 228         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.162      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.3       |\n",
      "|    reward             | 0.025544776 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.16        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 228        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | -0.0105    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.52      |\n",
      "|    reward             | 0.56182003 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 229        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.182      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.34      |\n",
      "|    reward             | -1.1740576 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | -0.253     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.68      |\n",
      "|    reward             | -0.0859885 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.677      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 228         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -3.95       |\n",
      "|    reward             | -0.22112836 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.634       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 228       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | -0.387    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -3.4      |\n",
      "|    reward             | 1.9431659 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 227       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | -0.0117   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -45.1     |\n",
      "|    reward             | 2.5911388 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 47.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 226         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.19       |\n",
      "|    explained_variance | 0.0601      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -15.8       |\n",
      "|    reward             | 0.012572467 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.76        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 226        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.21      |\n",
      "|    explained_variance | 0.0173     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -6         |\n",
      "|    reward             | -1.3311242 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 226       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 1.71      |\n",
      "|    reward             | 1.2178912 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.0646    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 227        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | -0.0544    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -17.8      |\n",
      "|    reward             | -1.0712049 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 230        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 0.68       |\n",
      "|    reward             | -0.7174923 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.482      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 235        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -5.58      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | 0.41581258 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.76       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 240      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -15.1    |\n",
      "|    reward             | 0.548325 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 7.2      |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 244         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 4.28        |\n",
      "|    reward             | -0.20592944 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.347       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 248        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -3.97      |\n",
      "|    reward             | -1.2759197 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.601      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 250       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 9.46      |\n",
      "|    reward             | 0.4837519 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.99      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 253         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 35          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -7          |\n",
      "|    reward             | -0.16595669 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.87        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 255        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -0.0456    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 7.89       |\n",
      "|    reward             | -0.4157413 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.34       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 257         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -4.52       |\n",
      "|    reward             | -0.69816196 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.928       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-04-04 to  2022-07-06\n",
      "A2C Sharpe Ratio:  -0.28331426102401075\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1260_10\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 416          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0067771375 |\n",
      "-------------------------------------\n",
      "day: 3083, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1205044.90\n",
      "total_reward: 205044.90\n",
      "total_cost: 1487699.50\n",
      "total_trades: 12172\n",
      "Sharpe: 0.175\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 286       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 14        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 3.759842  |\n",
      "|    clip_fraction        | 0.971     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -9.2      |\n",
      "|    explained_variance   | -0.152    |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 3.89      |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.252     |\n",
      "|    reward               | 0.6496595 |\n",
      "|    std                  | 1.76      |\n",
      "|    value_loss           | 4.48      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 261        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 23         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 6.85114    |\n",
      "|    clip_fraction        | 0.995      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -13.4      |\n",
      "|    explained_variance   | -3.5e-05   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 8.63       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | 0.303      |\n",
      "|    reward               | -4.1589503 |\n",
      "|    std                  | 8.69       |\n",
      "|    value_loss           | 12.6       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 251         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037219092 |\n",
      "|    clip_fraction        | 0.496       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.9       |\n",
      "|    explained_variance   | 0.000212    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 33.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | 0.041       |\n",
      "|    reward               | -0.12836859 |\n",
      "|    std                  | 8.9         |\n",
      "|    value_loss           | 45.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.04291739  |\n",
      "|    clip_fraction        | 0.47        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.9       |\n",
      "|    explained_variance   | -0.000661   |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 18.5        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0442      |\n",
      "|    reward               | -0.14942488 |\n",
      "|    std                  | 8.92        |\n",
      "|    value_loss           | 15.8        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-04 to  2022-07-06\n",
      "PPO Sharpe Ratio:  -0.2923112258152954\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1260_10\n",
      "day: 3083, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3111165.51\n",
      "total_reward: 2111165.51\n",
      "total_cost: 1459.83\n",
      "total_trades: 9250\n",
      "Sharpe: 0.619\n",
      "=================================\n",
      "======DDPG Validation from:  2022-04-04 to  2022-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-06\n",
      "======Trading from:  2022-07-06 to  2022-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1323_10\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 319          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.13        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -11.1        |\n",
      "|    reward             | -0.120043114 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 2.72         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 314        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.19      |\n",
      "|    reward             | 0.58631146 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 311        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.72      |\n",
      "|    reward             | -1.9723984 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.9        |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 308          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.16        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -10.6        |\n",
      "|    reward             | -0.042740528 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 2.43         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 309        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | -0.7114921 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.77       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 310      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -2.31    |\n",
      "|    reward             | 5.666553 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.47     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 308        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -5.24      |\n",
      "|    reward             | 0.59671587 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.514      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 309         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.15       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 8.61        |\n",
      "|    reward             | -0.19217183 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2           |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 310         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -1.4        |\n",
      "|    reward             | -0.05256079 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.925       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 309       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 2.07      |\n",
      "|    reward             | 0.9810738 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.236     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 4.19       |\n",
      "|    reward             | 0.96509165 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.51       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 305       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 15.6      |\n",
      "|    reward             | 1.0118906 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.62      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -3.14      |\n",
      "|    reward             | -0.4387222 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.593      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | -2.1702244 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 4.08       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 306      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 24       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.02    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -32.1    |\n",
      "|    reward             | 1.77449  |\n",
      "|    std                | 0.986    |\n",
      "|    value_loss         | 28.6     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 305         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -0.867      |\n",
      "|    reward             | 0.051252782 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 0.592       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 306        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -0.192     |\n",
      "|    reward             | -0.5481055 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 0.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 307       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -15       |\n",
      "|    reward             | 0.3982373 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 20.1      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 308          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 30           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.04        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 2.24         |\n",
      "|    reward             | -0.049529962 |\n",
      "|    std                | 0.989        |\n",
      "|    value_loss         | 0.34         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 309        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 8.34       |\n",
      "|    reward             | -1.0435591 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-07-06 to  2022-10-04\n",
      "A2C Sharpe Ratio:  -0.10212633817435264\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1323_10\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 422         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.028962603 |\n",
      "------------------------------------\n",
      "day: 3146, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 470469.81\n",
      "total_reward: -529530.19\n",
      "total_cost: 1269638.08\n",
      "total_trades: 11200\n",
      "Sharpe: -0.274\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 284       |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 14        |\n",
      "|    total_timesteps      | 4096      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 7.8660293 |\n",
      "|    clip_fraction        | 0.989     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -12.1     |\n",
      "|    explained_variance   | -0.0456   |\n",
      "|    learning_rate        | 0.01      |\n",
      "|    loss                 | 0.527     |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | 0.336     |\n",
      "|    reward               | 1.1027888 |\n",
      "|    std                  | 8.52      |\n",
      "|    value_loss           | 3.6       |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.045283034 |\n",
      "|    clip_fraction        | 0.504       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.3       |\n",
      "|    explained_variance   | 0.000587    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | 0.0466      |\n",
      "|    reward               | 0.15314107  |\n",
      "|    std                  | 8.42        |\n",
      "|    value_loss           | 3.8         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 236        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.05630288 |\n",
      "|    clip_fraction        | 0.489      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17.3      |\n",
      "|    explained_variance   | -0.00483   |\n",
      "|    learning_rate        | 0.01       |\n",
      "|    loss                 | 0.255      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | 0.0404     |\n",
      "|    reward               | 0.28455207 |\n",
      "|    std                  | 8.6        |\n",
      "|    value_loss           | 1.44       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 226         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.052972555 |\n",
      "|    clip_fraction        | 0.435       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17.3       |\n",
      "|    explained_variance   | -0.00167    |\n",
      "|    learning_rate        | 0.01        |\n",
      "|    loss                 | 1.86        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | 0.0319      |\n",
      "|    reward               | 0.08255758  |\n",
      "|    std                  | 8.5         |\n",
      "|    value_loss           | 3.92        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-06 to  2022-10-04\n",
      "PPO Sharpe Ratio:  -0.2871869778498048\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.01, 'batch_size': 16}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1323_10\n",
      "day: 3146, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3911431.23\n",
      "total_reward: 2911431.23\n",
      "total_cost: 2530.14\n",
      "total_trades: 6295\n",
      "Sharpe: 0.689\n",
      "=================================\n",
      "======DDPG Validation from:  2022-07-06 to  2022-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-04\n",
      "======Trading from:  2022-10-04 to  2023-01-04\n",
      "Ensemble Strategy took:  59.61696083545685  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.756501</td>\n",
       "      <td>0.104886</td>\n",
       "      <td>0.417703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.00931</td>\n",
       "      <td>-0.050068</td>\n",
       "      <td>-0.082933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.009017</td>\n",
       "      <td>-0.166887</td>\n",
       "      <td>0.074028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.339836</td>\n",
       "      <td>0.023851</td>\n",
       "      <td>0.316097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.286318</td>\n",
       "      <td>-0.43422</td>\n",
       "      <td>-0.289125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>441</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.647303</td>\n",
       "      <td>0.429269</td>\n",
       "      <td>0.624779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>504</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.02273</td>\n",
       "      <td>-0.026822</td>\n",
       "      <td>0.114625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>567</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.121557</td>\n",
       "      <td>-0.218156</td>\n",
       "      <td>-0.151744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>630</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.546106</td>\n",
       "      <td>0.366706</td>\n",
       "      <td>0.115436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>693</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>PPO</td>\n",
       "      <td>-0.08546</td>\n",
       "      <td>-0.047401</td>\n",
       "      <td>-0.080111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>756</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.321712</td>\n",
       "      <td>0.282374</td>\n",
       "      <td>0.319396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>819</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.186139</td>\n",
       "      <td>0.107357</td>\n",
       "      <td>0.198622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>PPO</td>\n",
       "      <td>0.400432</td>\n",
       "      <td>0.403017</td>\n",
       "      <td>0.173859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>945</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.217535</td>\n",
       "      <td>0.198004</td>\n",
       "      <td>0.175299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1008</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.095621</td>\n",
       "      <td>-0.133131</td>\n",
       "      <td>0.235725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1071</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.022333</td>\n",
       "      <td>-0.189637</td>\n",
       "      <td>-0.022333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1134</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.263741</td>\n",
       "      <td>0.098249</td>\n",
       "      <td>0.221051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1197</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.178462</td>\n",
       "      <td>-0.332041</td>\n",
       "      <td>-0.174984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1260</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.281056</td>\n",
       "      <td>-0.454244</td>\n",
       "      <td>-0.299522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1323</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.122779</td>\n",
       "      <td>-0.073914</td>\n",
       "      <td>-0.071315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0    126  2017-10-02  2018-01-02        A2C   0.756501   0.104886    0.417703\n",
       "1    189  2018-01-02  2018-04-04        A2C    0.00931  -0.050068   -0.082933\n",
       "2    252  2018-04-04  2018-07-03       DDPG   0.009017  -0.166887    0.074028\n",
       "3    315  2018-07-03  2018-10-02        A2C   0.339836   0.023851    0.316097\n",
       "4    378  2018-10-02  2019-01-03        A2C  -0.286318   -0.43422   -0.289125\n",
       "5    441  2019-01-03  2019-04-04        A2C   0.647303   0.429269    0.624779\n",
       "6    504  2019-04-04  2019-07-05       DDPG   -0.02273  -0.026822    0.114625\n",
       "7    567  2019-07-05  2019-10-03        A2C  -0.121557  -0.218156   -0.151744\n",
       "8    630  2019-10-03  2020-01-03        A2C   0.546106   0.366706    0.115436\n",
       "9    693  2020-01-03  2020-04-03        PPO   -0.08546  -0.047401   -0.080111\n",
       "10   756  2020-04-03  2020-07-06        A2C   0.321712   0.282374    0.319396\n",
       "11   819  2020-07-06  2020-10-02       DDPG   0.186139   0.107357    0.198622\n",
       "12   882  2020-10-02  2021-01-04        PPO   0.400432   0.403017    0.173859\n",
       "13   945  2021-01-04  2021-04-06        A2C   0.217535   0.198004    0.175299\n",
       "14  1008  2021-04-06  2021-07-06       DDPG   0.095621  -0.133131    0.235725\n",
       "15  1071  2021-07-06  2021-10-04       DDPG  -0.022333  -0.189637   -0.022333\n",
       "16  1134  2021-10-04  2022-01-03        A2C   0.263741   0.098249    0.221051\n",
       "17  1197  2022-01-03  2022-04-04       DDPG  -0.178462  -0.332041   -0.174984\n",
       "18  1260  2022-04-04  2022-07-06        A2C  -0.281056  -0.454244   -0.299522\n",
       "19  1323  2022-07-06  2022-10-04       DDPG  -0.122779  -0.073914   -0.071315"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.402930164666884\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp],ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.002954e+06</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.009057e+06</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.017899e+06</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.008763</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.017390e+06</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>-0.000500</td>\n",
       "      <td>2018-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0   1.000000e+06  2018-01-02           NaN  2018-01-02\n",
       "1   1.002954e+06  2018-01-03      0.002954  2018-01-03\n",
       "2   1.009057e+06  2018-01-04      0.006085  2018-01-04\n",
       "3   1.017899e+06  2018-01-05      0.008763  2018-01-05\n",
       "4   1.017390e+06  2018-01-08     -0.000500  2018-01-08"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.059897\n",
      "Cumulative returns     0.337575\n",
      "Annual volatility      0.189177\n",
      "Sharpe ratio           0.402930\n",
      "Calmar ratio           0.201544\n",
      "Stability              0.616096\n",
      "Max drawdown          -0.297190\n",
      "Omega ratio            1.076985\n",
      "Sortino ratio          0.548531\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.851226\n",
      "Daily value at risk   -0.023532\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (1259, 8)\n",
      "Annual return          0.059583\n",
      "Cumulative returns     0.335290\n",
      "Annual volatility      0.217954\n",
      "Sharpe ratio           0.375375\n",
      "Calmar ratio           0.160661\n",
      "Stability              0.685042\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.077914\n",
      "Sortino ratio          0.516270\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.893140\n",
      "Daily value at risk   -0.027135\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_dji_ = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:              date           dji\n",
      "0     2018-01-02  1.000000e+06\n",
      "1     2018-01-03  1.003975e+06\n",
      "2     2018-01-04  1.010116e+06\n",
      "3     2018-01-05  1.019008e+06\n",
      "4     2018-01-08  1.018490e+06\n",
      "...          ...           ...\n",
      "1255  2022-12-27  1.339089e+06\n",
      "1256  2022-12-28  1.324351e+06\n",
      "1257  2022-12-29  1.338253e+06\n",
      "1258  2022-12-30  1.335290e+06\n",
      "1259  2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.003975e+06\n",
      "2018-01-04  1.010116e+06\n",
      "2018-01-05  1.019008e+06\n",
      "2018-01-08  1.018490e+06\n",
      "...                  ...\n",
      "2022-12-27  1.339089e+06\n",
      "2022-12-28  1.324351e+06\n",
      "2022-12-29  1.338253e+06\n",
      "2022-12-30  1.335290e+06\n",
      "2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:          datadate\n",
      "0     2017-10-02\n",
      "1     2017-10-03\n",
      "2     2017-10-04\n",
      "3     2017-10-05\n",
      "4     2017-10-06\n",
      "...          ...\n",
      "1356  2023-02-22\n",
      "1357  2023-02-23\n",
      "1358  2023-02-24\n",
      "1359  2023-02-27\n",
      "1360  2023-02-28\n",
      "\n",
      "[1361 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.002954e+06\n",
      "2018-01-04  1.009057e+06\n",
      "2018-01-05  1.017899e+06\n",
      "2018-01-08  1.017390e+06\n",
      "...                  ...\n",
      "2022-12-27  1.336650e+06\n",
      "2022-12-28  1.315640e+06\n",
      "2022-12-29  1.349432e+06\n",
      "2022-12-30  1.345612e+06\n",
      "2023-01-03  1.337575e+06\n",
      "\n",
      "[1260 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2018-01-02  1.000000e+06  1.000000e+06\n",
      "2018-01-03  1.002954e+06  1.003975e+06\n",
      "2018-01-04  1.009057e+06  1.010116e+06\n",
      "2018-01-05  1.017899e+06  1.019008e+06\n",
      "2018-01-08  1.017390e+06  1.018490e+06\n",
      "...                  ...           ...\n",
      "2022-12-27  1.336650e+06  1.339089e+06\n",
      "2022-12-28  1.315640e+06  1.324351e+06\n",
      "2022-12-29  1.349432e+06  1.338253e+06\n",
      "2022-12-30  1.345612e+06  1.335290e+06\n",
      "2023-01-03  1.337575e+06           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value, \n",
    "#               baseline_ticker = '^DJI', \n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
