{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrds in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.26.4)\n",
      "Requirement already satisfied: packaging<23.3 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (23.2)\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.2.2)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.9.9)\n",
      "Requirement already satisfied: scipy<1.13,>=1.12 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.0.29)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (4.2.1)\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n"
     ]
    }
   ],
   "source": [
    "# ## install finrl library\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_5_TICKER = [\n",
    "    \"AXP\",\n",
    "    \"AMGN\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "    \"CAT\",\n",
    "]\n",
    "INDEX_5_TICKER = [\n",
    "    \"^DJI\", \n",
    "    \"^IXIC\", \n",
    "    \"^NYA\", \n",
    "    \"^RUT\", \n",
    "    \"^GSPC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "\n",
    "# # TRAIN_START_DATE = '2009-04-01'\n",
    "# # TRAIN_END_DATE = '2021-01-01'\n",
    "# # TEST_START_DATE = '2021-01-01'\n",
    "# # TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "\n",
    "# TRAIN_START_DATE = '2009-06-01'\n",
    "# #TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "# dfexport = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = DOW_30_TICKER).fetch_data()\n",
    "\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfexport.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data export\n",
    "# import pickle\n",
    "# datasetName = \"dailydata\"\n",
    "# datasetDir = \"./datasets\"\n",
    "\n",
    "# os.makedirs(datasetDir, exist_ok=True)\n",
    "# datasetPath = os.path.join(datasetDir, datasetName) + \".pkl\"\n",
    "\n",
    "\n",
    "# with open(datasetPath, 'wb') as file:\n",
    "#     pickle.dump(dfexport, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (16555, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "#TRAIN_START_DATE = '2000-01-01'\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2017-10-01'\n",
    "TEST_START_DATE = '2017-10-01'\n",
    "TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = INDEX_5_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Andrew Martin - UNCOMMENT BELOW TO ADD PREDICTION INDICATOR\n",
    "# import pickle\n",
    "# with open(\"./datasets/index_5_predictor_2.pkl\", 'rb') as file:\n",
    "#   df_prob = pickle.load(file)\n",
    "# df6 = df_prob.copy()\n",
    "# df6 = df6.loc[:, ~df6.columns.duplicated(keep='first')]\n",
    "# df6[\"date\"] = df6[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "# df2 = processed.merge(df6[['tic', 'date', 'Predicted_Target']], on=['tic', 'date'], how='left')\n",
    "# processed = df2.copy()\n",
    "# INDICATORS.append(\"Predicted_Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 5, State Space: 51\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2017-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 412         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.0425     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -8.35       |\n",
      "|    reward             | -0.10028748 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 2.29        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 445        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.202     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 1          |\n",
      "|    reward             | 0.97078323 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.715      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -5.21      |\n",
      "|    reward             | -1.2696742 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 467        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 11.2       |\n",
      "|    reward             | 0.47865936 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.96       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 473       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -0.515    |\n",
      "|    reward             | 0.7436869 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.178     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 473         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 1.69        |\n",
      "|    reward             | -0.63822013 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.43        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 477      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -3.01    |\n",
      "|    reward             | 0.550029 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.64     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 483       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -42.1     |\n",
      "|    reward             | 3.5337622 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 40        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 486       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -15.2     |\n",
      "|    reward             | 2.750595  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.99      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 487       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 13.9      |\n",
      "|    reward             | 1.5109367 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 489        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -0.316     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -22.3      |\n",
      "|    reward             | -1.0771759 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.1       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 488       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 4.47      |\n",
      "|    reward             | 0.7473467 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 490         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -20.3       |\n",
      "|    reward             | -0.83805656 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 12.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 490       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 11        |\n",
      "|    reward             | 0.5360536 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.03      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 490        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 6.5        |\n",
      "|    reward             | -0.1968118 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 1.58       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 491      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 1.57     |\n",
      "|    reward             | -1.83942 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 0.144    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 491        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.967     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 4.94       |\n",
      "|    reward             | 0.06975083 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 0.616      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 493        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | -2.7027314 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 5.41       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 494         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -3.63       |\n",
      "|    reward             | -0.53165245 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 0.963       |\n",
      "---------------------------------------\n",
      "day: 1949, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1290117.57\n",
      "total_reward: 290117.57\n",
      "total_cost: 214809.72\n",
      "total_trades: 4768\n",
      "Sharpe: 0.293\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 496         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -5.44       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 1.6         |\n",
      "|    reward             | -0.15588856 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.457       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2017-10-02 to  2018-01-02\n",
      "A2C Sharpe Ratio:  0.2897862210587806\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 728         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 2           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.12386204 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 681          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053825267 |\n",
      "|    clip_fraction        | 0.039        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0206      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00449     |\n",
      "|    reward               | 0.01881231   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.43         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 664          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061866017 |\n",
      "|    clip_fraction        | 0.0588       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.0254       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.99         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00686     |\n",
      "|    reward               | -0.40341836  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.39         |\n",
      "------------------------------------------\n",
      "day: 1949, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 533404.72\n",
      "total_reward: -466595.28\n",
      "total_cost: 800206.10\n",
      "total_trades: 7511\n",
      "Sharpe: -0.488\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 648          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076250117 |\n",
      "|    clip_fraction        | 0.067        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.0188       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00647     |\n",
      "|    reward               | -0.95364654  |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.85         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 641          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069430824 |\n",
      "|    clip_fraction        | 0.06         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.00571     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    reward               | -0.21697846  |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.48         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2017-10-02 to  2018-01-02\n",
      "PPO Sharpe Ratio:  0.12385737497142728\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_3\n",
      "day: 1949, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1903110.05\n",
      "total_reward: 903110.05\n",
      "total_cost: 998.97\n",
      "total_trades: 9744\n",
      "Sharpe: 0.613\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 229       |\n",
      "|    time_elapsed    | 34        |\n",
      "|    total_timesteps | 7800      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 2.3e+03   |\n",
      "|    critic_loss     | 733       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7699      |\n",
      "|    reward          | 0.6722778 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2017-10-02 to  2018-01-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-01-02\n",
      "======Trading from:  2018-01-02 to  2018-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-01-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 473         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -1.36       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -6.77       |\n",
      "|    reward             | 0.085798435 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 1.47        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 476       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.8      |\n",
      "|    reward             | 0.6827051 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 486        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.56      |\n",
      "|    reward             | -2.1015356 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.67       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.36      |\n",
      "|    reward             | 0.39102325 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.488      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 505       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 38.5      |\n",
      "|    reward             | 0.2523812 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 32.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 507        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 1.81       |\n",
      "|    reward             | -0.2314032 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 0.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 506        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 13.5       |\n",
      "|    reward             | -0.6500122 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 5.4        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 509         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -12.4       |\n",
      "|    reward             | -0.06872551 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 3.03        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 513        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -11.2      |\n",
      "|    reward             | -1.7183839 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 515       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -10.8     |\n",
      "|    reward             | 0.7154596 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.63      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | 0.2606111 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 518        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 6.76       |\n",
      "|    reward             | 0.48217592 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 520        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 23.3       |\n",
      "|    reward             | -2.8580534 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 522       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 15.9      |\n",
      "|    reward             | 0.1628606 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 524       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 8.28      |\n",
      "|    reward             | 2.8810892 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.92      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 525        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 2.23       |\n",
      "|    reward             | 0.15140066 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.371      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 526       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 4.59      |\n",
      "|    reward             | 1.0175656 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 528        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -1.09      |\n",
      "|    reward             | 0.08141161 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.326      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 528       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -41.6     |\n",
      "|    reward             | 3.0435922 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 31.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 529        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -3.99      |\n",
      "|    reward             | 0.24824561 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.312      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-01-02 to  2018-04-04\n",
      "A2C Sharpe Ratio:  0.016572922504236153\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 725        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 2          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.75561696 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 692          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 5            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058645657 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.00736      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.84         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00633     |\n",
      "|    reward               | -0.05875053  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.69         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 678          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056186086 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.017        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00475     |\n",
      "|    reward               | -3.5003126   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.14         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009220548 |\n",
      "|    clip_fraction        | 0.073       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.00665     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00556    |\n",
      "|    reward               | 1.0380887   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "day: 2012, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 779622.88\n",
      "total_reward: -220377.12\n",
      "total_cost: 829127.71\n",
      "total_trades: 7948\n",
      "Sharpe: -0.136\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007728269 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.034       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.89        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00498    |\n",
      "|    reward               | -0.14181246 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-01-02 to  2018-04-04\n",
      "PPO Sharpe Ratio:  -0.13934905607876735\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_3\n",
      "day: 2012, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2340056.00\n",
      "total_reward: 1340056.00\n",
      "total_cost: 998.35\n",
      "total_trades: 2012\n",
      "Sharpe: 0.798\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 220       |\n",
      "|    time_elapsed    | 36        |\n",
      "|    total_timesteps | 8052      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.41e+03 |\n",
      "|    critic_loss     | 4.23e+03  |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 7951      |\n",
      "|    reward          | -1.218869 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2018-01-02 to  2018-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-04-04\n",
      "======Trading from:  2018-04-04 to  2018-07-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 495         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.139      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -13.3       |\n",
      "|    reward             | 0.028039038 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 3.28        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.53     |\n",
      "|    reward             | 0.7974864 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 493       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -9.3      |\n",
      "|    reward             | -2.549694 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.86      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 492        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.53      |\n",
      "|    reward             | 0.33668554 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.954      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 491        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 17.2       |\n",
      "|    reward             | -1.2182063 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 490        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -10.5      |\n",
      "|    reward             | 0.26312342 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 491        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0.00038    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -76.1      |\n",
      "|    reward             | -0.9296482 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 136        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 492         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -14.5       |\n",
      "|    reward             | -0.16880266 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.56        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 493         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0.00819     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.36       |\n",
      "|    reward             | -0.95792884 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.78        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 494      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.15    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 3.87     |\n",
      "|    reward             | 1.177558 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.423    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 495         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.18       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -8.84       |\n",
      "|    reward             | 0.051706053 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.98        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 496        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -4.32      |\n",
      "|    reward             | -0.2752599 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.638      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 495       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.19     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.381    |\n",
      "|    reward             | 2.0590847 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.373     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 495       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 6.16      |\n",
      "|    reward             | 0.7588644 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.965     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 496         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.18       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -20.6       |\n",
      "|    reward             | -0.96926457 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 6.91        |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| time/                 |                |\n",
      "|    fps                | 496            |\n",
      "|    iterations         | 1600           |\n",
      "|    time_elapsed       | 16             |\n",
      "|    total_timesteps    | 8000           |\n",
      "| train/                |                |\n",
      "|    entropy_loss       | -7.2           |\n",
      "|    explained_variance | 1.19e-07       |\n",
      "|    learning_rate      | 0.0007         |\n",
      "|    n_updates          | 1599           |\n",
      "|    policy_loss        | -5.16          |\n",
      "|    reward             | -0.00089785154 |\n",
      "|    std                | 1.02           |\n",
      "|    value_loss         | 0.681          |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 496        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 10.6       |\n",
      "|    reward             | 0.97989744 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 495        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -8.53      |\n",
      "|    reward             | 0.10517319 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.71       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 496      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -8.69    |\n",
      "|    reward             | 1.084817 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.93     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 495       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 13.1      |\n",
      "|    reward             | 1.1994448 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.46      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2018-04-04 to  2018-07-03\n",
      "A2C Sharpe Ratio:  0.008940101656483967\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_3\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 702          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 2            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.013040682 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074295085 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.0317      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.19         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00431     |\n",
      "|    reward               | 0.5434423    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.39         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 632          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067384713 |\n",
      "|    clip_fraction        | 0.0938       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.00649      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.88         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00698     |\n",
      "|    reward               | 0.33786923   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.22         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 626         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006804362 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.0144      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00582    |\n",
      "|    reward               | 0.44891772  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063311653 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.016        |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.35         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00525     |\n",
      "|    reward               | 0.20138316   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.95         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-04-04 to  2018-07-03\n",
      "PPO Sharpe Ratio:  -0.037861328703169224\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 228       |\n",
      "|    time_elapsed    | 36        |\n",
      "|    total_timesteps | 8304      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.7e+03  |\n",
      "|    critic_loss     | 2.71e+03  |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8203      |\n",
      "|    reward          | 3.0783772 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2018-04-04 to  2018-07-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-07-03\n",
      "======Trading from:  2018-07-03 to  2018-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-07-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 513        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.0234    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -9.51      |\n",
      "|    reward             | 0.43528783 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 3.51       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0.126     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 4.34      |\n",
      "|    reward             | 1.4222323 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 510        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0.0658     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.98      |\n",
      "|    reward             | -1.9979812 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 2.72       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 510         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | -1.19       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -18.5       |\n",
      "|    reward             | -0.29641104 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 6.75        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 510       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0.0222    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -12.1     |\n",
      "|    reward             | 0.7984766 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 509        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0.412      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.97      |\n",
      "|    reward             | 0.91579443 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 0.0956     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 509        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -12.8      |\n",
      "|    reward             | 0.23350933 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 4.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 510        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -0.00238   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | 0.23555957 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 3.57       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 508       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -0.0385   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -4.6      |\n",
      "|    reward             | 1.7709323 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.852     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 508        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -15.6      |\n",
      "|    reward             | -1.6941309 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 5.18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 500        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 17.3       |\n",
      "|    reward             | 0.35829493 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 8.3        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 501         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0.0739      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -6.85       |\n",
      "|    reward             | -0.25976372 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 1.24        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 501        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -13.2      |\n",
      "|    reward             | -2.2914915 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 3.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 500        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -5.09      |\n",
      "|    reward             | 0.89362216 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 499         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 4.49        |\n",
      "|    reward             | -0.28827074 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 5.46        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 6.33       |\n",
      "|    reward             | 0.14832383 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 499        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | -1.0083824 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 7.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 499       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 7.22      |\n",
      "|    reward             | 2.0761678 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 3.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -5.12      |\n",
      "|    reward             | -1.1945572 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -23.4      |\n",
      "|    reward             | 0.28031346 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-07-03 to  2018-10-02\n",
      "A2C Sharpe Ratio:  0.476940942178433\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_3\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 676          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 3            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.011328699 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063465405 |\n",
      "|    clip_fraction        | 0.0443       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.00489     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.11         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00423     |\n",
      "|    reward               | 0.139828     |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 2.81         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 628          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060965423 |\n",
      "|    clip_fraction        | 0.0667       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.00731     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.42         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00663     |\n",
      "|    reward               | 0.13850385   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 4.24         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057343394 |\n",
      "|    clip_fraction        | 0.0471       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | 0.00226      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.97         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00449     |\n",
      "|    reward               | -0.1439738   |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 3.87         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 620         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006778351 |\n",
      "|    clip_fraction        | 0.0637      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.05       |\n",
      "|    explained_variance   | 0.000417    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00694    |\n",
      "|    reward               | 1.0624088   |\n",
      "|    std                  | 0.99        |\n",
      "|    value_loss           | 3.62        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-07-03 to  2018-10-02\n",
      "PPO Sharpe Ratio:  0.2810745640042402\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 220        |\n",
      "|    time_elapsed    | 38         |\n",
      "|    total_timesteps | 8556       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 772        |\n",
      "|    critic_loss     | 22         |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8455       |\n",
      "|    reward          | 0.33873555 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-07-03 to  2018-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-10-02\n",
      "======Trading from:  2018-10-02 to  2019-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_378_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 506        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 0          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.00113   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -9.83      |\n",
      "|    reward             | 0.23448002 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.42     |\n",
      "|    reward             | 0.6042413 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 1.07      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 499        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.0287     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.66      |\n",
      "|    reward             | -1.8885125 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 501         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -13.3       |\n",
      "|    reward             | -0.17334375 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.41        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 496        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 9.82       |\n",
      "|    reward             | -2.5665202 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 2.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 496        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 4.8        |\n",
      "|    reward             | 0.34517804 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 5.54      |\n",
      "|    reward             | -0.915492 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.662     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 8.36       |\n",
      "|    reward             | 0.02297268 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 499       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -6.24     |\n",
      "|    reward             | 1.5242952 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 7.06      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 500         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -11.7       |\n",
      "|    reward             | -0.23816088 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.03        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 500        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -17.9      |\n",
      "|    reward             | -1.7555991 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 7.68       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -16       |\n",
      "|    reward             | 0.9958699 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 6.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 496       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -22.8     |\n",
      "|    reward             | 0.6746455 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 9.95      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 496        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 3.8        |\n",
      "|    reward             | -3.6012583 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 491        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 5.72       |\n",
      "|    reward             | -1.2404333 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 488        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 26.3       |\n",
      "|    reward             | -1.0181333 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 11.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -23.8      |\n",
      "|    reward             | 0.19860102 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 486         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 2.42        |\n",
      "|    reward             | -0.16764535 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 1.41        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -6.7      |\n",
      "|    reward             | 0.4272818 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 486         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -23.4       |\n",
      "|    reward             | -0.21213633 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 13.2        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2018-10-02 to  2019-01-03\n",
      "A2C Sharpe Ratio:  -0.35152208351677244\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_378_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 671        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 3          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.30138752 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 581         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006710867 |\n",
      "|    clip_fraction        | 0.055       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0157     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00437    |\n",
      "|    reward               | 0.07551603  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 569         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005622065 |\n",
      "|    clip_fraction        | 0.0633      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.0068      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00633    |\n",
      "|    reward               | -0.11096309 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 573          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072804545 |\n",
      "|    clip_fraction        | 0.0406       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.0126       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0044      |\n",
      "|    reward               | 0.14524654   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.39         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006766211 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0226      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.43        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00597    |\n",
      "|    reward               | 0.4580855   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-10-02 to  2019-01-03\n",
      "PPO Sharpe Ratio:  -0.38334464187089823\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 217        |\n",
      "|    time_elapsed    | 40         |\n",
      "|    total_timesteps | 8808       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -141       |\n",
      "|    critic_loss     | 1.93       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8707       |\n",
      "|    reward          | 0.38201723 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-10-02 to  2019-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-01-03\n",
      "======Trading from:  2019-01-03 to  2019-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_441_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 487          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.11        |\n",
      "|    explained_variance | -0.0283      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -8.51        |\n",
      "|    reward             | -0.012506249 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 2.1          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 477       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.01     |\n",
      "|    reward             | 0.7168216 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 478        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.38      |\n",
      "|    reward             | -2.2170167 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 479       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -5.11     |\n",
      "|    reward             | 0.2497275 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.572     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 9.34      |\n",
      "|    reward             | 0.0968983 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.96      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 476          |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 0.0299       |\n",
      "|    reward             | -0.037106566 |\n",
      "|    std                | 0.997        |\n",
      "|    value_loss         | 0.385        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 1.15       |\n",
      "|    reward             | -1.0104114 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.0551     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 7.43       |\n",
      "|    reward             | 0.30646876 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 480       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.607     |\n",
      "|    reward             | 0.4230991 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 1.94      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 482       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.244     |\n",
      "|    reward             | 2.091642  |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 4.76      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 484       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -5.54     |\n",
      "|    reward             | 1.5260011 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.735     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 484         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 12          |\n",
      "|    reward             | -0.62195545 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 2.55        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 484         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -0.101      |\n",
      "|    reward             | -0.62926346 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 0.104       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 485         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -12         |\n",
      "|    reward             | -0.21188906 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 3.05        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 485          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 15           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.1         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | -7.29        |\n",
      "|    reward             | -0.052132655 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 2.08         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 485        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -21        |\n",
      "|    reward             | 0.19094454 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 7.93       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -6.37     |\n",
      "|    reward             | -1.828134 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 485        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -5.17      |\n",
      "|    reward             | -1.4037108 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.8        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 486       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -34       |\n",
      "|    reward             | -2.756858 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 36.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 486        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 4.65       |\n",
      "|    reward             | -0.6241517 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.28       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-01-03 to  2019-04-04\n",
      "A2C Sharpe Ratio:  0.5536410235648469\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_441_3\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 647          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 3            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.088200934 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008446839 |\n",
      "|    clip_fraction        | 0.0855      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0084     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.63        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00812    |\n",
      "|    reward               | -0.28901905 |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 4.14        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066377493 |\n",
      "|    clip_fraction        | 0.0602       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.00136      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.62         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00452     |\n",
      "|    reward               | 0.01659816   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.22         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 601         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007538829 |\n",
      "|    clip_fraction        | 0.104       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.00843     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.11        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00878    |\n",
      "|    reward               | -0.74970084 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.57        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053304094 |\n",
      "|    clip_fraction        | 0.0503       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | 0.00556      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.98         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00446     |\n",
      "|    reward               | 0.07975597   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.44         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-01-03 to  2019-04-04\n",
      "PPO Sharpe Ratio:  0.4672351012356493\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 222       |\n",
      "|    time_elapsed    | 40        |\n",
      "|    total_timesteps | 9060      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 247       |\n",
      "|    critic_loss     | 0.574     |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8959      |\n",
      "|    reward          | 1.0539858 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-01-03 to  2019-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-04-04\n",
      "======Trading from:  2019-04-04 to  2019-07-05\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_504_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.2      |\n",
      "|    reward             | 0.17976303 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 2.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 492       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -1.26     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 18.1      |\n",
      "|    reward             | 1.1919615 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 6.87      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 480        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.05      |\n",
      "|    reward             | -1.8857038 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 2.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -6.61      |\n",
      "|    reward             | 0.27651617 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.57       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 478        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 13.7       |\n",
      "|    reward             | 0.27769703 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 3.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 480        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -3.41      |\n",
      "|    reward             | 0.77520835 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.989      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 5.45       |\n",
      "|    reward             | 0.34057534 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 1.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -1.98      |\n",
      "|    reward             | -0.6525388 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 0.268      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 476       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -17       |\n",
      "|    reward             | 0.2909399 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 5.72      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 0.37       |\n",
      "|    reward             | -1.5501268 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.705      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 14.9       |\n",
      "|    reward             | 0.08508993 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 6.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 477       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -35.6     |\n",
      "|    reward             | 1.0756692 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 25.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 28.9       |\n",
      "|    reward             | 0.15725496 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 24.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 478       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -6.51     |\n",
      "|    reward             | 0.6524723 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.09      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 478        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 7.94       |\n",
      "|    reward             | 0.16831882 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 478          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 1.79e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | 0.912        |\n",
      "|    reward             | -0.023987353 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 0.321        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -32.6     |\n",
      "|    reward             | 1.7681866 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 34.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 474       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 0.956     |\n",
      "|    reward             | 2.4718363 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 0.148     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 474       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 9.2       |\n",
      "|    reward             | 0.2743503 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 475        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 4.78       |\n",
      "|    reward             | -1.3154925 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.59       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-04-04 to  2019-07-05\n",
      "A2C Sharpe Ratio:  0.10048776666353719\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_504_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 650         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.21728003 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 612         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006505925 |\n",
      "|    clip_fraction        | 0.0618      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.017      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00746    |\n",
      "|    reward               | 0.11515769  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 606        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00798145 |\n",
      "|    clip_fraction        | 0.0659     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.11      |\n",
      "|    explained_variance   | 0.00726    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.8        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00645   |\n",
      "|    reward               | 0.38658786 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 3.58       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065041026 |\n",
      "|    clip_fraction        | 0.0481       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.00413     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00594     |\n",
      "|    reward               | 1.261858     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.05         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 600          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074711256 |\n",
      "|    clip_fraction        | 0.0693       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.00455     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.26         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00358     |\n",
      "|    reward               | 0.65435785   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.4          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-04-04 to  2019-07-05\n",
      "PPO Sharpe Ratio:  0.028792187448686328\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 218       |\n",
      "|    time_elapsed    | 42        |\n",
      "|    total_timesteps | 9312      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.7e+03   |\n",
      "|    critic_loss     | 152       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9211      |\n",
      "|    reward          | 1.6860956 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-04-04 to  2019-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-07-05\n",
      "======Trading from:  2019-07-05 to  2019-10-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_567_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 462          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -11.9        |\n",
      "|    reward             | -0.022420598 |\n",
      "|    std                | 0.992        |\n",
      "|    value_loss         | 2.99         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 461       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.123     |\n",
      "|    reward             | 1.0668358 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 466        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.0227    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.55      |\n",
      "|    reward             | -2.0425653 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 470        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.135     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.86      |\n",
      "|    reward             | 0.36950684 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.505      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 471       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 9.08      |\n",
      "|    reward             | 2.5900493 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.11      |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 474           |\n",
      "|    iterations         | 600           |\n",
      "|    time_elapsed       | 6             |\n",
      "|    total_timesteps    | 3000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -7.09         |\n",
      "|    explained_variance | -2.38e-07     |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 599           |\n",
      "|    policy_loss        | 0.894         |\n",
      "|    reward             | -0.0016115616 |\n",
      "|    std                | 1             |\n",
      "|    value_loss         | 0.537         |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 472        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 7.5        |\n",
      "|    reward             | 0.11811734 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.77       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 472       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -9.13     |\n",
      "|    reward             | 1.2875663 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 472       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 9.08      |\n",
      "|    reward             | 1.4857413 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 473        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.576     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 8.07       |\n",
      "|    reward             | -1.4759679 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.94       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 474        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 8.06       |\n",
      "|    reward             | -1.2620695 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 473         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0.0598      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 17.9        |\n",
      "|    reward             | -0.49755657 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 5.42        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 474         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -1.72       |\n",
      "|    reward             | -0.06681355 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.116       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 9.22      |\n",
      "|    reward             | -4.698797 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 4.15      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -9.96     |\n",
      "|    reward             | 1.6923921 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.08      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 475         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 15.5        |\n",
      "|    reward             | -0.81500787 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.12        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 476       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 0.151     |\n",
      "|    reward             | 0.2683793 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.644     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 471      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.13    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 8.32     |\n",
      "|    reward             | 0.350358 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.45     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 472       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -10.5     |\n",
      "|    reward             | 0.7730304 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 472       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 5.73      |\n",
      "|    reward             | 2.2650177 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.87      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2019-07-05 to  2019-10-03\n",
      "A2C Sharpe Ratio:  -0.13606104084113282\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_567_3\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 628        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 3          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.28585234 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 599          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050497395 |\n",
      "|    clip_fraction        | 0.0541       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0255      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.31         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00521     |\n",
      "|    reward               | -1.1298436   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.54         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 588          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051657483 |\n",
      "|    clip_fraction        | 0.0327       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.00418     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.39         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00427     |\n",
      "|    reward               | 0.32530972   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.9          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 585         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005109356 |\n",
      "|    clip_fraction        | 0.0417      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.00466    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00431    |\n",
      "|    reward               | -0.16751592 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 583         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006894391 |\n",
      "|    clip_fraction        | 0.0827      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00308    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    reward               | 0.3811258   |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 3.88        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-07-05 to  2019-10-03\n",
      "PPO Sharpe Ratio:  -0.3128787131236871\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 218       |\n",
      "|    time_elapsed    | 43        |\n",
      "|    total_timesteps | 9564      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 316       |\n",
      "|    critic_loss     | 163       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9463      |\n",
      "|    reward          | 1.6856109 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-07-05 to  2019-10-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-10-03\n",
      "======Trading from:  2019-10-03 to  2020-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-10-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_630_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 474        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.866     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -5.02      |\n",
      "|    reward             | -0.1021897 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 479       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0.238     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.669     |\n",
      "|    reward             | 0.2776973 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 0.153     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 478        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.21      |\n",
      "|    reward             | -1.3837708 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 1.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.25      |\n",
      "|    reward             | 0.24563275 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.197      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 477       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 6.63      |\n",
      "|    reward             | 0.3444942 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 477         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 1.27        |\n",
      "|    reward             | -0.44713598 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.0857      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 4.14       |\n",
      "|    reward             | -1.0180615 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.51       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 476       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 17.9      |\n",
      "|    reward             | 1.6311612 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 15        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 476        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -0.619     |\n",
      "|    reward             | -1.5113893 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 6.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.00828   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -0.732     |\n",
      "|    reward             | 0.18480062 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 16         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 474        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -0.307     |\n",
      "|    reward             | -0.6029151 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 0.377      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 474        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -5.78      |\n",
      "|    reward             | 0.07722559 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 4.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 473       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -3.95     |\n",
      "|    reward             | 1.4349847 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 0.574     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 472       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 4.61      |\n",
      "|    reward             | 1.0454842 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 3.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 473       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.964     |\n",
      "|    reward             | 3.4808109 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.98      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 475         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -7.42       |\n",
      "|    reward             | -0.64975286 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 2.04        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -0.179    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -12.1     |\n",
      "|    reward             | 1.0453886 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 4.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 476        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0.00137    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -7.86      |\n",
      "|    reward             | 0.17144011 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 13.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 477       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -27.7     |\n",
      "|    reward             | 1.3718549 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 20.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 477        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 17.6       |\n",
      "|    reward             | -0.4068948 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 6.35       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-10-03 to  2020-01-03\n",
      "A2C Sharpe Ratio:  0.4814323988635954\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_630_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 647       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.4707378 |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.0044921   |\n",
      "|    clip_fraction        | 0.036       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0277     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00383    |\n",
      "|    reward               | -0.09160607 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 3.91        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 593          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074976273 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.000364     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.67         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00297     |\n",
      "|    reward               | -0.6333363   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.03         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 588         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005213633 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.0184      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.43        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    reward               | 1.0288161   |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 590         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007976063 |\n",
      "|    clip_fraction        | 0.075       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.0128      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.49        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00813    |\n",
      "|    reward               | -1.0034856  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.51        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-10-03 to  2020-01-03\n",
      "PPO Sharpe Ratio:  0.5515713618627259\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 218       |\n",
      "|    time_elapsed    | 44        |\n",
      "|    total_timesteps | 9816      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 1.47e+03  |\n",
      "|    critic_loss     | 213       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9715      |\n",
      "|    reward          | -4.635951 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-10-03 to  2020-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-01-03\n",
      "======Trading from:  2020-01-03 to  2020-04-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_693_3\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 485         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.0288     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -9.53       |\n",
      "|    reward             | -0.11345655 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 2.77        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 478        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.0406    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -2.57      |\n",
      "|    reward             | 0.48359168 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 0.96       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -0.257     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.19      |\n",
      "|    reward             | -1.3260597 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.74       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 475         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -0.314      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -5.18       |\n",
      "|    reward             | -0.15958089 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.883       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 475         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -4.01       |\n",
      "|    reward             | -0.19286473 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 0.429       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 478         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -34.4       |\n",
      "|    reward             | -0.45312867 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 20.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -5.65      |\n",
      "|    reward             | 0.19781768 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 0.706      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 482         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | -0.0391     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -3.13       |\n",
      "|    reward             | -0.29730245 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 2.74        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 484       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.65     |\n",
      "|    reward             | 1.0845315 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.503     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 485          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 10           |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.04        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 5.58         |\n",
      "|    reward             | -0.033476055 |\n",
      "|    std                | 0.99         |\n",
      "|    value_loss         | 1.13         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -5.91      |\n",
      "|    reward             | 0.49357152 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 5.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 487       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 1.2       |\n",
      "|    reward             | 0.5836915 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 0.158     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 4.8        |\n",
      "|    reward             | -0.5351505 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 0.877      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 486        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -1.21      |\n",
      "|    reward             | 0.26865903 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 0.0576     |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 487       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 10.9      |\n",
      "|    reward             | 1.0738599 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 4.78      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 485        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 16.2       |\n",
      "|    reward             | -0.4681926 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 10.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -7.39     |\n",
      "|    reward             | 2.6704476 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 484         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -19.7       |\n",
      "|    reward             | -0.15663536 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 10.3        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 483       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 1.09      |\n",
      "|    reward             | 0.4033969 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.0852    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 482         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -2.34       |\n",
      "|    reward             | -0.12005671 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 0.147       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2020-01-03 to  2020-04-03\n",
      "A2C Sharpe Ratio:  0.0015118199387197278\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_693_3\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 647          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 3            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.017729186 |\n",
      "-------------------------------------\n",
      "day: 2516, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 639984.91\n",
      "total_reward: -360015.09\n",
      "total_cost: 1142324.05\n",
      "total_trades: 9759\n",
      "Sharpe: -0.245\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 611          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068079443 |\n",
      "|    clip_fraction        | 0.0681       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0103      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.74         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00645     |\n",
      "|    reward               | 1.0691041    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 600          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053908033 |\n",
      "|    clip_fraction        | 0.0478       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.00293     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.6          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00394     |\n",
      "|    reward               | -0.29609096  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 4.7          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 597         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007486291 |\n",
      "|    clip_fraction        | 0.0759      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.00432     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.35        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00734    |\n",
      "|    reward               | 0.19768563  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.48        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 591         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008092935 |\n",
      "|    clip_fraction        | 0.0801      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0114      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00396    |\n",
      "|    reward               | 0.049215037 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.22        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-01-03 to  2020-04-03\n",
      "PPO Sharpe Ratio:  -0.27543149964620917\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_3\n",
      "day: 2516, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2737001.33\n",
      "total_reward: 1737001.33\n",
      "total_cost: 46105.66\n",
      "total_trades: 2847\n",
      "Sharpe: 0.757\n",
      "=================================\n",
      "======DDPG Validation from:  2020-01-03 to  2020-04-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-04-03\n",
      "======Trading from:  2020-04-03 to  2020-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-04-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_756_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 466        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.0343    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -8.66      |\n",
      "|    reward             | 0.36682445 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.322     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -2.13      |\n",
      "|    reward             | 0.44475007 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.35       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 458        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.01      |\n",
      "|    reward             | -1.2929649 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.853      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 459         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | -0.316      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -3.11       |\n",
      "|    reward             | -0.22520733 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.426       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 462         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -9.03       |\n",
      "|    reward             | -0.32941556 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.55        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 3.86       |\n",
      "|    reward             | -2.3082278 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.02       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 463       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 2.24      |\n",
      "|    reward             | 0.7348302 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.535     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 2.18       |\n",
      "|    reward             | -1.6878592 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.323      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 465      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -12      |\n",
      "|    reward             | -0.6645  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 4        |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 13.4      |\n",
      "|    reward             | 0.8627011 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 3.49      |\n",
      "|    reward             | 0.7928588 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.483     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 458        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 5.95       |\n",
      "|    reward             | 0.85004604 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 0.841      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 3.96      |\n",
      "|    reward             | 1.7204971 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.93      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 457         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -13.6       |\n",
      "|    reward             | -0.13876098 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 3.51        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 6.62      |\n",
      "|    reward             | 1.4804071 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.17      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 458         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 1.37        |\n",
      "|    reward             | -0.22170234 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.244       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 16.6      |\n",
      "|    reward             | 0.6905566 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 9.22      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 10.8      |\n",
      "|    reward             | -2.903784 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.75      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 459        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 4.97       |\n",
      "|    reward             | 0.34052914 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.89       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 461      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -48.3    |\n",
      "|    reward             | 9.112763 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 63.6     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-04-03 to  2020-07-06\n",
      "A2C Sharpe Ratio:  0.32580565423477065\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_756_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 657         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.17682801 |\n",
      "------------------------------------\n",
      "day: 2579, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 311545.54\n",
      "total_reward: -688454.46\n",
      "total_cost: 1052456.08\n",
      "total_trades: 9811\n",
      "Sharpe: -0.596\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 608          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069380663 |\n",
      "|    clip_fraction        | 0.0612       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.164       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00343     |\n",
      "|    reward               | 0.5302079    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.64         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 588          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007415831  |\n",
      "|    clip_fraction        | 0.0446       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0122      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.87         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00447     |\n",
      "|    reward               | -0.030117778 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.71         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 577         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009250466 |\n",
      "|    clip_fraction        | 0.0959      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.00752     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00809    |\n",
      "|    reward               | 1.7376317   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00547739  |\n",
      "|    clip_fraction        | 0.0651      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00474    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.5         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00465    |\n",
      "|    reward               | -0.21410774 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.98        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-04-03 to  2020-07-06\n",
      "PPO Sharpe Ratio:  0.29945756539817725\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_3\n",
      "day: 2579, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1512609.31\n",
      "total_reward: 512609.31\n",
      "total_cost: 998.81\n",
      "total_trades: 7737\n",
      "Sharpe: 0.318\n",
      "=================================\n",
      "======DDPG Validation from:  2020-04-03 to  2020-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-07-06\n",
      "======Trading from:  2020-07-06 to  2020-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_819_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 462          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.01        |\n",
      "|    explained_variance | -0.346       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -9.5         |\n",
      "|    reward             | -0.022654854 |\n",
      "|    std                | 0.984        |\n",
      "|    value_loss         | 2.69         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 459        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0.233      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.46      |\n",
      "|    reward             | 0.98059475 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 459        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.24      |\n",
      "|    reward             | -1.6135472 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 2.35       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 464         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -7.15       |\n",
      "|    reward             | -0.16052587 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 1.38        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 466        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.82      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -10.5      |\n",
      "|    reward             | -0.4008482 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 467        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -1.85      |\n",
      "|    reward             | -1.3675299 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.38       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 469        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0.171      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -4.24      |\n",
      "|    reward             | -1.6462836 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.415      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 471       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 5.2       |\n",
      "|    reward             | 0.9892496 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 470       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.32     |\n",
      "|    reward             | 1.1048244 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 0.727     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 470       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -19.5     |\n",
      "|    reward             | 1.3047338 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 468         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -1.98       |\n",
      "|    reward             | -0.24555779 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 0.229       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 466        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -14.3      |\n",
      "|    reward             | -0.9443814 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 6.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 465        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 10.5       |\n",
      "|    reward             | -0.3670518 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 4.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 463        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -6.82      |\n",
      "|    reward             | 0.22570856 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -55.6      |\n",
      "|    reward             | -1.9524522 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 49.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 3.8        |\n",
      "|    reward             | -1.8270986 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.423      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 464       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 4.02      |\n",
      "|    reward             | -2.057027 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.733     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 12.6       |\n",
      "|    reward             | -1.4914747 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 2.66       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | 0.17122188 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 2.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 464        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -12.3      |\n",
      "|    reward             | -1.8059307 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 2.99       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-07-06 to  2020-10-02\n",
      "A2C Sharpe Ratio:  -0.015281935376495548\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_819_3\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 615          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 3            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.039097074 |\n",
      "-------------------------------------\n",
      "day: 2642, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 496003.09\n",
      "total_reward: -503996.91\n",
      "total_cost: 1081969.11\n",
      "total_trades: 9960\n",
      "Sharpe: -0.300\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 585          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070620077 |\n",
      "|    clip_fraction        | 0.0881       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0052      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.16         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00645     |\n",
      "|    reward               | -0.05013883  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 575          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072124926 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.26         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00511     |\n",
      "|    reward               | 0.37360188   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.92         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 571          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058198245 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.00314      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00751     |\n",
      "|    reward               | -1.1906947   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.91         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 568         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005440964 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | -0.00527    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0038     |\n",
      "|    reward               | 0.037047055 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.86        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-06 to  2020-10-02\n",
      "PPO Sharpe Ratio:  0.1007740390247255\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_3\n",
      "day: 2642, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4420611.84\n",
      "total_reward: 3420611.84\n",
      "total_cost: 998.29\n",
      "total_trades: 5279\n",
      "Sharpe: 0.831\n",
      "=================================\n",
      "======DDPG Validation from:  2020-07-06 to  2020-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-10-02\n",
      "======Trading from:  2020-10-02 to  2021-01-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_882_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | 0.16144627 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.86       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 460      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.513    |\n",
      "|    reward             | 1.599547 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.45     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -12.2     |\n",
      "|    reward             | -2.342287 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.43      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 455       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -3.43     |\n",
      "|    reward             | 0.4509624 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.682     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 452        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -0.0466    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -12.4      |\n",
      "|    reward             | -0.5021879 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.9        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 450         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.15       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -2.09       |\n",
      "|    reward             | -0.70838726 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 451       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -4.04     |\n",
      "|    reward             | 0.0731252 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.21      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 454          |\n",
      "|    iterations         | 800          |\n",
      "|    time_elapsed       | 8            |\n",
      "|    total_timesteps    | 4000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.15        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 799          |\n",
      "|    policy_loss        | 7.6          |\n",
      "|    reward             | -0.056020897 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 1.39         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 6.47       |\n",
      "|    reward             | 0.09167346 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 449         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.18       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -8.61       |\n",
      "|    reward             | -0.46312058 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.71        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 449         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -21         |\n",
      "|    reward             | -0.18196626 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 7.69        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 449        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 11.3       |\n",
      "|    reward             | -1.0951904 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.44       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 449         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.19       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 0.194       |\n",
      "|    reward             | 0.008869336 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.126       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 450        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 13.8       |\n",
      "|    reward             | 0.57773453 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 451        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | -0.00363   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 7.41       |\n",
      "|    reward             | -1.2325605 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.93       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 450         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 89.2        |\n",
      "|    reward             | -0.43235713 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 159         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 451        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 25.9       |\n",
      "|    reward             | -2.4999506 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 15.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 452        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 3.88       |\n",
      "|    reward             | 0.70795554 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.993      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 452       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -3.66     |\n",
      "|    reward             | 0.6831335 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.612     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 3.92       |\n",
      "|    reward             | 0.32588398 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.394      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-10-02 to  2021-01-04\n",
      "A2C Sharpe Ratio:  0.4201099118829015\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_882_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 598         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.10151898 |\n",
      "------------------------------------\n",
      "day: 2705, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 504554.94\n",
      "total_reward: -495445.06\n",
      "total_cost: 1198217.45\n",
      "total_trades: 10468\n",
      "Sharpe: -0.279\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 581          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070666857 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0629      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00563     |\n",
      "|    reward               | 0.37586686   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.96         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 568          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0088841375 |\n",
      "|    clip_fraction        | 0.0668       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.000327     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.18         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00571     |\n",
      "|    reward               | 0.69598216   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.12         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 564         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006639732 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.0309      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00366    |\n",
      "|    reward               | 0.008517733 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 5.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 564         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003130576 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.0272      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.11        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00357    |\n",
      "|    reward               | 0.070943154 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.25        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-10-02 to  2021-01-04\n",
      "PPO Sharpe Ratio:  0.09023949696311961\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_3\n",
      "day: 2705, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1822127.00\n",
      "total_reward: 822127.00\n",
      "total_cost: 998.66\n",
      "total_trades: 5410\n",
      "Sharpe: 0.398\n",
      "=================================\n",
      "======DDPG Validation from:  2020-10-02 to  2021-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-01-04\n",
      "======Trading from:  2021-01-04 to  2021-04-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_945_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 466          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.02        |\n",
      "|    explained_variance | 0.0462       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -7.89        |\n",
      "|    reward             | -0.017753806 |\n",
      "|    std                | 0.985        |\n",
      "|    value_loss         | 1.5          |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 466        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0.0405     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 2.4        |\n",
      "|    reward             | 0.48387423 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 0.448      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 461      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.94    |\n",
      "|    explained_variance | -0.00383 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -1.73    |\n",
      "|    reward             | -1.5195  |\n",
      "|    std                | 0.97     |\n",
      "|    value_loss         | 0.93     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 463       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.58      |\n",
      "|    reward             | 0.6981803 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 466        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0.0171     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -13.1      |\n",
      "|    reward             | -0.6129318 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 3.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 464       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | -0.129    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 2.73      |\n",
      "|    reward             | 1.0047607 |\n",
      "|    std                | 0.972     |\n",
      "|    value_loss         | 0.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 465       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | -0.00294  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 7.02      |\n",
      "|    reward             | 1.0098208 |\n",
      "|    std                | 0.971     |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 463        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0.105      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 0.104      |\n",
      "|    reward             | 0.85596913 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 0.0212     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 463        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0.0046     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 33.7       |\n",
      "|    reward             | -0.5162473 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 23.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 462       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 22.3      |\n",
      "|    reward             | -5.660679 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 24.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 461         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | 0.00201     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -65.1       |\n",
      "|    reward             | -0.62104744 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 102         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 1.41       |\n",
      "|    reward             | -2.7925365 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 2.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0.00739   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 6.4       |\n",
      "|    reward             | 1.0473473 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 1.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 7.88      |\n",
      "|    reward             | 1.4808671 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 1.65      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 453       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | 0.011     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.02      |\n",
      "|    reward             | 1.4894894 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 0.703     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 452       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -17       |\n",
      "|    reward             | 2.2098415 |\n",
      "|    std                | 0.969     |\n",
      "|    value_loss         | 8.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 452        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0.00174    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -2.9       |\n",
      "|    reward             | 0.54403204 |\n",
      "|    std                | 0.974      |\n",
      "|    value_loss         | 0.248      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 454       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -8.48     |\n",
      "|    reward             | -0.311125 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 2.01      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 455         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.93       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -12.6       |\n",
      "|    reward             | -0.26621094 |\n",
      "|    std                | 0.967       |\n",
      "|    value_loss         | 2.98        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 455        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.93      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -3.57      |\n",
      "|    reward             | -1.2213424 |\n",
      "|    std                | 0.968      |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  0.3989653909597167\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_945_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 627         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.09838299 |\n",
      "------------------------------------\n",
      "day: 2768, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 445549.55\n",
      "total_reward: -554450.45\n",
      "total_cost: 1092414.07\n",
      "total_trades: 10447\n",
      "Sharpe: -0.325\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 587         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007834561 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.0538     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00643    |\n",
      "|    reward               | 0.08468852  |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 3.57        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 573          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067123994 |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | 0.00954      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.36         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    reward               | -1.6113337   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 566          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044438625 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | 0.0145       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.9          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00352     |\n",
      "|    reward               | 0.28057617   |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 3.65         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 560          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067527406 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.0165       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00641     |\n",
      "|    reward               | 0.027041998  |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 4.7          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  0.17490366976421048\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_3\n",
      "day: 2768, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2881122.04\n",
      "total_reward: 1881122.04\n",
      "total_cost: 994.89\n",
      "total_trades: 2768\n",
      "Sharpe: 0.642\n",
      "=================================\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1008_3\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 454       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0.0127    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -13.9     |\n",
      "|    reward             | 0.3381658 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 4.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | -0.0154   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 2.94      |\n",
      "|    reward             | 1.2265383 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 2.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 461       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | -0.239    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -10.4     |\n",
      "|    reward             | -1.834233 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 3.43      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 448        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.331     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4         |\n",
      "|    reward             | 0.27870068 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 0.498      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 452         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | -0.418      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -8.67       |\n",
      "|    reward             | -0.42629397 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 1.9         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 447        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -12.6      |\n",
      "|    reward             | 0.48449785 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 3.63       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 445         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.96       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 2.36        |\n",
      "|    reward             | -0.21989931 |\n",
      "|    std                | 0.973       |\n",
      "|    value_loss         | 0.302       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 444       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 16.1      |\n",
      "|    reward             | 0.7357253 |\n",
      "|    std                | 0.97      |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 446        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 2.64       |\n",
      "|    reward             | 0.39984316 |\n",
      "|    std                | 0.969      |\n",
      "|    value_loss         | 0.733      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 446      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 20.7     |\n",
      "|    reward             | 1.770164 |\n",
      "|    std                | 0.971    |\n",
      "|    value_loss         | 17.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 448        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 71         |\n",
      "|    reward             | 0.37911904 |\n",
      "|    std                | 0.973      |\n",
      "|    value_loss         | 122        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 449         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.96       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 14.9        |\n",
      "|    reward             | -0.58113194 |\n",
      "|    std                | 0.973       |\n",
      "|    value_loss         | 7.16        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 449        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 13         |\n",
      "|    reward             | -1.3240511 |\n",
      "|    std                | 0.972      |\n",
      "|    value_loss         | 3.8        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 452        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.94      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 0.282      |\n",
      "|    reward             | -1.3254002 |\n",
      "|    std                | 0.969      |\n",
      "|    value_loss         | 1.62       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 451       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -13       |\n",
      "|    reward             | 2.2402995 |\n",
      "|    std                | 0.97      |\n",
      "|    value_loss         | 3.54      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 450       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.92     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -7.27     |\n",
      "|    reward             | 0.7790176 |\n",
      "|    std                | 0.967     |\n",
      "|    value_loss         | 3.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 446        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 68.6       |\n",
      "|    reward             | 0.61245424 |\n",
      "|    std                | 0.973      |\n",
      "|    value_loss         | 136        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 446       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 13.9      |\n",
      "|    reward             | 1.6038146 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 6.33      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 447          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 21           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.96        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 11.7         |\n",
      "|    reward             | -0.025316993 |\n",
      "|    std                | 0.975        |\n",
      "|    value_loss         | 3.47         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 449       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 13.1      |\n",
      "|    reward             | 1.7232707 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 5.57      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.21158172620083723\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1008_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 621         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.117863216 |\n",
      "------------------------------------\n",
      "day: 2831, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 529618.59\n",
      "total_reward: -470381.41\n",
      "total_cost: 1179081.20\n",
      "total_trades: 10690\n",
      "Sharpe: -0.245\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 588          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067632026 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0361      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.51         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00499     |\n",
      "|    reward               | -0.16932869  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.36         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 571         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008575265 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.00824    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.99        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00752    |\n",
      "|    reward               | 2.6805596   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.04        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 559         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008004407 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.063       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00463    |\n",
      "|    reward               | -0.42785934 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 553         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007923859 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.0137      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00682    |\n",
      "|    reward               | 0.32941774  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 2.32        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  0.005412942183451492\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1008_3\n",
      "day: 2831, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2900066.84\n",
      "total_reward: 1900066.84\n",
      "total_cost: 998.33\n",
      "total_trades: 8493\n",
      "Sharpe: 0.625\n",
      "=================================\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1071_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 426        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.5      |\n",
      "|    reward             | 0.26200455 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 4.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 429       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -4.67     |\n",
      "|    reward             | 1.5551139 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 2.37      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 438        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.0287    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -17.8      |\n",
      "|    reward             | -2.2264328 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 7.44       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 444        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -23.9      |\n",
      "|    reward             | -0.5845102 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 12.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 445        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -18.1      |\n",
      "|    reward             | -0.6689642 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 9.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 445        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 22         |\n",
      "|    reward             | -3.8532887 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 7.43       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 446       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -3.82     |\n",
      "|    reward             | 1.5473905 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 4.74      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 448       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 2.88      |\n",
      "|    reward             | 2.0862725 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 450        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -13        |\n",
      "|    reward             | -1.3031025 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 3.64       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 449         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 30.1        |\n",
      "|    reward             | -0.09680005 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 17.1        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 449        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.00132   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -42.3      |\n",
      "|    reward             | -1.3062038 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 62.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 450       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -0.0163   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 2.71      |\n",
      "|    reward             | 2.1834755 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.535     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 451        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 1.59       |\n",
      "|    reward             | 0.40317047 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.798      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 452       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0.00354   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -20.2     |\n",
      "|    reward             | 1.9492207 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 11.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 453       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -15.8     |\n",
      "|    reward             | 0.7341636 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 5.14      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 453       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -20.7     |\n",
      "|    reward             | 2.1900659 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 8.8       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 453       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 23.6      |\n",
      "|    reward             | 1.6324521 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 16        |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 453         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 2.15        |\n",
      "|    reward             | 0.039419115 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 0.376       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -2.99      |\n",
      "|    reward             | 0.69406825 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.395      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 454         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 5.35        |\n",
      "|    reward             | -0.41651246 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 1.84        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.09579443024563776\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1071_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 624         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.020798896 |\n",
      "------------------------------------\n",
      "day: 2894, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 585675.94\n",
      "total_reward: -414324.06\n",
      "total_cost: 1269254.96\n",
      "total_trades: 11005\n",
      "Sharpe: -0.170\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 574          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059940014 |\n",
      "|    clip_fraction        | 0.0812       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0141      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.94         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00848     |\n",
      "|    reward               | -1.8304601   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.32         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 561         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005296034 |\n",
      "|    clip_fraction        | 0.0326      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00221    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.35        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0036     |\n",
      "|    reward               | 0.56116617  |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 4.67        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 557          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.003949019  |\n",
      "|    clip_fraction        | 0.0431       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.0494       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    reward               | -0.010840566 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.46         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 556         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008727608 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | 0.000783    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.934       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00894    |\n",
      "|    reward               | 0.39520654  |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 2.4         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -0.21003144739718135\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1071_3\n",
      "day: 2894, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3791588.73\n",
      "total_reward: 2791588.73\n",
      "total_cost: 10318.29\n",
      "total_trades: 2910\n",
      "Sharpe: 0.757\n",
      "=================================\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1134_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | 0.23157007 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.58       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 455       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.975    |\n",
      "|    reward             | 0.6117932 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.07      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0.00336   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -5.5      |\n",
      "|    reward             | -1.759159 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.81      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 450         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.0391     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -6.59       |\n",
      "|    reward             | -0.20568807 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.12        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 452         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -6.28       |\n",
      "|    reward             | -0.31152672 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 1.99        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 457       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -3.05     |\n",
      "|    reward             | 1.6299458 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.328     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 458        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -1.08      |\n",
      "|    reward             | -0.4275678 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.765      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 459         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 25.6        |\n",
      "|    reward             | -0.08959458 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 15.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -24.2     |\n",
      "|    reward             | 0.7912882 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 24.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -56.1     |\n",
      "|    reward             | 0.5931136 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 79.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 1.6280572 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 8.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 458        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | -1.7147032 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 1.7       |\n",
      "|    reward             | 1.0981249 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.604     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 457        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 17.9       |\n",
      "|    reward             | -1.6322961 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.74       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0.00151    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 16.5       |\n",
      "|    reward             | 0.18060847 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.84       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 452       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -5.48     |\n",
      "|    reward             | 1.9268765 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 7.58      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 452      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.14    |\n",
      "|    explained_variance | -0.00201 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -33.1    |\n",
      "|    reward             | 2.268566 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 42.2     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 453         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 3.33e-05    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -6.35       |\n",
      "|    reward             | -0.67306525 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.83        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 451       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0.0372    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 9.19      |\n",
      "|    reward             | 1.1205267 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.39      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 451      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | 11.3     |\n",
      "|    reward             | 1.017488 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 2.61     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  0.0771048843214946\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1134_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 597         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.034877438 |\n",
      "------------------------------------\n",
      "day: 2957, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 815761.80\n",
      "total_reward: -184238.20\n",
      "total_cost: 1629285.01\n",
      "total_trades: 11509\n",
      "Sharpe: -0.021\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 565         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005789281 |\n",
      "|    clip_fraction        | 0.0595      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.00359    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00667    |\n",
      "|    reward               | 0.3676353   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.7         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00676959  |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.00253     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.16        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    reward               | -0.19451961 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 6.56        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 548          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050273645 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.0237       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00573     |\n",
      "|    reward               | 0.764003     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.17         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073491596 |\n",
      "|    clip_fraction        | 0.0677       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.0045       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00565     |\n",
      "|    reward               | -0.099230036 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.78         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
      "PPO Sharpe Ratio:  0.09118513648588393\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1134_3\n",
      "day: 2957, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2495470.25\n",
      "total_reward: 1495470.25\n",
      "total_cost: 998.55\n",
      "total_trades: 11827\n",
      "Sharpe: 0.539\n",
      "=================================\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
      "======Trading from:  2022-01-03 to  2022-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1197_3\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 434          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.15        |\n",
      "|    explained_variance | -0.0956      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -6.71        |\n",
      "|    reward             | -0.024811579 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 1.59         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 434       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0.0165    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.67     |\n",
      "|    reward             | 0.5411576 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.881     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 427       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0.0226    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -5.8      |\n",
      "|    reward             | -1.674281 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.7       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 427        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.177      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -2.2       |\n",
      "|    reward             | 0.29560548 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.218      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 424         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -7.04       |\n",
      "|    reward             | -0.33610463 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.31        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 426       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.61     |\n",
      "|    reward             | 3.4477396 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 426         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.18       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -22         |\n",
      "|    reward             | -0.29348657 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 9.41        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 427        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0.0832     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | 0.88315094 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.81       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 428         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.57       |\n",
      "|    reward             | -0.18278438 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 3.23        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 429         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 0.354       |\n",
      "|    reward             | -0.22861134 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.0602      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 432       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -2.51     |\n",
      "|    reward             | 0.5898575 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.366     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 433        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 2.65       |\n",
      "|    reward             | -0.2131961 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.656      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 434       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -5.57     |\n",
      "|    reward             | 1.432489  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 435        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 11.7       |\n",
      "|    reward             | -0.8969939 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.74       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 435         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 8.67        |\n",
      "|    reward             | -0.22481965 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.05        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 435         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 6.01        |\n",
      "|    reward             | -0.29192272 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.36        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 436      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -34.8    |\n",
      "|    reward             | 1.184853 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 22.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 435        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 12.8       |\n",
      "|    reward             | -1.2392886 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 6.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 435       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -4.69     |\n",
      "|    reward             | 1.3432758 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 1.58      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 436         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 5.27        |\n",
      "|    reward             | -0.21579596 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 0.832       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-01-03 to  2022-04-04\n",
      "A2C Sharpe Ratio:  -0.15305385597100352\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1197_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 571         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.073024474 |\n",
      "------------------------------------\n",
      "day: 3020, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 452675.26\n",
      "total_reward: -547324.74\n",
      "total_cost: 1306187.70\n",
      "total_trades: 11467\n",
      "Sharpe: -0.315\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 549         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006843084 |\n",
      "|    clip_fraction        | 0.0759      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0918     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00793    |\n",
      "|    reward               | -0.9517111  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.53        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006508128 |\n",
      "|    clip_fraction        | 0.0689      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0107     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.48        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    reward               | -2.2130804  |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 4.05        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 544         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008332703 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | 0.0251      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00851    |\n",
      "|    reward               | -0.12041584 |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 2.5         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 540         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005861057 |\n",
      "|    clip_fraction        | 0.0497      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | 0.0028      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    reward               | -0.07445707 |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-01-03 to  2022-04-04\n",
      "PPO Sharpe Ratio:  -0.4523811820367423\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1197_3\n",
      "day: 3020, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2626600.31\n",
      "total_reward: 1626600.31\n",
      "total_cost: 997.27\n",
      "total_trades: 6041\n",
      "Sharpe: 0.557\n",
      "=================================\n",
      "======DDPG Validation from:  2022-01-03 to  2022-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
      "======Trading from:  2022-04-04 to  2022-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1260_3\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.0964    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -15.3      |\n",
      "|    reward             | 0.11231236 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0.0472     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.602     |\n",
      "|    reward             | 0.55048126 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.898      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 424         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.0276     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -6.17       |\n",
      "|    reward             | -0.33697206 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 1.13        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 426         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | 0.101       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -3.75       |\n",
      "|    reward             | -0.28240097 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.413       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 425         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | -0.636      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -7.36       |\n",
      "|    reward             | -0.16775328 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.14        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 425       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.922    |\n",
      "|    reward             | 1.8270739 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.351     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 424       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -30.6     |\n",
      "|    reward             | 1.2648227 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 27        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 427       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -13       |\n",
      "|    reward             | 0.6325118 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 3.46      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 429        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -5.55      |\n",
      "|    reward             | -1.3293995 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.924      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 429       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -0.000977 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.132    |\n",
      "|    reward             | 1.1494124 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.04      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 428        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -25.9      |\n",
      "|    reward             | -0.9747887 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 14.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 420        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 3.37       |\n",
      "|    reward             | -1.4396636 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.4        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 417      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 15       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 8.76     |\n",
      "|    reward             | 0.429701 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 1.93     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 418        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -13.6      |\n",
      "|    reward             | 0.56957006 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 8.06       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 414         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.0985     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 3.89        |\n",
      "|    reward             | -0.21675718 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 0.419       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 413        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0.113      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -2.7       |\n",
      "|    reward             | -1.3177539 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.376      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 414        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 10         |\n",
      "|    reward             | 0.50487965 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 3.35       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 415         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 11.9        |\n",
      "|    reward             | 0.100553565 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 7.18        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 416        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -0.627     |\n",
      "|    reward             | 0.14592773 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 0.108      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 417        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -0.162     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -4.41      |\n",
      "|    reward             | -0.9526649 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.446      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-04-04 to  2022-07-06\n",
      "A2C Sharpe Ratio:  -0.385622482743754\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1260_3\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 596         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.20220739 |\n",
      "------------------------------------\n",
      "day: 3083, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 603086.65\n",
      "total_reward: -396913.35\n",
      "total_cost: 1390744.54\n",
      "total_trades: 11731\n",
      "Sharpe: -0.165\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 572          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075136963 |\n",
      "|    clip_fraction        | 0.0757       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.129       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.68         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00495     |\n",
      "|    reward               | 0.49779224   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.98         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 542          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043821144 |\n",
      "|    clip_fraction        | 0.0447       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | 0.00722      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.9          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00732     |\n",
      "|    reward               | -1.0752604   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.93         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 529         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005671883 |\n",
      "|    clip_fraction        | 0.0617      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0151      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00586    |\n",
      "|    reward               | -0.28192884 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.78        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 521         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008522695 |\n",
      "|    clip_fraction        | 0.0425      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.00857    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00476    |\n",
      "|    reward               | -0.1892708  |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 4.74        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-04 to  2022-07-06\n",
      "PPO Sharpe Ratio:  -0.3452753237796908\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1260_3\n",
      "day: 3083, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2284687.20\n",
      "total_reward: 1284687.20\n",
      "total_cost: 997.45\n",
      "total_trades: 3083\n",
      "Sharpe: 0.477\n",
      "=================================\n",
      "======DDPG Validation from:  2022-04-04 to  2022-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-06\n",
      "======Trading from:  2022-07-06 to  2022-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1323_3\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 436       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -11.3     |\n",
      "|    reward             | 0.2228668 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.5       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 442      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.19    |\n",
      "|    explained_variance | -0.104   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 6.19     |\n",
      "|    reward             | 1.059535 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.01     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 442        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.19      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14        |\n",
      "|    reward             | -1.4004675 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.74       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 443        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.22      |\n",
      "|    explained_variance | 0.0868     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.46      |\n",
      "|    reward             | 0.19097337 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.563      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 445         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.21       |\n",
      "|    explained_variance | -0.0341     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -10.6       |\n",
      "|    reward             | -0.39608654 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 3.28        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 444      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.22    |\n",
      "|    explained_variance | 0.00901  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -1.62    |\n",
      "|    reward             | 3.95978  |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.51     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 441       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.2      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -3.42     |\n",
      "|    reward             | 0.6014028 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.409     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 442        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.2       |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 6.95       |\n",
      "|    reward             | -0.3079625 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 441          |\n",
      "|    iterations         | 900          |\n",
      "|    time_elapsed       | 10           |\n",
      "|    total_timesteps    | 4500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.2         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 899          |\n",
      "|    policy_loss        | -1.1         |\n",
      "|    reward             | -0.035035536 |\n",
      "|    std                | 1.02         |\n",
      "|    value_loss         | 0.694        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 441       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 1.77      |\n",
      "|    reward             | 0.8996678 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.245     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 442        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.23      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 11.4       |\n",
      "|    reward             | 0.49711177 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.42       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 443        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.24      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 12.2       |\n",
      "|    reward             | 0.78449434 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 3.95       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 443         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.25       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -1.15       |\n",
      "|    reward             | -0.22308995 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 0.531       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 444        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.25      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -9.86      |\n",
      "|    reward             | -1.7444656 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 2.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 444       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.24     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -25.7     |\n",
      "|    reward             | 1.8513111 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 19.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 445        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.26      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -1.91      |\n",
      "|    reward             | 0.17901239 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.684      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 443        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.27      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -1.3       |\n",
      "|    reward             | 0.11930702 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.396      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 443        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.26      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -17.9      |\n",
      "|    reward             | 0.33588287 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 13.9       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 444          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 21           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.25        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 4.24         |\n",
      "|    reward             | 0.0118278805 |\n",
      "|    std                | 1.03         |\n",
      "|    value_loss         | 0.438        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 444        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.25      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 7.4        |\n",
      "|    reward             | -1.0766762 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.92       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-07-06 to  2022-10-04\n",
      "A2C Sharpe Ratio:  -0.0870505329919831\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1323_3\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 602       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 3         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.3764454 |\n",
      "----------------------------------\n",
      "day: 3146, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 303913.41\n",
      "total_reward: -696086.59\n",
      "total_cost: 1291176.09\n",
      "total_trades: 11906\n",
      "Sharpe: -0.466\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 573         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005840077 |\n",
      "|    clip_fraction        | 0.0591      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.169      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.55        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00658    |\n",
      "|    reward               | 1.7408867   |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009059083 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.06       |\n",
      "|    explained_variance   | -0.0169     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0073     |\n",
      "|    reward               | 0.35550216  |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 546          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085056955 |\n",
      "|    clip_fraction        | 0.0959       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | 0.0189       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.83         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    reward               | 0.27394482   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 2.63         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 538        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 19         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0047627  |\n",
      "|    clip_fraction        | 0.0502     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.08      |\n",
      "|    explained_variance   | -0.0314    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.65       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00349   |\n",
      "|    reward               | 0.27352569 |\n",
      "|    std                  | 0.998      |\n",
      "|    value_loss           | 3.6        |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-07-06 to  2022-10-04\n",
      "PPO Sharpe Ratio:  -0.19345281375687437\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1323_3\n",
      "day: 3146, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2613300.12\n",
      "total_reward: 1613300.12\n",
      "total_cost: 998.65\n",
      "total_trades: 12584\n",
      "Sharpe: 0.515\n",
      "=================================\n",
      "======DDPG Validation from:  2022-07-06 to  2022-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-04\n",
      "======Trading from:  2022-10-04 to  2023-01-04\n",
      "Ensemble Strategy took:  28.747223699092864  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.289786</td>\n",
       "      <td>0.123857</td>\n",
       "      <td>0.423477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.016573</td>\n",
       "      <td>-0.139349</td>\n",
       "      <td>-0.032351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.00894</td>\n",
       "      <td>-0.037861</td>\n",
       "      <td>0.374275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.476941</td>\n",
       "      <td>0.281075</td>\n",
       "      <td>0.614279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.351522</td>\n",
       "      <td>-0.383345</td>\n",
       "      <td>-0.35052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>441</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.553641</td>\n",
       "      <td>0.467235</td>\n",
       "      <td>0.642283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>504</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.100488</td>\n",
       "      <td>0.028792</td>\n",
       "      <td>0.122981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>567</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.136061</td>\n",
       "      <td>-0.312879</td>\n",
       "      <td>-0.100713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>630</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.481432</td>\n",
       "      <td>0.551571</td>\n",
       "      <td>0.725964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>693</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>-0.275431</td>\n",
       "      <td>-0.082412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>756</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.325806</td>\n",
       "      <td>0.299458</td>\n",
       "      <td>0.330448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>819</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.015282</td>\n",
       "      <td>0.100774</td>\n",
       "      <td>0.219619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.42011</td>\n",
       "      <td>0.090239</td>\n",
       "      <td>0.457635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>945</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.398965</td>\n",
       "      <td>0.174904</td>\n",
       "      <td>0.398965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1008</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.211582</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.242806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1071</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.095794</td>\n",
       "      <td>-0.210031</td>\n",
       "      <td>0.025171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1134</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.077105</td>\n",
       "      <td>0.091185</td>\n",
       "      <td>0.222535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1197</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.153054</td>\n",
       "      <td>-0.452381</td>\n",
       "      <td>-0.156913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1260</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.385622</td>\n",
       "      <td>-0.345275</td>\n",
       "      <td>-0.299245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1323</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.087051</td>\n",
       "      <td>-0.193453</td>\n",
       "      <td>-0.082272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0    126  2017-10-02  2018-01-02       DDPG   0.289786   0.123857    0.423477\n",
       "1    189  2018-01-02  2018-04-04        A2C   0.016573  -0.139349   -0.032351\n",
       "2    252  2018-04-04  2018-07-03       DDPG    0.00894  -0.037861    0.374275\n",
       "3    315  2018-07-03  2018-10-02       DDPG   0.476941   0.281075    0.614279\n",
       "4    378  2018-10-02  2019-01-03       DDPG  -0.351522  -0.383345    -0.35052\n",
       "5    441  2019-01-03  2019-04-04       DDPG   0.553641   0.467235    0.642283\n",
       "6    504  2019-04-04  2019-07-05       DDPG   0.100488   0.028792    0.122981\n",
       "7    567  2019-07-05  2019-10-03       DDPG  -0.136061  -0.312879   -0.100713\n",
       "8    630  2019-10-03  2020-01-03       DDPG   0.481432   0.551571    0.725964\n",
       "9    693  2020-01-03  2020-04-03        A2C   0.001512  -0.275431   -0.082412\n",
       "10   756  2020-04-03  2020-07-06       DDPG   0.325806   0.299458    0.330448\n",
       "11   819  2020-07-06  2020-10-02       DDPG  -0.015282   0.100774    0.219619\n",
       "12   882  2020-10-02  2021-01-04       DDPG    0.42011   0.090239    0.457635\n",
       "13   945  2021-01-04  2021-04-06       DDPG   0.398965   0.174904    0.398965\n",
       "14  1008  2021-04-06  2021-07-06       DDPG   0.211582   0.005413    0.242806\n",
       "15  1071  2021-07-06  2021-10-04       DDPG  -0.095794  -0.210031    0.025171\n",
       "16  1134  2021-10-04  2022-01-03       DDPG   0.077105   0.091185    0.222535\n",
       "17  1197  2022-01-03  2022-04-04        A2C  -0.153054  -0.452381   -0.156913\n",
       "18  1260  2022-04-04  2022-07-06       DDPG  -0.385622  -0.345275   -0.299245\n",
       "19  1323  2022-07-06  2022-10-04       DDPG  -0.087051  -0.193453   -0.082272"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.5053529652893635\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp],ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.003237e+06</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.008739e+06</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.014535e+06</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.005745</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.015400e+06</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>2018-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0   1.000000e+06  2018-01-02           NaN  2018-01-02\n",
       "1   1.003237e+06  2018-01-03      0.003237  2018-01-03\n",
       "2   1.008739e+06  2018-01-04      0.005485  2018-01-04\n",
       "3   1.014535e+06  2018-01-05      0.005745  2018-01-05\n",
       "4   1.015400e+06  2018-01-08      0.000853  2018-01-08"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.076448\n",
      "Cumulative returns     0.445327\n",
      "Annual volatility      0.176881\n",
      "Sharpe ratio           0.505353\n",
      "Calmar ratio           0.281272\n",
      "Stability              0.736030\n",
      "Max drawdown          -0.271795\n",
      "Omega ratio            1.093692\n",
      "Sortino ratio          0.712957\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.877581\n",
      "Daily value at risk   -0.021930\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n",
      "Shape of DataFrame:  (1259, 8)\n",
      "Annual return          0.059583\n",
      "Cumulative returns     0.335290\n",
      "Annual volatility      0.217954\n",
      "Sharpe ratio           0.375375\n",
      "Calmar ratio           0.160661\n",
      "Stability              0.685042\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.077914\n",
      "Sortino ratio          0.516270\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.893140\n",
      "Daily value at risk   -0.027135\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_dji_ = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:              date           dji\n",
      "0     2018-01-02  1.000000e+06\n",
      "1     2018-01-03  1.003975e+06\n",
      "2     2018-01-04  1.010116e+06\n",
      "3     2018-01-05  1.019008e+06\n",
      "4     2018-01-08  1.018490e+06\n",
      "...          ...           ...\n",
      "1255  2022-12-27  1.339089e+06\n",
      "1256  2022-12-28  1.324351e+06\n",
      "1257  2022-12-29  1.338253e+06\n",
      "1258  2022-12-30  1.335290e+06\n",
      "1259  2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.003975e+06\n",
      "2018-01-04  1.010116e+06\n",
      "2018-01-05  1.019008e+06\n",
      "2018-01-08  1.018490e+06\n",
      "...                  ...\n",
      "2022-12-27  1.339089e+06\n",
      "2022-12-28  1.324351e+06\n",
      "2022-12-29  1.338253e+06\n",
      "2022-12-30  1.335290e+06\n",
      "2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:          datadate\n",
      "0     2017-10-02\n",
      "1     2017-10-03\n",
      "2     2017-10-04\n",
      "3     2017-10-05\n",
      "4     2017-10-06\n",
      "...          ...\n",
      "1356  2023-02-22\n",
      "1357  2023-02-23\n",
      "1358  2023-02-24\n",
      "1359  2023-02-27\n",
      "1360  2023-02-28\n",
      "\n",
      "[1361 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.003237e+06\n",
      "2018-01-04  1.008739e+06\n",
      "2018-01-05  1.014535e+06\n",
      "2018-01-08  1.015400e+06\n",
      "...                  ...\n",
      "2022-12-27  1.451385e+06\n",
      "2022-12-28  1.434113e+06\n",
      "2022-12-29  1.453615e+06\n",
      "2022-12-30  1.448195e+06\n",
      "2023-01-03  1.445327e+06\n",
      "\n",
      "[1260 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2018-01-02  1.000000e+06  1.000000e+06\n",
      "2018-01-03  1.003237e+06  1.003975e+06\n",
      "2018-01-04  1.008739e+06  1.010116e+06\n",
      "2018-01-05  1.014535e+06  1.019008e+06\n",
      "2018-01-08  1.015400e+06  1.018490e+06\n",
      "...                  ...           ...\n",
      "2022-12-27  1.451385e+06  1.339089e+06\n",
      "2022-12-28  1.434113e+06  1.324351e+06\n",
      "2022-12-29  1.453615e+06  1.338253e+06\n",
      "2022-12-30  1.448195e+06  1.335290e+06\n",
      "2023-01-03  1.445327e+06           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value, \n",
    "#               baseline_ticker = '^DJI', \n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_result_ensemble))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./pkl_results/ensemble.pkl', 'wb') as file:\n",
    "    pickle.dump(df, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
