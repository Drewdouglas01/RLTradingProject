{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrds in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.26.4)\n",
      "Requirement already satisfied: packaging<23.3 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (23.2)\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.2.1)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.9.9)\n",
      "Requirement already satisfied: scipy<1.13,>=1.12 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.0.29)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
      "Requirement already satisfied: swig in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (4.2.1)\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n"
     ]
    }
   ],
   "source": [
    "# ## install finrl library\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 21:09:43.887439: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-26 21:09:43.920620: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-26 21:09:44.688868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_5_TICKER = [\n",
    "    \"AXP\",\n",
    "    \"AMGN\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "    \"CAT\",\n",
    "]\n",
    "INDEX_5_TICKER = [\n",
    "    \"^DJI\", \n",
    "    \"^IXIC\", \n",
    "    \"^NYA\", \n",
    "    \"^RUT\", \n",
    "    \"^GSPC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "\n",
    "# # TRAIN_START_DATE = '2009-04-01'\n",
    "# # TRAIN_END_DATE = '2021-01-01'\n",
    "# # TEST_START_DATE = '2021-01-01'\n",
    "# # TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "\n",
    "# TRAIN_START_DATE = '2009-06-01'\n",
    "# #TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "# dfexport = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = DOW_30_TICKER).fetch_data()\n",
    "\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfexport.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data export\n",
    "# import pickle\n",
    "# datasetName = \"dailydata\"\n",
    "# datasetDir = \"./datasets\"\n",
    "\n",
    "# os.makedirs(datasetDir, exist_ok=True)\n",
    "# datasetPath = os.path.join(datasetDir, datasetName) + \".pkl\"\n",
    "\n",
    "\n",
    "# with open(datasetPath, 'wb') as file:\n",
    "#     pickle.dump(dfexport, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (16555, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "#TRAIN_START_DATE = '2000-01-01'\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2017-10-01'\n",
    "TEST_START_DATE = '2017-10-01'\n",
    "TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = INDEX_5_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Andrew Martin - UNCOMMENT BELOW TO ADD PREDICTION INDICATOR\n",
    "# import pickle\n",
    "# with open(\"./datasets/index_5_predictor_2.pkl\", 'rb') as file:\n",
    "#   df_prob = pickle.load(file)\n",
    "# df6 = df_prob.copy()\n",
    "# df6 = df6.loc[:, ~df6.columns.duplicated(keep='first')]\n",
    "# df6[\"date\"] = df6[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "# df2 = processed.merge(df6[['tic', 'date', 'Predicted_Target']], on=['tic', 'date'], how='left')\n",
    "# processed = df2.copy()\n",
    "# INDICATORS.append(\"Predicted_Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 5, State Space: 51\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2017-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 303        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.4        |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -16        |\n",
      "|    reward             | 0.05003921 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.3       |\n",
      "|    reward             | 0.47790936 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 0.626      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 317        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0.0267     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.51      |\n",
      "|    reward             | -1.2869126 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 0.933      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 327       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -0.0533   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 7.62      |\n",
      "|    reward             | 0.4401895 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 334        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -1.06      |\n",
      "|    reward             | 0.12991184 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.0636     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 332        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 6.83       |\n",
      "|    reward             | 0.00105785 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 326       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 0.167     |\n",
      "|    reward             | 0.2776417 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.432     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 328       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -43.5     |\n",
      "|    reward             | 0.8971335 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 31        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 327       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -13.9     |\n",
      "|    reward             | 1.2305671 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 3.22      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 325        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 8.5        |\n",
      "|    reward             | 0.15822598 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 328        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -6.33      |\n",
      "|    reward             | 0.07129453 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.57       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 332       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -0.195    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 9.09      |\n",
      "|    reward             | 1.0209743 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 330        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.0927    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | -0.5521561 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 3.71       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 328        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 6.4        |\n",
      "|    reward             | 0.17170905 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.824      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 326         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 4.58        |\n",
      "|    reward             | -0.27300167 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.424       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 3.67       |\n",
      "|    reward             | -1.8895351 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.412      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 323         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 3.46        |\n",
      "|    reward             | 0.038364332 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 0.22        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 325       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0.0764    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -9.96     |\n",
      "|    reward             | -2.716765 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.59      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 323        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 0.117      |\n",
      "|    reward             | -0.4522214 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.981      |\n",
      "--------------------------------------\n",
      "day: 1949, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1757099.12\n",
      "total_reward: 757099.12\n",
      "total_cost: 418086.89\n",
      "total_trades: 7003\n",
      "Sharpe: 0.500\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 323         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 6.44        |\n",
      "|    reward             | -0.19473448 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 1.3         |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2017-10-02 to  2018-01-02\n",
      "A2C Sharpe Ratio:  0.4332780504768566\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 416         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.01234937 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 426          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0085951425 |\n",
      "|    clip_fraction        | 0.0715       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.0563      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.96         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00614     |\n",
      "|    reward               | 0.73020506   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.58         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 405          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064530876 |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.000149    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.43         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00563     |\n",
      "|    reward               | -0.624154    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.26         |\n",
      "------------------------------------------\n",
      "day: 1949, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 589839.05\n",
      "total_reward: -410160.95\n",
      "total_cost: 818572.26\n",
      "total_trades: 7496\n",
      "Sharpe: -0.378\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00893452  |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00348    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.61        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00875    |\n",
      "|    reward               | -0.66132534 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.7         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006318756 |\n",
      "|    clip_fraction        | 0.0541      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.000428   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.65        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00536    |\n",
      "|    reward               | -1.2190515  |\n",
      "|    std                  | 0.995       |\n",
      "|    value_loss           | 3.67        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2017-10-02 to  2018-01-02\n",
      "PPO Sharpe Ratio:  -0.11338821013280005\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_1\n",
      "day: 1949, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2302711.40\n",
      "total_reward: 1302711.40\n",
      "total_cost: 998.57\n",
      "total_trades: 9744\n",
      "Sharpe: 0.803\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 150        |\n",
      "|    time_elapsed    | 51         |\n",
      "|    total_timesteps | 7800       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -548       |\n",
      "|    critic_loss     | 22.5       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7699       |\n",
      "|    reward          | 0.67288697 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2017-10-02 to  2018-01-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-01-02\n",
      "======Trading from:  2018-01-02 to  2018-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-01-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 333        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.9      |\n",
      "|    reward             | 0.29733393 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 3.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 312       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 2.9       |\n",
      "|    reward             | 1.3739494 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 2.7       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 312        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.3      |\n",
      "|    reward             | -2.3368502 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 5.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 313        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -6.93      |\n",
      "|    reward             | 0.29882067 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.06       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 314          |\n",
      "|    iterations         | 500          |\n",
      "|    time_elapsed       | 7            |\n",
      "|    total_timesteps    | 2500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.1         |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 499          |\n",
      "|    policy_loss        | 40.8         |\n",
      "|    reward             | -0.014406152 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 28.3         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 317         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 13.1        |\n",
      "|    reward             | 0.033682056 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 5.64        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 324         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.109      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 27.3        |\n",
      "|    reward             | -0.78454834 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 18.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | 0.20127925 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 6.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -23.2     |\n",
      "|    reward             | -2.245475 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -27.1      |\n",
      "|    reward             | 0.30572826 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 325        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 17         |\n",
      "|    reward             | -0.5113258 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 8.32       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 326      |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 27.1     |\n",
      "|    reward             | 2.094391 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 20.9     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 328        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 20.5       |\n",
      "|    reward             | -2.2836416 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.55       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 326         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 22          |\n",
      "|    reward             | -0.13825361 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 10.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 326       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 11.9      |\n",
      "|    reward             | 3.3730328 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 325          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.07        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | 4.75         |\n",
      "|    reward             | -0.014067432 |\n",
      "|    std                | 0.995        |\n",
      "|    value_loss         | 1.22         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 327        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 6.73       |\n",
      "|    reward             | 0.90271795 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 7.4        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 326         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 5.13        |\n",
      "|    reward             | -0.70764756 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 1.28        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 324       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -72.2     |\n",
      "|    reward             | 3.0641153 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 82.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 323         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -9.93       |\n",
      "|    reward             | 0.008670426 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 2.39        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2018-01-02 to  2018-04-04\n",
      "A2C Sharpe Ratio:  -0.0008220592160709752\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 497       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 4         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.8785723 |\n",
      "----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 467         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005780573 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0184     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.42        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    reward               | 0.16321678  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006311315 |\n",
      "|    clip_fraction        | 0.0554      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.00516     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00384    |\n",
      "|    reward               | -3.5322192  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065770466 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.0182       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.6          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0063      |\n",
      "|    reward               | 0.19286416   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.87         |\n",
      "------------------------------------------\n",
      "day: 2012, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 775626.93\n",
      "total_reward: -224373.07\n",
      "total_cost: 935391.39\n",
      "total_trades: 7994\n",
      "Sharpe: -0.146\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 426         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006922777 |\n",
      "|    clip_fraction        | 0.0568      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.00768    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    reward               | -0.10466891 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.23        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-01-02 to  2018-04-04\n",
      "PPO Sharpe Ratio:  -0.13151874560488502\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_1\n",
      "day: 2012, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1900849.82\n",
      "total_reward: 900849.82\n",
      "total_cost: 998.81\n",
      "total_trades: 6036\n",
      "Sharpe: 0.590\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 146        |\n",
      "|    time_elapsed    | 54         |\n",
      "|    total_timesteps | 8052       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 100        |\n",
      "|    critic_loss     | 1.37       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7951       |\n",
      "|    reward          | -0.8676032 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-01-02 to  2018-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-04-04\n",
      "======Trading from:  2018-04-04 to  2018-07-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 366        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | 0.45767245 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.97       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 354      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.17    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 5.73     |\n",
      "|    reward             | 1.42152  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.82     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 333        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -13.5      |\n",
      "|    reward             | -1.7692981 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 7.69       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 331         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.18       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -19         |\n",
      "|    reward             | -0.47727224 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 8.8         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 324       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.2      |\n",
      "|    explained_variance | 0.0145    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 25        |\n",
      "|    reward             | -2.025178 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 20.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 327        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -15.9      |\n",
      "|    reward             | 0.44478214 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 6.68       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 321         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | -0.0122     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -64.6       |\n",
      "|    reward             | -0.50106525 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 146         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 316       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -26.3     |\n",
      "|    reward             | 0.5406859 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 10.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 312         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.2        |\n",
      "|    explained_variance | -0.109      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.47       |\n",
      "|    reward             | -0.71494365 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.49        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 310        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.2       |\n",
      "|    explained_variance | -0.794     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 4.76       |\n",
      "|    reward             | 0.81011724 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.468      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 307         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.21       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -5.92       |\n",
      "|    reward             | -0.04315582 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.17        |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 306           |\n",
      "|    iterations         | 1200          |\n",
      "|    time_elapsed       | 19            |\n",
      "|    total_timesteps    | 6000          |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -7.21         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 1199          |\n",
      "|    policy_loss        | -0.659        |\n",
      "|    reward             | -0.0029801514 |\n",
      "|    std                | 1.02          |\n",
      "|    value_loss         | 0.0661        |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 305       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.19     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -1.73     |\n",
      "|    reward             | 2.1851854 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 8.36       |\n",
      "|    reward             | 0.42932737 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.82       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 309         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.17       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | -27.5       |\n",
      "|    reward             | -0.19598374 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 20.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.91     |\n",
      "|    reward             | 0.9118072 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 311       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 5.08      |\n",
      "|    reward             | 1.5780579 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.6       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 312        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.21      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -13.3      |\n",
      "|    reward             | -0.2680338 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 311       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.21     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -9.79     |\n",
      "|    reward             | 1.2642062 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 5.37      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 308       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.2      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 23.2      |\n",
      "|    reward             | 1.4000089 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 8.67      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2018-04-04 to  2018-07-03\n",
      "A2C Sharpe Ratio:  0.18046489705727473\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 457          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.050424375 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 431          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058329897 |\n",
      "|    clip_fraction        | 0.0575       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.694       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.64         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00683     |\n",
      "|    reward               | 0.54500586   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.45         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 439          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060236277 |\n",
      "|    clip_fraction        | 0.0274       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.00101     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00327     |\n",
      "|    reward               | 0.5847437    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.84         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 440         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007196823 |\n",
      "|    clip_fraction        | 0.0628      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | -0.00281    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.26        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    reward               | 0.40023768  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 444         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009739846 |\n",
      "|    clip_fraction        | 0.0864      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.16       |\n",
      "|    explained_variance   | -0.000831   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.27        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00866    |\n",
      "|    reward               | 0.37487036  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 4.44        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-04-04 to  2018-07-03\n",
      "PPO Sharpe Ratio:  0.040903996560620104\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_1\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 147      |\n",
      "|    time_elapsed    | 56       |\n",
      "|    total_timesteps | 8304     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -102     |\n",
      "|    critic_loss     | 2.47     |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8203     |\n",
      "|    reward          | 2.275288 |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2018-04-04 to  2018-07-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-07-03\n",
      "======Trading from:  2018-07-03 to  2018-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-07-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 351        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -0.327     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -8.07      |\n",
      "|    reward             | 0.11299669 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 321       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | -0.188    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.637     |\n",
      "|    reward             | 1.1939883 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.15      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 333        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0.0094     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.99      |\n",
      "|    reward             | -1.6825578 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.14       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | -0.0328   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -1.33     |\n",
      "|    reward             | 0.6172043 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.812     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0.0124     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | 0.62877667 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.84       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 311       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.863    |\n",
      "|    reward             | 0.6304464 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.125     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 312         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -10.7       |\n",
      "|    reward             | -0.19663607 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.42        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 311       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -12.5     |\n",
      "|    reward             | 0.6950779 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.86     |\n",
      "|    reward             | 1.6065351 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.02      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 307        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -19.6      |\n",
      "|    reward             | -1.6982552 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 19.1       |\n",
      "|    reward             | 0.53105515 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 7.77       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 304         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -7.78       |\n",
      "|    reward             | -0.80365056 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.35        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 306       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -12.9     |\n",
      "|    reward             | -2.191105 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 3.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 309       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -2.37     |\n",
      "|    reward             | 1.0133157 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.801     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 307          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.15        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | 9.91         |\n",
      "|    reward             | 0.0140147675 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 4.88         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 306        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 5.94       |\n",
      "|    reward             | 0.47355077 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.23       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 306        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -18        |\n",
      "|    reward             | -1.1722604 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 307      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.16    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 9.8      |\n",
      "|    reward             | 1.729987 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.83     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 308        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -8.74      |\n",
      "|    reward             | -1.2891593 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 308        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -23.1      |\n",
      "|    reward             | 0.44475418 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.49       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-07-03 to  2018-10-02\n",
      "A2C Sharpe Ratio:  0.6142788247658761\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 503        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.31137642 |\n",
      "-----------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 432           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 9             |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0065866075  |\n",
      "|    clip_fraction        | 0.0588        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -7.09         |\n",
      "|    explained_variance   | -0.0219       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.31          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00457      |\n",
      "|    reward               | -0.0100288745 |\n",
      "|    std                  | 0.998         |\n",
      "|    value_loss           | 3.46          |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 446          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064136386 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0117      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.53         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    reward               | 0.042941656  |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.05         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 482          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074554225 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.00559      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.06         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00444     |\n",
      "|    reward               | -0.14840204  |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.15         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 508          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070836022 |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.00355      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.15         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00533     |\n",
      "|    reward               | 0.58125293   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.48         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-07-03 to  2018-10-02\n",
      "PPO Sharpe Ratio:  0.24981850719146848\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_1\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 232      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 8556     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -848     |\n",
      "|    critic_loss     | 107      |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8455     |\n",
      "|    reward          | 0.84292  |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2018-07-03 to  2018-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-10-02\n",
      "======Trading from:  2018-10-02 to  2019-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_378_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 346          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.12        |\n",
      "|    explained_variance | -0.143       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -9.19        |\n",
      "|    reward             | 0.0015536635 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 2.16         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | -0.0706   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.914     |\n",
      "|    reward             | 1.3511715 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 359        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.12      |\n",
      "|    reward             | -2.1054084 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.44       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 357        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -16.7      |\n",
      "|    reward             | 0.21275307 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 349        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | -2.1873338 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 341        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 4.08       |\n",
      "|    reward             | 0.41893074 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.972      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 10.8       |\n",
      "|    reward             | -1.2137305 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 330       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 12.3      |\n",
      "|    reward             | 0.4214026 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 332       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -7.12     |\n",
      "|    reward             | 1.0838003 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6.32      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 328         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -20.3       |\n",
      "|    reward             | -0.42657572 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 12.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 326         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -0.00734    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -14.3       |\n",
      "|    reward             | -0.59076107 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.14        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 323        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -24.6      |\n",
      "|    reward             | 0.72627777 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 13.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -28.6      |\n",
      "|    reward             | -1.7655252 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 24.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 319        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 2.9        |\n",
      "|    reward             | -1.8239149 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.621      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 319         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -2.38e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 2.18        |\n",
      "|    reward             | -0.39952856 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.452       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 320         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 14.2        |\n",
      "|    reward             | 0.123258255 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 4.98        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 322        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -7.53      |\n",
      "|    reward             | 0.18621112 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 320         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -3.85       |\n",
      "|    reward             | -0.17810541 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 0.612       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 319       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -2.88     |\n",
      "|    reward             | 0.7339826 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 0.251     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 318         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -2.15       |\n",
      "|    reward             | -0.39418977 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.648       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2018-10-02 to  2019-01-03\n",
      "A2C Sharpe Ratio:  -0.27103803669553667\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_378_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 494          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.021389242 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056971447 |\n",
      "|    clip_fraction        | 0.073        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0228      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.5          |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0085      |\n",
      "|    reward               | 0.10812597   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.99         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 453          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.007061433  |\n",
      "|    clip_fraction        | 0.0686       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.00589      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00673     |\n",
      "|    reward               | 0.0006142799 |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.33         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 453         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006471452 |\n",
      "|    clip_fraction        | 0.0721      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.00716     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00611    |\n",
      "|    reward               | 0.041488234 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 448           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 22            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008053197   |\n",
      "|    clip_fraction        | 0.0857        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -7.1          |\n",
      "|    explained_variance   | -0.01         |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.41          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00561      |\n",
      "|    reward               | -0.0024044393 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.64          |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2018-10-02 to  2019-01-03\n",
      "PPO Sharpe Ratio:  -0.3360251991489326\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 145        |\n",
      "|    time_elapsed    | 60         |\n",
      "|    total_timesteps | 8808       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 909        |\n",
      "|    critic_loss     | 49.2       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8707       |\n",
      "|    reward          | 0.90583575 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-10-02 to  2019-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-01-03\n",
      "======Trading from:  2019-01-03 to  2019-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_441_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 359         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -8.75       |\n",
      "|    reward             | -0.01987793 |\n",
      "|    std                | 0.979       |\n",
      "|    value_loss         | 1.58        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 330        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | -0.203     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.986     |\n",
      "|    reward             | 0.35602933 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 0.918      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 309        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -0.0197    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -2.22      |\n",
      "|    reward             | -1.7362056 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 1.22       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 307       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0.0537    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.83      |\n",
      "|    reward             | 0.8076461 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 311         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | 0.0019      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 9.99        |\n",
      "|    reward             | 0.124828726 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 2.65        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 308         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | -0.0486     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.83       |\n",
      "|    reward             | -0.12868398 |\n",
      "|    std                | 0.976       |\n",
      "|    value_loss         | 0.706       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 310         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | -0.254      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 1.81        |\n",
      "|    reward             | 0.004461133 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 0.0784      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 305         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 13          |\n",
      "|    reward             | 0.036421433 |\n",
      "|    std                | 0.979       |\n",
      "|    value_loss         | 6.78        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 302       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 10.2      |\n",
      "|    reward             | 1.0783752 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 13.8      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 305       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0.00132   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.779    |\n",
      "|    reward             | 2.4163198 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 3.44      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 307       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.574    |\n",
      "|    reward             | 1.5663425 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 0.221     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 15.8       |\n",
      "|    reward             | -0.4703039 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 7.63       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 304         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 7.49        |\n",
      "|    reward             | -0.79467773 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 2.63        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 307        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -7.89      |\n",
      "|    reward             | 0.05085547 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 1.58       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 308       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -13.1     |\n",
      "|    reward             | -0.074954 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 5.09      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 309      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7       |\n",
      "|    explained_variance | -0.0189  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -18.1    |\n",
      "|    reward             | -0.2303  |\n",
      "|    std                | 0.982    |\n",
      "|    value_loss         | 7.57     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 309        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -0.0158    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -7.39      |\n",
      "|    reward             | -1.9035996 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 309       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -6.78     |\n",
      "|    reward             | -1.714082 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 2.48      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 308        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -31.3      |\n",
      "|    reward             | -2.2616456 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 28.5       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 307         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 32          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -6.86       |\n",
      "|    reward             | -0.64454806 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 1.99        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2019-01-03 to  2019-04-04\n",
      "A2C Sharpe Ratio:  0.5724393321259554\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_441_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 476          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.039659634 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 421          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069191623 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.114       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.29         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00446     |\n",
      "|    reward               | -0.06878543  |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.38         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 425        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 14         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00499848 |\n",
      "|    clip_fraction        | 0.0563     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.08      |\n",
      "|    explained_variance   | -0.00722   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.6        |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00518   |\n",
      "|    reward               | 0.31618518 |\n",
      "|    std                  | 0.997      |\n",
      "|    value_loss           | 3.59       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 419         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00603461  |\n",
      "|    clip_fraction        | 0.0508      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00192    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.51        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    reward               | -0.49953097 |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 4.89        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059621073 |\n",
      "|    clip_fraction        | 0.0598       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.06        |\n",
      "|    explained_variance   | 0.000647     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.39         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00589     |\n",
      "|    reward               | -0.08133137  |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 3.98         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-01-03 to  2019-04-04\n",
      "PPO Sharpe Ratio:  0.4205141320095989\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 143        |\n",
      "|    time_elapsed    | 62         |\n",
      "|    total_timesteps | 9060       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 154        |\n",
      "|    critic_loss     | 13.6       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8959       |\n",
      "|    reward          | 0.26147386 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2019-01-03 to  2019-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-04-04\n",
      "======Trading from:  2019-04-04 to  2019-07-05\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_504_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 280         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -0.212      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.4       |\n",
      "|    reward             | -0.13353986 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 3.87        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 5.79      |\n",
      "|    reward             | 1.30935   |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 2.46      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 318        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -0.00612   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14.3      |\n",
      "|    reward             | -1.6660435 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 5.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -16.7      |\n",
      "|    reward             | -0.4344926 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 7.37       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 329        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 20.2       |\n",
      "|    reward             | 0.05361229 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 7.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 328        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -7.01      |\n",
      "|    reward             | 0.49990678 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 318       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 2.9       |\n",
      "|    reward             | 1.2163235 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 1.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 319        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -5.37      |\n",
      "|    reward             | -1.6383116 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0.0281    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -13.8     |\n",
      "|    reward             | 0.8248366 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 3.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 326        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 0.99       |\n",
      "|    reward             | -1.8616854 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 2.28       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 325         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 13.5        |\n",
      "|    reward             | -0.19988228 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 7.53        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 327        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -37.7      |\n",
      "|    reward             | 0.90043384 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 37.2       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 322      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7       |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 33.2     |\n",
      "|    reward             | -1.1656  |\n",
      "|    std                | 0.982    |\n",
      "|    value_loss         | 22.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 320        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -7.96      |\n",
      "|    reward             | 0.30167142 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 6.52       |\n",
      "|    reward             | 0.21677783 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 1.47       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 319       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 2.36      |\n",
      "|    reward             | 0.2545442 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 0.396     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 319        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -36.5      |\n",
      "|    reward             | 0.40038952 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 27.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 320       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 2.62      |\n",
      "|    reward             | 0.6211366 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 0.294     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 319       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 11.4      |\n",
      "|    reward             | 0.4652496 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 3.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 318        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -0.00712   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 2.68       |\n",
      "|    reward             | -1.8083895 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-04-04 to  2019-07-05\n",
      "A2C Sharpe Ratio:  0.013077949403097621\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 487        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.08687173 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 462          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065552043 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0392      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00599     |\n",
      "|    reward               | 0.2368921    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.69         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 437         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007947978 |\n",
      "|    clip_fraction        | 0.0605      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.0292      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.74        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00329    |\n",
      "|    reward               | 0.5174713   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.82        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 426         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006146248 |\n",
      "|    clip_fraction        | 0.0649      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0474      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00461    |\n",
      "|    reward               | 1.3874717   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.24        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 423          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070440248 |\n",
      "|    clip_fraction        | 0.0709       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | 0.0014       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.03         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0051      |\n",
      "|    reward               | -0.07148179  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.67         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-04-04 to  2019-07-05\n",
      "PPO Sharpe Ratio:  0.0019012468850541009\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 144        |\n",
      "|    time_elapsed    | 64         |\n",
      "|    total_timesteps | 9312       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -411       |\n",
      "|    critic_loss     | 6.68       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 9211       |\n",
      "|    reward          | 0.33032054 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2019-04-04 to  2019-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-07-05\n",
      "======Trading from:  2019-07-05 to  2019-10-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_567_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 283        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -14.6      |\n",
      "|    reward             | 0.26851925 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 5.06       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 281       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.2      |\n",
      "|    reward             | 1.8643231 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 2.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 280        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0.0117     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -12.4      |\n",
      "|    reward             | -2.8614612 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 6.25       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 281         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -27.8       |\n",
      "|    reward             | -0.53082657 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 17.9        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 282      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 12.5     |\n",
      "|    reward             | 2.583075 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 6.19     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 280       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.855    |\n",
      "|    reward             | 0.5399525 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 289         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 17          |\n",
      "|    reward             | -0.23417664 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 6.9         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -2.53     |\n",
      "|    reward             | 1.4544663 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.34      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 16.9      |\n",
      "|    reward             | 1.7034923 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 14        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 302        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 10.1       |\n",
      "|    reward             | -1.6205606 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 3.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 302        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 7.76       |\n",
      "|    reward             | -1.8154703 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 1.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 37.3       |\n",
      "|    reward             | -0.6647238 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 24.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 308        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 6.92       |\n",
      "|    reward             | -1.1234651 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 1.08       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 306        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 46.4       |\n",
      "|    reward             | -14.218854 |\n",
      "|    std                | 0.978      |\n",
      "|    value_loss         | 69.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 308       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -8.37     |\n",
      "|    reward             | 2.5064955 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 2.7       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 310        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 14.2       |\n",
      "|    reward             | -1.6703079 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 6.63       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 311        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -1.28      |\n",
      "|    reward             | -0.4764413 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 312        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 16.9       |\n",
      "|    reward             | 0.63191235 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 6.16       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 313      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 30       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -25.2    |\n",
      "|    reward             | 2.039922 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 21.5     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 312       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 9.9       |\n",
      "|    reward             | 1.4527221 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 5.76      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2019-07-05 to  2019-10-03\n",
      "A2C Sharpe Ratio:  -0.12646354773801122\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_567_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 416         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.04992097 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 414         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007867228 |\n",
      "|    clip_fraction        | 0.0846      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0584     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00732    |\n",
      "|    reward               | -1.0636438  |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 3.84        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061796205 |\n",
      "|    clip_fraction        | 0.0438       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.00721     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.86         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00538     |\n",
      "|    reward               | 0.30350295   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.72         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 421         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008238308 |\n",
      "|    clip_fraction        | 0.0877      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.16       |\n",
      "|    explained_variance   | 0.00553     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.79        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00887    |\n",
      "|    reward               | 0.026110362 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.21        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 424         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005458917 |\n",
      "|    clip_fraction        | 0.0436      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.18       |\n",
      "|    explained_variance   | -0.000678   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.17        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00332    |\n",
      "|    reward               | 0.08713022  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 3.35        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-07-05 to  2019-10-03\n",
      "PPO Sharpe Ratio:  -0.19043246797360944\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 144       |\n",
      "|    time_elapsed    | 66        |\n",
      "|    total_timesteps | 9564      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -259      |\n",
      "|    critic_loss     | 2.84      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9463      |\n",
      "|    reward          | 1.1875563 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-07-05 to  2019-10-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-10-03\n",
      "======Trading from:  2019-10-03 to  2020-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-10-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_630_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 353         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -9.06       |\n",
      "|    reward             | -0.12474252 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 2.53        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 314        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -6.47      |\n",
      "|    reward             | 0.59525824 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 2.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 305        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.75      |\n",
      "|    reward             | -1.7258402 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 2.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 311        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3         |\n",
      "|    reward             | 0.32812208 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.343      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 306       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 6.05      |\n",
      "|    reward             | 0.3449701 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 1.45      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 303         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 1.2         |\n",
      "|    reward             | -0.45751685 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 0.0729      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 304        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 5.58       |\n",
      "|    reward             | -1.2985072 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 0.756      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 309       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 19        |\n",
      "|    reward             | 1.6091895 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 14        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 306        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 3.14       |\n",
      "|    reward             | -1.4987769 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.99       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 305         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 1.96        |\n",
      "|    reward             | -0.18643579 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 10.1        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 308         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -1.72       |\n",
      "|    reward             | -0.90463185 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.177       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 312       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 4.41      |\n",
      "|    reward             | 0.7851621 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.812     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 315       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -2.81     |\n",
      "|    reward             | 0.8020371 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.482     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 313        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 2.22       |\n",
      "|    reward             | 0.87804055 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.854      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 313      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 23       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 6.6      |\n",
      "|    reward             | 2.3222   |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.44     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 314         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -9.97       |\n",
      "|    reward             | -0.20780508 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.31        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 315       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.955    |\n",
      "|    reward             | 0.7496336 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.353     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 315         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 1.53        |\n",
      "|    reward             | -0.37550077 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 11.6        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 316         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.19       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -15.4       |\n",
      "|    reward             | -0.25391093 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 5.81        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 317        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 7.66       |\n",
      "|    reward             | -0.5123109 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-10-03 to  2020-01-03\n",
      "A2C Sharpe Ratio:  0.6536300955078401\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_630_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 471         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.05071571 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 411         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007457943 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.0193     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.24        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00512    |\n",
      "|    reward               | -0.09331102 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007153198 |\n",
      "|    clip_fraction        | 0.0567      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0115      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.36        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00434    |\n",
      "|    reward               | -1.0477502  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 398         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009588572 |\n",
      "|    clip_fraction        | 0.0788      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.00488     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 3.06        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    reward               | 1.0303057   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004379238 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.0062      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00215    |\n",
      "|    reward               | -0.59857595 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.29        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-10-03 to  2020-01-03\n",
      "PPO Sharpe Ratio:  0.38290678514005255\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 162        |\n",
      "|    time_elapsed    | 60         |\n",
      "|    total_timesteps | 9816       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -1.49e+03  |\n",
      "|    critic_loss     | 201        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 9715       |\n",
      "|    reward          | -4.8188114 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2019-10-03 to  2020-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-01-03\n",
      "======Trading from:  2020-01-03 to  2020-04-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_693_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0.072     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -12.4     |\n",
      "|    reward             | 0.4483116 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 3.92      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 514        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 0.512      |\n",
      "|    reward             | 0.78751636 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.56       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 516        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.63      |\n",
      "|    reward             | -1.9940062 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 2.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.0718     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -5.87      |\n",
      "|    reward             | 0.16487159 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.859      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.0628    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -9.57      |\n",
      "|    reward             | -0.5496005 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.38       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 517         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -35.3       |\n",
      "|    reward             | -0.43377888 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 27.3        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -9.95      |\n",
      "|    reward             | 0.92732626 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 3.3        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 517         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -8.85       |\n",
      "|    reward             | -0.24556953 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 5.75        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.47     |\n",
      "|    reward             | 1.8161086 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 0.494     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 518        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 7.54       |\n",
      "|    reward             | 0.19007578 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 518       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -5.59     |\n",
      "|    reward             | 0.6974773 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 4.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 518        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 4.48       |\n",
      "|    reward             | 0.55603945 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 0.506      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 519         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 9.81        |\n",
      "|    reward             | -0.21627812 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 2.86        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 519       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -13.5     |\n",
      "|    reward             | 0.4341681 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 2.39      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 519       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 14.5      |\n",
      "|    reward             | 2.1648088 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 8.59      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 519         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 15.3        |\n",
      "|    reward             | -0.32487804 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 9.83        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 519      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -12.1    |\n",
      "|    reward             | 3.185699 |\n",
      "|    std                | 0.989    |\n",
      "|    value_loss         | 2.6      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 519        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -35.9      |\n",
      "|    reward             | -0.7470287 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 23.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 518        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0.00729    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 8.13       |\n",
      "|    reward             | 0.35840294 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 1.58       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -13       |\n",
      "|    reward             | -1.897958 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 4.23      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-01-03 to  2020-04-03\n",
      "A2C Sharpe Ratio:  -0.17896181079045329\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_693_1\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 689       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 2         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.1331937 |\n",
      "----------------------------------\n",
      "day: 2516, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 590087.59\n",
      "total_reward: -409912.41\n",
      "total_cost: 1145884.17\n",
      "total_trades: 9581\n",
      "Sharpe: -0.297\n",
      "=================================\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 650        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 6          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00666245 |\n",
      "|    clip_fraction        | 0.0747     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.11      |\n",
      "|    explained_variance   | -0.025     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.57       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0059    |\n",
      "|    reward               | 1.2867228  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 3.35       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 640         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007637389 |\n",
      "|    clip_fraction        | 0.0669      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.0046      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.85        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00683    |\n",
      "|    reward               | -0.12696952 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.28        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 635          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047455085 |\n",
      "|    clip_fraction        | 0.0542       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.00865     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.98         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00608     |\n",
      "|    reward               | 0.45672792   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.74         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 633         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005102895 |\n",
      "|    clip_fraction        | 0.0311      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.000808    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    reward               | 0.40655333  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.82        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-01-03 to  2020-04-03\n",
      "PPO Sharpe Ratio:  -0.2867836781761165\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_1\n",
      "day: 2516, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2434629.94\n",
      "total_reward: 1434629.94\n",
      "total_cost: 53259.21\n",
      "total_trades: 5252\n",
      "Sharpe: 0.614\n",
      "=================================\n",
      "======DDPG Validation from:  2020-01-03 to  2020-04-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-04-03\n",
      "======Trading from:  2020-04-03 to  2020-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-04-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_756_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 524       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 0         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.0242    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -8.82     |\n",
      "|    reward             | 0.2345313 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 2.53      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 526         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0.158       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | -1.45       |\n",
      "|    reward             | 0.082789026 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 0.705       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0.137      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | -1.0698502 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 3.16       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 501         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -11.7       |\n",
      "|    reward             | -0.27118587 |\n",
      "|    std                | 0.978       |\n",
      "|    value_loss         | 2.74        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 507         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -7.43       |\n",
      "|    reward             | -0.40714216 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 1.56        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 510        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 2.75       |\n",
      "|    reward             | -2.6044672 |\n",
      "|    std                | 0.978      |\n",
      "|    value_loss         | 6.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 512       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 11.3      |\n",
      "|    reward             | 1.5135037 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 3.34      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 514        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 0.116      |\n",
      "|    reward             | -1.1533151 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 0.839      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 516         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -31.7       |\n",
      "|    reward             | -0.17321011 |\n",
      "|    std                | 0.979       |\n",
      "|    value_loss         | 21.8        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 518         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 14.1        |\n",
      "|    reward             | -0.32568586 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 15.8        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 518       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -17.1     |\n",
      "|    reward             | 1.9288231 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 6.22      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 519       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 8.88      |\n",
      "|    reward             | 1.1869628 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 2.16      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 520       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 3.03      |\n",
      "|    reward             | 1.1810371 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.774     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 521       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 3.56      |\n",
      "|    reward             | 1.2358145 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 2.81      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 521      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.524   |\n",
      "|    reward             | 3.707389 |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 1.56     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 522         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -0.29       |\n",
      "|    reward             | -0.12963948 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 0.493       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 522        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 25.5       |\n",
      "|    reward             | 0.45051324 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 15.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 521       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 11.1      |\n",
      "|    reward             | -2.663585 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 2.76      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 521        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 3.29       |\n",
      "|    reward             | 0.24843699 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 520      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 19       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -42.1    |\n",
      "|    reward             | 9.749716 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 65.7     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-04-03 to  2020-07-06\n",
      "A2C Sharpe Ratio:  0.3290178443369478\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_756_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 677         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.11706584 |\n",
      "------------------------------------\n",
      "day: 2579, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 523471.92\n",
      "total_reward: -476528.08\n",
      "total_cost: 1209678.21\n",
      "total_trades: 9880\n",
      "Sharpe: -0.298\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 646          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059701186 |\n",
      "|    clip_fraction        | 0.074        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.0123      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.54         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00635     |\n",
      "|    reward               | 0.29116094   |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 3.64         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 637          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.00642342   |\n",
      "|    clip_fraction        | 0.053        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.00355      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.4          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00699     |\n",
      "|    reward               | -0.087503344 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 4.77         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 631          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056189606 |\n",
      "|    clip_fraction        | 0.0525       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.0148       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.71         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00399     |\n",
      "|    reward               | 1.430313     |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.88         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 628          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083263405 |\n",
      "|    clip_fraction        | 0.0646       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.00455     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.54         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00544     |\n",
      "|    reward               | -0.26081437  |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 4.07         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2020-04-03 to  2020-07-06\n",
      "PPO Sharpe Ratio:  0.19065155219554178\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_1\n",
      "day: 2579, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1473057.13\n",
      "total_reward: 473057.13\n",
      "total_cost: 998.75\n",
      "total_trades: 10312\n",
      "Sharpe: 0.307\n",
      "=================================\n",
      "======DDPG Validation from:  2020-04-03 to  2020-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-07-06\n",
      "======Trading from:  2020-07-06 to  2020-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_819_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 488          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.11        |\n",
      "|    explained_variance | -1.09        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -8.18        |\n",
      "|    reward             | -0.017221188 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 1.55         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 501        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.564     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | 1.63       |\n",
      "|    reward             | 0.32591817 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.501      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 504        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -2.41      |\n",
      "|    reward             | -1.3786389 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.785      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 506       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2.17      |\n",
      "|    reward             | 0.6065586 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.951     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 507         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | -0.0106     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -9.48       |\n",
      "|    reward             | -0.53258413 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 2.29        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 508         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -4.18       |\n",
      "|    reward             | -0.91708237 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.27        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 509        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -2.83      |\n",
      "|    reward             | -1.9642236 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.421      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 509      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 6.51     |\n",
      "|    reward             | 1.141641 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.58     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 510       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -6.44     |\n",
      "|    reward             | 1.3331109 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 509       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -29.3     |\n",
      "|    reward             | 2.0178056 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 21.2      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 509         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -0.0295     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -2.44       |\n",
      "|    reward             | -0.35030338 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.265       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 510        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -25.6      |\n",
      "|    reward             | -1.3136016 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 12.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 510         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.000157   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 3.76        |\n",
      "|    reward             | -0.29572937 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.5         |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 512       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -8.02     |\n",
      "|    reward             | 0.7278096 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 513        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -92.6      |\n",
      "|    reward             | -3.0800095 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 156        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 514        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 1.59       |\n",
      "|    reward             | -1.1894175 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 515        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 2.92       |\n",
      "|    reward             | -2.0154884 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.419      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 516        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 6.43       |\n",
      "|    reward             | -1.5763236 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 517       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -4.85     |\n",
      "|    reward             | 1.0235499 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.912     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 517        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -16.1      |\n",
      "|    reward             | -3.5813565 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 7.15       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-07-06 to  2020-10-02\n",
      "A2C Sharpe Ratio:  0.21603172661161485\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_819_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 699         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 2           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.07326123 |\n",
      "------------------------------------\n",
      "day: 2642, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 485562.19\n",
      "total_reward: -514437.81\n",
      "total_cost: 1159281.41\n",
      "total_trades: 10101\n",
      "Sharpe: -0.310\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056260615 |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0412      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00435     |\n",
      "|    reward               | 0.095561184  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.81         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 648         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007583214 |\n",
      "|    clip_fraction        | 0.0395      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.00131     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.68        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    reward               | 0.40017602  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 5.3         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050452957 |\n",
      "|    clip_fraction        | 0.0315       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.15        |\n",
      "|    explained_variance   | 0.0216       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.39         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00194     |\n",
      "|    reward               | -0.31023577  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.38         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 635         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008136584 |\n",
      "|    clip_fraction        | 0.0817      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.15       |\n",
      "|    explained_variance   | 0.0344      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.93        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    reward               | 0.1850858   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.24        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-06 to  2020-10-02\n",
      "PPO Sharpe Ratio:  0.14909999673051916\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_1\n",
      "day: 2642, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2344052.69\n",
      "total_reward: 1344052.69\n",
      "total_cost: 1768.67\n",
      "total_trades: 5290\n",
      "Sharpe: 0.551\n",
      "=================================\n",
      "======DDPG Validation from:  2020-07-06 to  2020-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-10-02\n",
      "======Trading from:  2020-10-02 to  2021-01-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_882_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 430        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | -0.033     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | 0.45814037 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.6        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 457      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.19    |\n",
      "|    explained_variance | -0.00768 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 4.75     |\n",
      "|    reward             | 1.4233   |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 2.94     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 463        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | -1.7815471 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 8.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 470        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -20.3      |\n",
      "|    reward             | -0.4828522 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 8.86       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 474         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -9.98       |\n",
      "|    reward             | -0.55478334 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.19        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 476        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 2.08       |\n",
      "|    reward             | -0.5947056 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 478        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -0.00537   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -3.64      |\n",
      "|    reward             | 0.42302728 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.59       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0.0612     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 3.65       |\n",
      "|    reward             | 0.13977173 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.512      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 9.7        |\n",
      "|    reward             | 0.03853125 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.000574  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -6.33      |\n",
      "|    reward             | -0.9119016 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.38       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | -0.3466709 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.18       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 480        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0.234      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 8.71       |\n",
      "|    reward             | -0.5737753 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.18       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 481         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -0.0491     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 0.677       |\n",
      "|    reward             | -0.19897588 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.0946      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.00584   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 7.15       |\n",
      "|    reward             | 0.12539122 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.00271   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 5.43       |\n",
      "|    reward             | -1.6677583 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 482        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 82.3       |\n",
      "|    reward             | 0.25977063 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 204        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0.0112     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 34.2       |\n",
      "|    reward             | -1.3943603 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 13.7       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 482       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 1.43      |\n",
      "|    reward             | 0.7995507 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 483       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -8.86     |\n",
      "|    reward             | 1.2456036 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 484        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 8.48       |\n",
      "|    reward             | 0.13271523 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.02       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-10-02 to  2021-01-04\n",
      "A2C Sharpe Ratio:  0.3483377415118481\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_882_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 652        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 3          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.11820896 |\n",
      "-----------------------------------\n",
      "day: 2705, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 421221.25\n",
      "total_reward: -578778.75\n",
      "total_cost: 1195981.76\n",
      "total_trades: 10396\n",
      "Sharpe: -0.373\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 623          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006639913  |\n",
      "|    clip_fraction        | 0.0812       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0188      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.14         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0078      |\n",
      "|    reward               | -0.017972274 |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.74         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060556782 |\n",
      "|    clip_fraction        | 0.0515       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | -0.0117      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00421     |\n",
      "|    reward               | 0.7159687    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 4.54         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 596          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073710247 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.0125       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00511     |\n",
      "|    reward               | -0.041026328 |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 4.86         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 586         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004937269 |\n",
      "|    clip_fraction        | 0.0358      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | 0.0432      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00409    |\n",
      "|    reward               | 0.20230578  |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 2.85        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-10-02 to  2021-01-04\n",
      "PPO Sharpe Ratio:  0.18158110711378841\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_1\n",
      "day: 2705, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3161861.93\n",
      "total_reward: 2161861.93\n",
      "total_cost: 998.64\n",
      "total_trades: 8114\n",
      "Sharpe: 0.700\n",
      "=================================\n",
      "======DDPG Validation from:  2020-10-02 to  2021-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-01-04\n",
      "======Trading from:  2021-01-04 to  2021-04-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_945_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 488         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.8       |\n",
      "|    reward             | 0.006137744 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.33        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 494       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -2.85     |\n",
      "|    reward             | 0.8578301 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.97      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 498        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.83      |\n",
      "|    reward             | -2.7668135 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.24       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 500        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -5.03      |\n",
      "|    reward             | 0.40835968 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.11       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 493         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -15.6       |\n",
      "|    reward             | -0.89549136 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 6.22        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 490      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 6        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 2.59     |\n",
      "|    reward             | 1.01021  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.716    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 490       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 7.88      |\n",
      "|    reward             | 1.0077337 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.96      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 488        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -0.0472    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -0.553     |\n",
      "|    reward             | 0.85811913 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.0248     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 489         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 32.1        |\n",
      "|    reward             | -0.51693726 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 22.9        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 490       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 23.8      |\n",
      "|    reward             | -5.670896 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 23.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 491         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -56.4       |\n",
      "|    reward             | -0.62792736 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 106         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 492        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 0.539      |\n",
      "|    reward             | -2.8054912 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 492       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 5.7       |\n",
      "|    reward             | 1.0506092 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.28      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 492       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0.00896   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 7.59      |\n",
      "|    reward             | 1.4877912 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 493       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.41      |\n",
      "|    reward             | 1.5100895 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.688     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 493       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -15.5     |\n",
      "|    reward             | 2.2425952 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 8.99      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 493       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -1.76     |\n",
      "|    reward             | 0.6102187 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.206     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 493         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -11.1       |\n",
      "|    reward             | -0.28609988 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 2.28        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 494         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -12.3       |\n",
      "|    reward             | -0.41963354 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 3.82        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 494        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -3.31      |\n",
      "|    reward             | -1.2413926 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 1.37       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  0.3896882850044252\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_945_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 652         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.024848957 |\n",
      "------------------------------------\n",
      "day: 2768, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 505243.66\n",
      "total_reward: -494756.34\n",
      "total_cost: 1138261.69\n",
      "total_trades: 10518\n",
      "Sharpe: -0.276\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041998597 |\n",
      "|    clip_fraction        | 0.0537       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0081      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00469     |\n",
      "|    reward               | 0.16190207   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.04         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 607          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059905755 |\n",
      "|    clip_fraction        | 0.0413       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.0046       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0052      |\n",
      "|    reward               | -1.2503512   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.28         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 604         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006219711 |\n",
      "|    clip_fraction        | 0.0528      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.0236      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.13        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00603    |\n",
      "|    reward               | 0.6498904   |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 4.7         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 603          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062646903 |\n",
      "|    clip_fraction        | 0.0754       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.000526     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.11         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00797     |\n",
      "|    reward               | -0.021697044 |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.87         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  0.3011635668133988\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_1\n",
      "day: 2768, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4882608.95\n",
      "total_reward: 3882608.95\n",
      "total_cost: 998.99\n",
      "total_trades: 8301\n",
      "Sharpe: 0.863\n",
      "=================================\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1008_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 469       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -13.5     |\n",
      "|    reward             | 0.4638514 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 3.93      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 468      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 7.13     |\n",
      "|    reward             | 1.497475 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 3.05     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 476        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | -1.9352618 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 8.17       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 482        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -26.3      |\n",
      "|    reward             | -0.5075895 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 483        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -12.2      |\n",
      "|    reward             | -0.6790368 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 7.83       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 484       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -10.8     |\n",
      "|    reward             | 1.0398302 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 3.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 485        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 8.49       |\n",
      "|    reward             | 0.16324912 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 2.39       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 486       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 17.3      |\n",
      "|    reward             | 0.7774041 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 9.83      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 4.05      |\n",
      "|    reward             | 0.4754435 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.996     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 486       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 26.1      |\n",
      "|    reward             | 2.0804186 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 23.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 486       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 26.6      |\n",
      "|    reward             | 0.9858642 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 19.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 14.8       |\n",
      "|    reward             | -0.6401001 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 5.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 12.8       |\n",
      "|    reward             | -1.2389145 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 486        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 0.6        |\n",
      "|    reward             | -1.3201263 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 487       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 2.1862538 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 3.3       |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 487          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.02        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -7.01        |\n",
      "|    reward             | -0.101219535 |\n",
      "|    std                | 0.986        |\n",
      "|    value_loss         | 1.99         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 487        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 26.9       |\n",
      "|    reward             | 0.48796126 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 20.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 488       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 14        |\n",
      "|    reward             | 1.8668685 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 6.81      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 489          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 19           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.99        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 14.4         |\n",
      "|    reward             | -0.052320704 |\n",
      "|    std                | 0.981        |\n",
      "|    value_loss         | 4.03         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 489       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 14.6      |\n",
      "|    reward             | 1.9782962 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 5.08      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.23972702031280071\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1008_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 656         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.10166826 |\n",
      "------------------------------------\n",
      "day: 2831, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 495537.91\n",
      "total_reward: -504462.09\n",
      "total_cost: 1188708.70\n",
      "total_trades: 10780\n",
      "Sharpe: -0.274\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 617          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066572786 |\n",
      "|    clip_fraction        | 0.0506       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.112       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.67         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    reward               | -0.6539593   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.64         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 606         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007431482 |\n",
      "|    clip_fraction        | 0.0479      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0163      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.45        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00501    |\n",
      "|    reward               | 2.8084447   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.38        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 600         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005004134 |\n",
      "|    clip_fraction        | 0.0428      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0565      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    reward               | -0.3894862  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.67        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 594         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008784214 |\n",
      "|    clip_fraction        | 0.097       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0245      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0093     |\n",
      "|    reward               | 0.12452645  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  0.10295188444557561\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1008_1\n",
      "day: 2831, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2426944.73\n",
      "total_reward: 1426944.73\n",
      "total_cost: 999.54\n",
      "total_trades: 5664\n",
      "Sharpe: 0.541\n",
      "=================================\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1071_1\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 470       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -0.12     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -10.4     |\n",
      "|    reward             | 0.4438913 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 3.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 475       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -0.188    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.54     |\n",
      "|    reward             | 1.4080125 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 2.31      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 480        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -12.8      |\n",
      "|    reward             | -1.6775712 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 6.6        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 482         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0.0687      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -21.1       |\n",
      "|    reward             | -0.42448187 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 6.87        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 484         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -7.36       |\n",
      "|    reward             | -0.48951662 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 5.16        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 485        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 26.5       |\n",
      "|    reward             | -5.1603036 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 16         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 481       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -2.28     |\n",
      "|    reward             | 1.6516472 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 4.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 483       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 1.21      |\n",
      "|    reward             | 2.4639406 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 484        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -13.8      |\n",
      "|    reward             | -1.2573509 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 3.34       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 485      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 26.8     |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 0.989    |\n",
      "|    value_loss         | 17.3     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 485        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -48.4      |\n",
      "|    reward             | -1.2952838 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 64.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 483       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 3.37      |\n",
      "|    reward             | 2.2182868 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 0.625     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 484        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 2.62       |\n",
      "|    reward             | 0.49071336 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.02       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 25.1      |\n",
      "|    reward             | 3.0312893 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 13.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 485       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -21.5     |\n",
      "|    reward             | 1.1047933 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 8.99      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 479        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -41.1      |\n",
      "|    reward             | 0.20007752 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 42.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 479       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 47.4      |\n",
      "|    reward             | 2.6699347 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 45.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 480        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 3.52       |\n",
      "|    reward             | 0.40377566 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -3.77      |\n",
      "|    reward             | -0.7122926 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 0.562      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 481        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 12.7       |\n",
      "|    reward             | -0.2901577 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 3.81       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.02415825657451785\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1071_1\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 638         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 3           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.011480908 |\n",
      "------------------------------------\n",
      "day: 2894, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 417327.84\n",
      "total_reward: -582672.16\n",
      "total_cost: 1048813.33\n",
      "total_trades: 10883\n",
      "Sharpe: -0.351\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 608         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004823231 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0244     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00512    |\n",
      "|    reward               | -0.93276423 |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 3.26        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 602        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00604558 |\n",
      "|    clip_fraction        | 0.0447     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.09      |\n",
      "|    explained_variance   | 0.0276     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.98       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00399   |\n",
      "|    reward               | 0.30684698 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 3.42       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 599         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007948408 |\n",
      "|    clip_fraction        | 0.0753      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.22        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00563    |\n",
      "|    reward               | 0.056936212 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.11        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 597         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005683898 |\n",
      "|    clip_fraction        | 0.0437      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.00386     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.2         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.003      |\n",
      "|    reward               | 0.40402713  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.94        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -0.31423781871060535\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1071_1\n",
      "day: 2894, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5365710.64\n",
      "total_reward: 4365710.64\n",
      "total_cost: 1510.87\n",
      "total_trades: 5792\n",
      "Sharpe: 0.868\n",
      "=================================\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1134_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0.283      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | 0.28574315 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 2.54       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -0.147    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.393     |\n",
      "|    reward             | 0.8676451 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 462        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.0112    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | -1.3242515 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 3.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 462         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -15.9       |\n",
      "|    reward             | -0.28775722 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 3.83        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 462         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -0.089      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -4.86       |\n",
      "|    reward             | -0.46556586 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 1.02        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -2.56     |\n",
      "|    reward             | 1.8696889 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.301     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 458         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -2.56       |\n",
      "|    reward             | -0.40528387 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 0.769       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 459         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 24          |\n",
      "|    reward             | -0.09765795 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 15.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 459        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0.00123    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -27.6      |\n",
      "|    reward             | 0.65032804 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 21.1       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | -41.8    |\n",
      "|    reward             | 1.133432 |\n",
      "|    std                | 0.994    |\n",
      "|    value_loss         | 76.6     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -0.308    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 6.43      |\n",
      "|    reward             | 3.1757889 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 2.85      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -14.6     |\n",
      "|    reward             | -1.044593 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 4.63      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -0.00185  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -6.88     |\n",
      "|    reward             | 0.8026503 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 1.63      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 15.5       |\n",
      "|    reward             | -3.1295307 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 5.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 459        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 23.4       |\n",
      "|    reward             | 0.32958966 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 11.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -0.0288   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -6.82     |\n",
      "|    reward             | 2.4168344 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 10.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -0.0853   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -37.4     |\n",
      "|    reward             | 2.9748585 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 49.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 460         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -10.9       |\n",
      "|    reward             | 0.027933886 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 4.24        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 460      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.07    |\n",
      "|    explained_variance | -0.724   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 9.58     |\n",
      "|    reward             | 1.111629 |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 3.49     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 7.91       |\n",
      "|    reward             | 0.65527344 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.5        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  0.2208541133670137\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1134_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 616          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 3            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.084039725 |\n",
      "-------------------------------------\n",
      "day: 2957, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 544208.82\n",
      "total_reward: -455791.18\n",
      "total_cost: 1265319.61\n",
      "total_trades: 11227\n",
      "Sharpe: -0.225\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 576          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064989123 |\n",
      "|    clip_fraction        | 0.0574       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0238      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    reward               | 0.4638651    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.8          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 575         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009304555 |\n",
      "|    clip_fraction        | 0.0901      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.000887    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00612    |\n",
      "|    reward               | -0.16219132 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.65        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 570         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005320626 |\n",
      "|    clip_fraction        | 0.0615      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.0122      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.23        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.005      |\n",
      "|    reward               | 1.0548148   |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 5.66        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 567          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066227517 |\n",
      "|    clip_fraction        | 0.0628       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.00262     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.24         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00554     |\n",
      "|    reward               | -0.30263337  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 5.87         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
      "PPO Sharpe Ratio:  0.09493764301325554\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1134_1\n",
      "day: 2957, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2395041.10\n",
      "total_reward: 1395041.10\n",
      "total_cost: 998.66\n",
      "total_trades: 5914\n",
      "Sharpe: 0.504\n",
      "=================================\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
      "======Trading from:  2022-01-03 to  2022-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1197_1\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 436          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | -0.0117      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -10.6        |\n",
      "|    reward             | -0.023928402 |\n",
      "|    std                | 0.993        |\n",
      "|    value_loss         | 2.7          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 448       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.67     |\n",
      "|    reward             | 0.6912034 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 453       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -7.81     |\n",
      "|    reward             | -2.338504 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 3         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.95      |\n",
      "|    reward             | 0.22740747 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 0.781      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 454         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -11.8       |\n",
      "|    reward             | -0.74226636 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 4.24        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 453       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 2.01      |\n",
      "|    reward             | 4.7852864 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 0.512     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 456       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -21.2     |\n",
      "|    reward             | -0.275757 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 11.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 15.3      |\n",
      "|    reward             | 1.2756863 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 3.64      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -13.8     |\n",
      "|    reward             | -0.242012 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 461         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 4.02        |\n",
      "|    reward             | -0.20280598 |\n",
      "|    std                | 0.986       |\n",
      "|    value_loss         | 0.398       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 461       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 1.55      |\n",
      "|    reward             | 0.7383514 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 1.06      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 461       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 16.7      |\n",
      "|    reward             | 1.4861223 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 6.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -3.34     |\n",
      "|    reward             | 1.1368507 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 1.07      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 460         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 14.5        |\n",
      "|    reward             | -0.74750686 |\n",
      "|    std                | 0.981       |\n",
      "|    value_loss         | 6.93        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 460         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 8.88        |\n",
      "|    reward             | 0.047558203 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 2.71        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 459         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 16.6        |\n",
      "|    reward             | -0.37127206 |\n",
      "|    std                | 0.982       |\n",
      "|    value_loss         | 3.76        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -5.28e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -55.8     |\n",
      "|    reward             | 1.2390691 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 61.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 60.5      |\n",
      "|    reward             | -3.867238 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 111       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 459      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 20       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.97    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -10.6    |\n",
      "|    reward             | 1.486368 |\n",
      "|    std                | 0.976    |\n",
      "|    value_loss         | 3.19     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 459         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 7.91        |\n",
      "|    reward             | -0.39072108 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 1.12        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-01-03 to  2022-04-04\n",
      "A2C Sharpe Ratio:  -0.22723516323635173\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1197_1\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 612          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 3            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.023318408 |\n",
      "-------------------------------------\n",
      "day: 3020, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 390717.55\n",
      "total_reward: -609282.45\n",
      "total_cost: 1196263.94\n",
      "total_trades: 11230\n",
      "Sharpe: -0.381\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 576         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005489525 |\n",
      "|    clip_fraction        | 0.0616      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.00744    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    reward               | -1.1101875  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.17        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 567          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076068463 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.00267     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.47         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00912     |\n",
      "|    reward               | -1.0715059   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.66         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 561          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0043499605 |\n",
      "|    clip_fraction        | 0.0571       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.00499     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    reward               | -0.2523829   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.25         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 561         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008849554 |\n",
      "|    clip_fraction        | 0.0726      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0197     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00598    |\n",
      "|    reward               | -0.20427017 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.92        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-01-03 to  2022-04-04\n",
      "PPO Sharpe Ratio:  -0.34764892794594826\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1197_1\n",
      "day: 3020, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3433938.51\n",
      "total_reward: 2433938.51\n",
      "total_cost: 998.59\n",
      "total_trades: 6040\n",
      "Sharpe: 0.685\n",
      "=================================\n",
      "======DDPG Validation from:  2022-01-03 to  2022-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
      "======Trading from:  2022-04-04 to  2022-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1260_1\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 463        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -0.241     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -8.67      |\n",
      "|    reward             | 0.04627528 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.81       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 459       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0.0946    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.554    |\n",
      "|    reward             | 1.6648195 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.5       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 457        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0.0397     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -13        |\n",
      "|    reward             | -2.5021963 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 5.71       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 459         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -25.7       |\n",
      "|    reward             | -0.38000697 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 13.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -0.0394    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -17.2      |\n",
      "|    reward             | -1.1809071 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 9.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0.00134   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -15.6     |\n",
      "|    reward             | 3.5868382 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 13.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 458       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -40       |\n",
      "|    reward             | 2.5002565 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 53.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 459        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -18.6      |\n",
      "|    reward             | 0.80878806 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 8.58       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -14        |\n",
      "|    reward             | -1.7087324 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 5.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 7.27      |\n",
      "|    reward             | 3.4927769 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 0.993     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -70.5     |\n",
      "|    reward             | -3.346899 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 111       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -0.0759   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 14.3      |\n",
      "|    reward             | 0.8016507 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 14.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 11.4       |\n",
      "|    reward             | 0.11448633 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 3.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 460       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -29.9     |\n",
      "|    reward             | 0.8766578 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 21.3      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 460      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.89     |\n",
      "|    reward             | 0.83265  |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 1.3      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 460        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -7.57      |\n",
      "|    reward             | -1.3303399 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.31       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 461      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 35.2     |\n",
      "|    reward             | 2.341315 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 45.8     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 461         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 19          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0.000112    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -27.4       |\n",
      "|    reward             | -0.30281836 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 58.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 461         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | -0.00275    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -1.69       |\n",
      "|    reward             | 0.021043114 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 0.0678      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 461        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | -1.2349632 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 2.89       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-04-04 to  2022-07-06\n",
      "A2C Sharpe Ratio:  -0.30359531378092586\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1260_1\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 610        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 3          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.09974829 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 638449.34\n",
      "total_reward: -361550.66\n",
      "total_cost: 1366979.33\n",
      "total_trades: 11736\n",
      "Sharpe: -0.132\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 572          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059524467 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0566      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.89         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00595     |\n",
      "|    reward               | 0.30428433   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.73         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 565          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068066306 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.0103      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.71         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    reward               | -0.8036097   |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 4.54         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 560          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045406558 |\n",
      "|    clip_fraction        | 0.0622       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.0612       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.731        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00663     |\n",
      "|    reward               | -0.2604853   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.97         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 554         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004922295 |\n",
      "|    clip_fraction        | 0.0494      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.0247      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00361    |\n",
      "|    reward               | -0.32631516 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-04 to  2022-07-06\n",
      "PPO Sharpe Ratio:  -0.35676167595348185\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1260_1\n",
      "day: 3083, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2613322.66\n",
      "total_reward: 1613322.66\n",
      "total_cost: 998.81\n",
      "total_trades: 9249\n",
      "Sharpe: 0.530\n",
      "=================================\n",
      "======DDPG Validation from:  2022-04-04 to  2022-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-06\n",
      "======Trading from:  2022-07-06 to  2022-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1323_1\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 456         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -11.5       |\n",
      "|    reward             | -0.11595176 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 3.62        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 448      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.03    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -2.5     |\n",
      "|    reward             | 0.732206 |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 1.54     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 451        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0.327      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -12.1      |\n",
      "|    reward             | -1.7878647 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 3.36       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 450        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -6.81      |\n",
      "|    reward             | 0.29963693 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.968      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 450         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -8.99       |\n",
      "|    reward             | -0.40879294 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 1.99        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 450       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -0.0199   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -2.48     |\n",
      "|    reward             | 4.1881604 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 451        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -5.78      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -1.77      |\n",
      "|    reward             | 0.59035015 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.709      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 449         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 7.02        |\n",
      "|    reward             | -0.32498622 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 1.18        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 451         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -0.274      |\n",
      "|    reward             | -0.03912225 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 0.702       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 452      |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 5000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.03    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 2.16     |\n",
      "|    reward             | 0.908081 |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 0.246    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 452       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 3.44      |\n",
      "|    reward             | 0.7666655 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.677     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 10.4       |\n",
      "|    reward             | 0.79839844 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 4.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0.0192     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -2.99      |\n",
      "|    reward             | -0.6740508 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 0.749      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 453        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -9.3       |\n",
      "|    reward             | -1.6074219 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 2.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 454       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -28       |\n",
      "|    reward             | 1.7111732 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 455        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -1.32      |\n",
      "|    reward             | 0.16559967 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.598      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 456         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -2.03       |\n",
      "|    reward             | 0.110135406 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 0.341       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 455        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -20.2      |\n",
      "|    reward             | 0.31063765 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 11.9       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 455          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 20           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.02        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 2.94         |\n",
      "|    reward             | -0.056167748 |\n",
      "|    std                | 0.985        |\n",
      "|    value_loss         | 0.377        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 455        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 8.54       |\n",
      "|    reward             | -1.0644642 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 1.92       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-07-06 to  2022-10-04\n",
      "A2C Sharpe Ratio:  -0.08227241563900246\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1323_1\n",
      "---------------------------------------\n",
      "| time/              |                |\n",
      "|    fps             | 603            |\n",
      "|    iterations      | 1              |\n",
      "|    time_elapsed    | 3              |\n",
      "|    total_timesteps | 2048           |\n",
      "| train/             |                |\n",
      "|    reward          | -0.00029972353 |\n",
      "---------------------------------------\n",
      "day: 3146, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 466418.33\n",
      "total_reward: -533581.67\n",
      "total_cost: 1399021.59\n",
      "total_trades: 11929\n",
      "Sharpe: -0.264\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 573          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076118954 |\n",
      "|    clip_fraction        | 0.0586       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0244      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.89         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    reward               | 1.7085514    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.91         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 568          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072788345 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.00308     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.65         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00722     |\n",
      "|    reward               | -0.08643828  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.03         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 562          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067761815 |\n",
      "|    clip_fraction        | 0.0557       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.00405      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.46         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00417     |\n",
      "|    reward               | 0.26962852   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 2.08         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 558         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007988488 |\n",
      "|    clip_fraction        | 0.0571      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.00326     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00626    |\n",
      "|    reward               | 0.34502706  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.96        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-06 to  2022-10-04\n",
      "PPO Sharpe Ratio:  -0.11638169788586741\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1323_1\n",
      "day: 3146, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4171086.59\n",
      "total_reward: 3171086.59\n",
      "total_cost: 2985.29\n",
      "total_trades: 9442\n",
      "Sharpe: 0.672\n",
      "=================================\n",
      "======DDPG Validation from:  2022-07-06 to  2022-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-04\n",
      "======Trading from:  2022-10-04 to  2023-01-04\n",
      "Ensemble Strategy took:  33.31378730932872  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.25202</td>\n",
       "      <td>0.083493</td>\n",
       "      <td>0.222535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.145116</td>\n",
       "      <td>-0.328071</td>\n",
       "      <td>-0.06517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.289769</td>\n",
       "      <td>-0.312519</td>\n",
       "      <td>-0.317338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.076598</td>\n",
       "      <td>-0.259067</td>\n",
       "      <td>-0.102126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0  126  2021-10-04  2022-01-03        A2C    0.25202   0.083493    0.222535\n",
       "1  189  2022-01-03  2022-04-04       DDPG  -0.145116  -0.328071    -0.06517\n",
       "2  252  2022-04-04  2022-07-06        A2C  -0.289769  -0.312519   -0.317338\n",
       "3  315  2022-07-06  2022-10-04        A2C  -0.076598  -0.259067   -0.102126"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  -0.7474223875756687\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp],ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.004790e+06</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>0.004790</td>\n",
       "      <td>2022-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.940060e+05</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>-0.010733</td>\n",
       "      <td>2022-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9.893893e+05</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>-0.004645</td>\n",
       "      <td>2022-01-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.892121e+05</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>-0.000179</td>\n",
       "      <td>2022-01-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0   1.000000e+06  2022-01-03           NaN  2022-01-03\n",
       "1   1.004790e+06  2022-01-04      0.004790  2022-01-04\n",
       "2   9.940060e+05  2022-01-05     -0.010733  2022-01-05\n",
       "3   9.893893e+05  2022-01-06     -0.004645  2022-01-06\n",
       "4   9.892121e+05  2022-01-07     -0.000179  2022-01-07"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return         -0.169528\n",
      "Cumulative returns    -0.169528\n",
      "Annual volatility      0.217778\n",
      "Sharpe ratio          -0.747422\n",
      "Calmar ratio          -0.671204\n",
      "Stability              0.595413\n",
      "Max drawdown          -0.252573\n",
      "Omega ratio            0.884711\n",
      "Sortino ratio         -1.022268\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.927363\n",
      "Daily value at risk   -0.028083\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (251, 8)\n",
      "Annual return         -0.094324\n",
      "Cumulative returns    -0.093968\n",
      "Annual volatility      0.198502\n",
      "Sharpe ratio          -0.402058\n",
      "Calmar ratio          -0.429901\n",
      "Stability              0.236972\n",
      "Max drawdown          -0.219408\n",
      "Omega ratio            0.936015\n",
      "Sortino ratio         -0.559755\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             1.014390\n",
      "Daily value at risk   -0.025326\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_dji_ = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:             date           dji\n",
      "0    2022-01-03  1.000000e+06\n",
      "1    2022-01-04  1.005866e+06\n",
      "2    2022-01-05  9.951360e+05\n",
      "3    2022-01-06  9.904718e+05\n",
      "4    2022-01-07  9.903404e+05\n",
      "..          ...           ...\n",
      "247  2022-12-27  9.086102e+05\n",
      "248  2022-12-28  8.986103e+05\n",
      "249  2022-12-29  9.080428e+05\n",
      "250  2022-12-30  9.060324e+05\n",
      "251  2023-01-03           NaN\n",
      "\n",
      "[252 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2022-01-03  1.000000e+06\n",
      "2022-01-04  1.005866e+06\n",
      "2022-01-05  9.951360e+05\n",
      "2022-01-06  9.904718e+05\n",
      "2022-01-07  9.903404e+05\n",
      "...                  ...\n",
      "2022-12-27  9.086102e+05\n",
      "2022-12-28  8.986103e+05\n",
      "2022-12-29  9.080428e+05\n",
      "2022-12-30  9.060324e+05\n",
      "2023-01-03           NaN\n",
      "\n",
      "[252 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:         datadate\n",
      "0    2021-10-04\n",
      "1    2021-10-05\n",
      "2    2021-10-06\n",
      "3    2021-10-07\n",
      "4    2021-10-08\n",
      "..          ...\n",
      "348  2023-02-22\n",
      "349  2023-02-23\n",
      "350  2023-02-24\n",
      "351  2023-02-27\n",
      "352  2023-02-28\n",
      "\n",
      "[353 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2022-01-03  1.000000e+06\n",
      "2022-01-04  1.004790e+06\n",
      "2022-01-05  9.940060e+05\n",
      "2022-01-06  9.893893e+05\n",
      "2022-01-07  9.892121e+05\n",
      "...                  ...\n",
      "2022-12-27  8.339333e+05\n",
      "2022-12-28  8.240245e+05\n",
      "2022-12-29  8.352327e+05\n",
      "2022-12-30  8.321307e+05\n",
      "2023-01-03  8.304722e+05\n",
      "\n",
      "[252 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2022-01-03  1.000000e+06  1.000000e+06\n",
      "2022-01-04  1.004790e+06  1.005866e+06\n",
      "2022-01-05  9.940060e+05  9.951360e+05\n",
      "2022-01-06  9.893893e+05  9.904718e+05\n",
      "2022-01-07  9.892121e+05  9.903404e+05\n",
      "...                  ...           ...\n",
      "2022-12-27  8.339333e+05  9.086102e+05\n",
      "2022-12-28  8.240245e+05  8.986103e+05\n",
      "2022-12-29  8.352327e+05  9.080428e+05\n",
      "2022-12-30  8.321307e+05  9.060324e+05\n",
      "2023-01-03  8.304722e+05           NaN\n",
      "\n",
      "[252 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABMcAAAHPCAYAAABX+L2dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAEAAElEQVR4nOzddXhUZ/bA8e9Y3Ii7JxDcHUoF6u7uspWt/Nrt1nbbbbfdbrctK916S23rpS7UaClQKBI0OCEJAZJAXCYj9/fHO5IQm+gk4XyeJ8/cmXnvve8gk5lzzzmvTtM0DSGEEEIIIYQQQgghjkB6b09ACCGEEEIIIYQQQghvkeCYEEIIIYQQQgghhDhiSXBMCCGEEEIIIYQQQhyxJDgmhBBCCCGEEEIIIY5YEhwTQgghhBBCCCGEEEcsCY4JIYQQQgghhBBCiCOWBMeEEEIIIYQQQgghxBFLgmNCCCGEEEIIIYQQ4oglwTEhhBBCCCGEEEIIccSS4JgQQgghhBBCCCGEOGINuuDYzz//zKmnnkp8fDw6nY6PP/6408fQNI1//OMfZGdn4+vrS1JSEo8++mjPT1YIIYQQQgghhBBCeJXR2xPoabW1tYwZM4Yrr7ySs88+u0vHuPXWW1m0aBH/+Mc/GDVqFJWVlZSVlfXwTIUQQgghhBBCCCGEt+k0TdO8PYneotPpWLhwIWeccYbrscbGRu6//37eeustKioqGDlyJI8//jhz5swBIC8vj9GjR7Nx40aGDh3qnYkLIYQQQgghhBBCiD4x6MoqO3LllVeydOlS3nnnHdavX8+5557LCSecwPbt2wH47LPPSE9P5/PPPyctLY3U1FSuueYaDh065OWZCyGEEEIIIYQQQoiedkQFx3bu3Mnbb7/N+++/z6xZs8jIyODOO+9k5syZvPrqqwDs2rWLPXv28P777/P666+zYMECVq9ezTnnnOPl2QshhBBCCCGEEEKInjboeo61Z82aNWiaRnZ2drPHzWYzERERANjtdsxmM6+//rpr3Msvv8yECRPYunWrlFoKIYQQQgghhBBCDCJHVHDMbrdjMBhYvXo1BoOh2XNBQUEAxMXFYTQamwXQcnJyACgoKJDgmBBCCCGEEEIIIcQgckQFx8aNG4fNZqOkpIRZs2a1OmbGjBlYrVZ27txJRkYGANu2bQMgJSWlz+YqhBBCCCGEEEIIIXrfoFutsqamhh07dgAqGPbUU09x9NFHEx4eTnJyMpdccglLly7lySefZNy4cZSVlfHDDz8watQoTjrpJOx2O5MmTSIoKIj58+djt9u56aabCAkJYdGiRV5+dUIIIYQQQgghhBCiJw264NjixYs5+uijWzx++eWXs2DBAiwWC4888givv/46e/fuJSIigmnTpvHQQw8xatQoAIqLi7nllltYtGgRgYGBnHjiiTz55JOEh4f39csRQgghhBBCCCGEEL1o0AXHhBBCCCGEEEIIIYTwlN7bExBCCCGEEEIIIYQQwlskOCaEEEIIIYQQQgghjliDZrVKu91OcXExwcHB6HQ6b09HCCGEEEIIIYQQQniJpmlUV1cTHx+PXt9+btigCY4VFxeTlJTk7WkIIYQQQgghhBBCiH6isLCQxMTEdscMmuBYcHAwoF50SEiIl2cjhBBCCCGEEEIIIbylqqqKpKQkV7yoPYMmOOYspQwJCZHgmBBCCCGEEEIIIYTwqPWWNOQXQgghhBBCCCGEEEcsCY4JIYQQQgghhBBCiCOWBMeEEEIIIYQQQgghxBFr0PQcE0IIIYQQQgghhOiIzWbDYrF4exqiB5hMJgwGQ7ePI8ExIYQQQgghhBBCHBFqamooKipC0zRvT0X0AJ1OR2JiIkFBQd06jgTHhBBCCCGEEEIIMejZbDaKiooICAggKirKo1UMRf+laRqlpaUUFRWRlZXVrQwyCY4JIYQQQgghhBBi0LNYLGiaRlRUFP7+/t6ejugBUVFR5OfnY7FYuhUck4b8QgghhBBCCCGEOGJIxtjg0VN/lxIcE0IIIYQQQgghhBBHLAmOCSGEEEIIIYQQQogjlgTHhBBCCCGEEEIIIUSPWrBgAWFhYe2OefDBBxk7dmyfzKc9EhwTQgghhBBCCCGEEEcsCY4JIYQQQgghhBBCiCOWBMcGirpD8O6lsOYNb89ECCGEEEIIIYQY8DRNo67R6pUfTdM6Pde///3vpKen4+/vz5gxY/jggw8AWLx4MTqdju+//56JEycSEBDA9OnT2bp1q2v/devWcfTRRxMcHExISAgTJkxg1apVrueXLVvG7Nmz8ff3Jykpid///vfU1ta6nk9NTeWRRx7hsssuIygoiJSUFD755BNKS0s5/fTTCQoKYtSoUc2O6fTxxx+TnZ2Nn58fc+fOpbCwsN3X+uqrr5KTk4Ofnx/Dhg3jv//9b6f+rLrC2OtnED1jyZOQ9yn23UvQj70I9AZvz0gIIYQQQgghhBiw6i02hv/pG6+ce/NfjifAx/OQzP33389HH33Es88+S1ZWFj///DOXXHIJUVFRrjH33XcfTz75JFFRUdxwww1cddVVLF26FICLL76YcePG8eyzz2IwGMjNzcVkMgGwYcMGjj/+eB5++GFefvllSktLufnmm7n55pt59dVXXcd/+umnefTRR3nggQd4+umnufTSS5kxYwZXXXUVTzzxBHfffTeXXXYZmzZtQqfTAVBXV8df//pXXnvtNXx8fLjxxhu54IILXPM63Isvvsif//xn/vOf/zBu3DjWrl3LtddeS2BgIJdffnmn/5w9JcGxgaByL7YVL2AA9A3lsHcNJE3y9qyEEEIIIYQQQgjRy2pra3nqqaf44YcfmDZtGgDp6en88ssvPP/881x33XUA/PWvf+Woo44C4I9//CMnn3wyDQ0N+Pn5UVBQwF133cWwYcMAyMrKch3/iSee4KKLLuK2225zPfevf/2Lo446imeffRY/Pz8ATjrpJK6//noA/vSnP/Hss88yadIkzj33XADuvvtupk2bxoEDB4iNjQXAYrHwn//8hylTpgDw2muvkZOTw8qVK5k8eXKL1/rwww/z5JNPctZZZwGQlpbG5s2bef755yU4dqQz//A4vvZG1/3aTV8RKMExIYQQQgghhBCiy/xNBjb/5XivndtTmzdvpqGhgblz5zZ7vLGxkXHjxrnujx492rUdFxcHQElJCcnJydxxxx1cc801vPHGGxx33HGce+65ZGRkALB69Wp27NjBW2+95dpf0zTsdju7d+8mJyenxfFjYmIAGDVqVIvHSkpKXMExo9HIxIkTXWOGDRtGWFgYeXl5LYJjpaWlFBYWcvXVV3Pttde6HrdarYSGhnr859UVEhzr7w7uxLjuTQDetc7hfONizHlfE3jCn7w8MSGEEEIIIYQQYuDS6XSdKm30FrvdDsAXX3xBQkJCs+d8fX3ZuXMngKtMEnCVNTr3ffDBB7nooov44osv+Oqrr/jzn//MO++8w5lnnondbuf666/n97//fYtzJycnu7ZbO3575zz88Y4ec+734osvujLNnAyG3m0t1emG/D///DOnnnoq8fHx6HQ6Pv744w73+emnn5gwYQJ+fn6kp6fz3HPPtRjz4YcfMnz4cHx9fRk+fDgLFy7s7NQGpZLPHsKAjR9tY/gl5QYAwis3QU2pl2cmhBBCCCGEEEKI3uaMlRQUFJCZmdnsJykpyePjZGdnc/vtt7No0SLOOussVz+x8ePHs2nTphbHzszMxMfHp1tzt1qtzZr0b926lYqKCld5Z1MxMTEkJCSwa9euFvNIS0vr1jw60ungWG1tLWPGjOE///mPR+N3797NSSedxKxZs1i7di333nsvv//97/nwww9dY5YvX87555/PpZdeyrp167j00ks577zzWLFiRWenN6g0FG0gMv9TALYOv5XL5k5hoz0VAPuO77w4MyGEEEIIIYQQQvSF4OBg7rzzTm6//XZee+01du7cydq1a3nmmWd47bXXOty/vr6em2++mcWLF7Nnzx6WLl3Kb7/95iqXvPvuu1m+fDk33XQTubm5bN++nU8//ZRbbrml23M3mUzccsstrFixgjVr1nDllVcyderUVvuNgcpwe+yxx/jnP//Jtm3b2LBhA6+++ipPPfVUt+fSnk7nD5544omceOKJHo9/7rnnSE5OZv78+QDk5OSwatUq/vGPf3D22WcDMH/+fObOncs999wDwD333MNPP/3E/Pnzefvttzs7xUGj4IN7yUbje/10LjnrdPyMel7Vj2Mk+VSs/5LwsRd6e4pCCCGEEEIIIYToZQ8//DDR0dE89thj7Nq1i7CwMMaPH8+9997boozxcAaDgYMHD3LZZZdx4MABIiMjOeuss3jooYcA1Uvsp59+4r777mPWrFlomkZGRgbnn39+t+cdEBDA3XffzUUXXURRUREzZ87klVdeaXP8NddcQ0BAAE888QR/+MMfCAwMZNSoUa7FAnqLTtM0rcs763QsXLiQM844o80xs2fPZty4cfzzn/90PbZw4ULOO+886urqMJlMJCcnc/vtt3P77be7xjz99NPMnz+fPXv2tHpcs9mM2Wx23a+qqiIpKYnKykpCQkK6+pL6jZ1rfyLjk9OwaTpWnvQl06ZMB+Cpl17jjqLfU28Mwf/efND3bt2tEEIIIYQQQggxGDQ0NLB7927S0tJcKzCKga29v9OqqipCQ0M9ihN1uqyys/bv3+9ascApJiYGq9VKWVlZu2P279/f5nEfe+wxQkNDXT+dqbMdCMp//DcAv4Ue7wqMASSNnk2lFoC/tQr2rvHW9IQQQgghhBBCCCEGhV4PjkHLVQicyWpNH29tTGurFzjdc889VFZWun4KCwt7cMbeN+amN/gl6w9knfdIs8dnD4tjiV0tlVq36UtvTE0IIYQQQgghhBBi0Oj14FhsbGyLDLCSkhKMRiMRERHtjjk8m6wpX19fQkJCmv0MJiZff2ZefB8RiVnNHo8J8WNb8FQAzFu+8cbUhBBCCCGEEEIIIQaNXg+OTZs2jW+//bbZY4sWLWLixImYTKZ2x0yfPh3Rks+weQAMqdgENaVeno0QQgghhBBCCCHEwNXp4FhNTQ25ubnk5uYCsHv3bnJzcykoKABUueNll13mGn/DDTewZ88e7rjjDvLy8njllVd4+eWXufPOO11jbr31VhYtWsTjjz/Oli1bePzxx/nuu+96fTWCgWrSyBw22lMBsO/4zruTEUIIIYQQQgghhBjAOh0cW7VqFePGjWPcuHEA3HHHHYwbN44//elPAOzbt88VKANIS0vjyy+/ZPHixYwdO5aHH36Yf/3rX5x99tmuMdOnT+edd97h1VdfZfTo0SxYsIB3332XKVOmdPf1DUrjU4awTKf+/CvXS98xIYQQQgghhBBCiK7Sac7u+ANcZ5boHAyeeGEBdxXfSoMxBL+7NoNvsLenJIQQQgghhBBC9FsNDQ3s3r2btLQ0/Pz8vD0d0QPa+zvtTJyoT1arFD0vefRs9tij8bNWwbd/9vZ0hBBCCCGEEEIIIQYkCY4NULNz4vij9Vp1Z9XLsHuJdyckhBBCCCGEEEIIMQBJcGyAigv1J3n8CbxlPRYA+yc3Q2Otl2clhBBCCCGEEEKIvjBnzhzXQoZNtwFSU1OZP3++V+Y1EBm9PQHRdfedksOZ265kjjmXhIp8+OEROOGxrh3MbofCX2HzpxAQDrPvAp2uR+crhBBCCCGEEEKInvfRRx9hMplc93/77TcCAwO9OKOBRYJjA1iIn4n7z57Cva9dw2s+j6P9+iy64adD8lTPD7J/I+T+DzYthOpi9+OhSTD2wp6ftBBCCCGEEEIIIXpUeHh4s/tRUVFemsnAJGWVA9zRQ6OJHncy71tno0ND++AqWHgDfH4HLLofVrwAlvrWdy74FV44Cn59BqqLqdUF8Js9GwDb1/dAbVkfvhIhhBBCCCGEEKIPaZpqT+SNH03r1FRra2u57LLLCAoKIi4ujieffLLZ81JW2T2SOTYI3H/KcM7edjWzG9cTU7UX1r3dfMDeVXDWC80fs1mwfXYbBruVX+05vGw9kZ/to7Gh51OfBxjesAftm3vQnfVi370QIYQQQgghhBCir1jq4NF475z73mLw8bzs8a677uLHH39k4cKFxMbGcu+997J69WrGjh3be3M8gkhwbBAI9Tdx79nTOO+1PzFbv54AzATozIRQy2XGbzGsfxeGngQjznDv9OuzGErzOKQFcaPlNoalp/Kn0XEMiw3mgRev4z3d/RjWvwejz4fM47z22oQQQgghhBBCiCNZTU0NL7/8Mq+//jpz584F4LXXXiMxMdHLMxs8JDg2SBwzLIbNc2ezYFkKVQ0WGq12AGrw5xbjx9g/vx198lQIjoXKImw/PoYBeMx6Ef+86lhmZbnrkWfPOZ4Fi3/hauNX2D+7Hf1Nv3Yqoi2EEEIIIYQQQvR7pgCVweWtc3to586dNDY2Mm3aNNdj4eHhDB06tDdmdkSS4NggcvMxWdx8TBYAZquNijoLlz7vw9HVuYysz4dPb4GL3kP76m4M1jp+s2djHnFBs8AYwA1z0jkz93KOr/6NxMoCWPwYzHvEC69ICCGEEEIIIYToJTrdgEgE0TrZn0x0njTkH6R8jQZiQvx44oJJ3Gm7EbNmgu2L4KNr0W35HKum51Hdddx3yohW9/3TWZN5wHIlANryZ2D/hr5+CUIIIYQQQgghxBEvMzMTk8nEr7/+6nqsvLycbdu2eXFWg4sExwa5MUlhnHj0MTxhPU89sOF9AF62ncgpc48jJsSv1f2mpkcQOe5UPrdNQafZsX9zf6dX0xBCCCGEEEIIIUT3BAUFcfXVV3PXXXfx/fffs3HjRq644gr0egnp9BT5kzwC3HR0BqviLuRXew4AxVo4X0VczuXTUtrd796TcnjOdBlmzYh+92LY8V0fzFYIIYQQQgghhBBNPfHEE8yePZvTTjuN4447jpkzZzJhwgRvT2vQkJ5jRwCjQc9T54/jyn/9nt9Z3+Vd29E8cNYkjIb2Y6NDAn24+ITZLPj0eK43foHt6/swpB8NBvlnI4QQQgghhBBC9JWgoCDeeOMN3njjDddjd911l2vbbDYTFBTkup+fn9+X0xvwJHPsCJEeFcTvTp3JH63XMWLKsUxICfdov/MmJvFt+CWUa0EYDm6F3Dd7eaZCCCGEEEIIIYTwhNlsZtWqVWzatIkRI1r2FBeekeDYEeSCycmsuPdY/nLaSI/3Meh13HrqJP5lPRMA63ePgLm6t6YohBBCCCGEEEIID3311Vccc8wxnHrqqZxzzjnens6AJcGxI0xMiB96va5T+8zKiqIw40J222Mw1pfC0n/10uyEEEIIIYQQQgjhqTPOOIOqqireeustTCaTt6czYElwTHjk7pNH83fbRQDYlv4Lqoq9PCMhhBBCCCGEEEKI7pPgmPBIVkwwkRPP5jd7NgZbA/ZVC7w9JSGEEEIIIYQQotM0TfP2FEQP6am/SwmOCY/dNjebb/SzACjdstTLsxFCCCGEEEIIITxnMBgAaGxs9PJMRE9x/l06/267ytgTkxFHhoggX5JGzoSNLxN0cANoGug6179MCCGEEEIIIYTwBqPRSEBAAKWlpZhMJvR6yRcayOx2O6WlpQQEBGA0di+8JcEx0SmJQyfRuMFAoK0SKvbAkFRvT0kIIYQQQgghhOiQTqcjLi6O3bt3s2fPHm9PR/QAvV5PcnIyum4m7khwTHTKqJQotmjJjNbtxrxnFb4SHGvOblfZdJJRJ4QQQgghhBD9jo+PD1lZWVJaOUj4+Pj0SAagBMdEp0SH+PGLMZvR9t0c2v4rcWPP8faU+g+bFV46Fiz1cMMSMPp6e0ZCCCGEEEIIIQ6j1+vx8/Pz9jREPyIFtqLTqsJHqY29a7w7kf5m12LYlwtlW2Hfem/PRgghhBBCCCGEEB6Q4JjoNJ/kiQAMqdqsyggFAJa1b7m26/NXenEmQgghhBBCCCGE8JQEx0SnJWSNoU7zxc9eDwe3e3s6/UNDFbotX7juVu341YuTEUIIIYQQQgghhKckOCY6bVRyJBu1VADq83/z7mT6i82fYLSbMWuqjZ/vgbVenpAQQgghhBBCCCE8IcEx0WnhgT7sMmUDUDFQM6RsVvjwGvjsVtC0bh+uYbUqqXzVdiIAYQ2FUHeo28cVQgghhBBCCCFE75LgmOiSuqjRAOj3DdAMqY0fwob3YfUCqCzs3rHK9+C3dzl2Tcdnfqeyyx4LgLZ3dffnKYQQQgghhBBCiF4lwTHRJX7JkwAIr94K1kYvz6aTbFZsix933bUWdK95vrb+XQCW2Ydz0dypbCADkL5jQgghhBBCCCHEQCDBMdElqVkjqdQCMGkWKNns7el0zsYPMZTvdN2t2La868fSNMyr/wfA57qjOH1sAgeCRwLQICtWCiGEEEIIIYQQ/Z4Ex0SXjEgMY709HYDa/FVenk0nNMka22RPAcBe1I35F/2GX9Vu6jRf7MNOIcjXiJYwEYDgsnU90s9MCCGEEEIIIYQQvUeCY6JLQv1NFPgNA6Bq5wovz6YTHFljh7Qg/mC5DoAhlZu7XBpqW6uyxr6yT+KUiWqRgpisiZg1IwG2SijP75FpCyGEEEIIIYQQondIcEx0WUP0GACM+3O9OxFP2axYHVljL1pPYcKUoyjXgjBpjXBgY+ePZzVj3/AhAD/4HMOMzEgARqdGk6eprDRr4W89M3chhBBCCCGEEEL0CgmOiS7zT3U05a/dAY11Xp6NBzZ+iNGRNbY25mxumzuUXLtqnl+/uwvZbzt/xGSpYr82hKTxx2PQ6wBIjQhksz4LgIrt0pRfCCGEEEIIIYTozyQ4JrosMyObEi0MA3bYv8Hb02nfYVlj188bS3igD7v9cgCo2tH5pvzmzV8AsMg2kTMmJLse1+t1VIaPBrrZz0wIIYQQQgghhBC9ToJjostGJISywZ4GQHVXMq/60pbPXVlja2LPYc7QKADMMeMA8N2/pnPHs9uxb/0agK2hMxkWG9Lsad8UlVU3pDIPbJZuTl4IIYQQQgghhBC9RYJjossCfY0UBajMq9p+3pS/8dcXAXjTdhw3zB2DTqdKIEMzpwIQ1lAIdYc8P+C+XPwbSqjR/IgefVyLp1OyRlOhBXa9n5kQQgghhBBCCCH6hATHRLfUxDqCS8U/998MqdJt+BT+gk3TsTbyDFfWGMDwjFR22uMA0Io8b55v3fIlAEvsozh6RGKL58ckD2Gds59Z/sruzF4IIYQQQgghhBC9SIJjoluCs2dSqoXgZ62C3T95ezqtsq96GYAf7OM5ZdYkV9YYQE5cMOtRzfMrt3ved6x+o+o39pvPZEbGh7Z4PjLIl12+QwGo2iFN+UUrKgqhosDbsxBCCCGEEEKII54Ex0S3zMyO4Rub6q9lXr/Qy7NpRWMdtjX/A2Ch8XhOHh3X7Glfo4GSkJEAmPd4mDlWuZfg8s3YNR2GYSeg1+taHdYQrfqZmTrbz0wMfvXl8NxM+OcY+Px2qD3o7RkJIYQQQgghxBFLgmOiW9KjgtgQerS6k/d5/yut3PghJksVe+zRxI8/GT+ToeWYJBXcCzm4Duz2Dg+pbf0KgDVaFtNGDW1zXHDGFADC6/KhobLzcxeDlj3vc2ioAM0Oq16Bf4+HFS+AzertqQkhhBBCCCHEEUeCY6LbEscdx0EtGF9LBeT/4u3pNONsxP8/27FcPC2t1TFxWeNp0Ez426rh0M4Oj1mz4XMAfmIC0zMi2xw3LCOdArvqb6btlewx4Va9+gMAPrNNZYuWrAJlX90FLx8HjXXenZwQQgghhBBCHGEkOCa67cTRiU1KKz/y8mya2LsGn5J1mDUjBclnkhYZ2OqwsSnRbNBU4KxxTwerbppr8C9aCkBV8nGtZ6I5jIgPJVdT/cyqdyzrwgsQg1J9OUHFKoj8ov48Tjb/lfstV1JDABSvhZ3fe3mCQgghhBBCCHFkkeCY6LbM6GByQ+YAoOV93m9Kw2wrVSP+L+1TOH3G6DbHJYX7k2dQ5ZEdNuXf9SNGrZE99mhGjJ7c7lB/HwPFwaMAaNglTfmFom35EoNmZas9kRvPPZn/XDyJH4JO5XOr+vd0aHsHAdrBoKYUDnacpSmEEEIIIYQQfUGCY6JHxI2dyyEtCL/GQ7BnqbenA/XlaBtV6doXPidxbE5Mm0N1Oh21kWPV9t5V7R62bqMqqfzePp6j2zmmkz1RZdQFl631qJ+ZGPyq1qh/l98ylTlDozhxVBzf/98cKoaMAKBhT/v/Bgc8mwVemQf/nQaHdnt7NkIIIYQQQgghwTHRM5qWVjZu6AerVm74AKOtgTx7EsOnzMVkaP+ful+aap4/pHobFP4GlvqWg+x2dNsXAbAncjZRwb4dTiMueyL1mo/qZ3ZwR+dfhxhc6isIKvoZgMr0U1xluf4+BgLTVOZYaMUm0DSvTbHXbVoIh3aBzQx5n3l7NkIIIYQQQgghwTHRM4bGBLM2aDYA9s2fgt3m1fnUbvwSgE/ss7hwSnKH4zMzh7JPC8eAXTVFfzQBnpkK718BH1ytbv93Hv6Nh6jSAogddYxH8xiXGs16LR0Ayx4prTzSaVtVSeU2ewITJk5t9lxc1gTMmpFAWxVU7PHSDHuZpsGyf7vvO1Z+FUIIIYQQQghvkuCY6BE6nY6YMfOo0ALxMx+Egg56d/UmqxlTkWqA35B6NHGh/h3uMjppCPdZruJn2yhqjUNAs0Fpnspy2fiBut3xLQDf2sdz7MhEj6aSEhHg6mdWsU2a8h/pqhyrVH6rm8acodHNnhuVEqVWrgTMBav7fG59In8J7F8PBkfWZeGvUHvQu3MSQgghhBBCHPGM3p6AGDyOH53EoqUTOc/4E5b1H2FKnemdiRT8io+9gQNaGKPGTvNol1B/EyVxc7hs73iwaMRQznD9HtJ0+0EHdk2PFT0N+LA5ZBZfRAd5dFydTkdN1Hgo+QTD3t+686rEQNdQSaCjpLI89aQWK53GhPjxizGTMfZdlG//ldgxZ3tjlr1r2X/U7biLoeg32L8Bti+CsRd6d15CCCGEEEKII5oEx0SPGREfwmuBsznP/BO2TZ9iOuUJ0Bs63rGH1W/5Fn9giX00s7OjPN7vhUsn8vXG/Ww7UM3WA0NYuT+KHxtblofePWUYOp3O4+MGZEyDEgir3QkNleAX6vG+YvDQtnyJUbOwwx7P+AmtB22rw0dB2SLse9f28ez6QOlW2P4NoIOpN8GG91RwbOsXEhwTQgghhBBCeJUEx0SP0el0RI6ex6GVTxNuLoXdP0GGZ725elLjVhUc2xE0iXNC/DzeLz7Mn6tmprnu2+0ah+oam/VGNxl0hAX4dGo+OZmZFCyNIllfCntXt/9nYm2ET29W29N/D7EjO3Uu0X9Vr/mAEGAR07hyWOsrnfokTYAyGFK5Wa1uqh9Ele/Ln1G3Q0+EyEx1+9PjsOMHsDSAyfP/q0IIIYQQQgjRkwbRNy/RH5w4JpkvbKrRuGXtO30/geoDhFZuwa7p8Mk+tluH0ut1RAb5EhXs/ulsYAxgTFIoa7VsAKq2d9B3bPfPsP5d9fPcDHj7Iti7pivTF/1J3SECCn8CVEmlv0/rGZXx2eNo0Ez422vVio6DRU0prHO8H0y/Rd3GjYXgOLDUql5kQgghhBBCCOElEhwTPWpUQijLg45Td/I+g8a6Pj2/tvMHADZqqUwYkd2n525LgI+R4mCVAdawu/0VKxu3q/kf1IWjoVMlZy8eDe9e4vUVQEUXVRSiLTgZo2Zhuz2BsROmtzl0VFIEm7RUAOr3eNCjbu8aqD7QQxPtRb+9BDYzxI+HZEdJqU6nsscAtn7pvbkJIYQQQgghjngSHBM9SqfTkTn+GArsUZhsdX3+pbdm0yIAlmmjmZwa3qfnblfiZACCy3JVuVwb6rd+D8CfzRdznPnvfGM8Gjt6FWjcv6EvZip6UtEqePEYdCWbKdVCuUe7kaNzotscHhHkyy6TCuqW71jZ9nE1DRY/rgKnr5/W7r8pr7OaVXAMYPrNKijmNPQkdbv1K5rVLwshhBBCCCFEH5LgmOhxp49L4GP7DAAa+7K00m7HmP8jAKWxM9ssXfOG+KETqdd88LdVw8HtrQ+qKSW0cgsAh2KmctA/letrrmWFbRgAjfs29tV0RU/Y+CHagpOhtoQ8ezKnmx9m3NRjCPBpv9VjbcQoAHTFbTTlt9vhq7th8aPqfumW/l2WWLwW6sogIBJyTm/+XOosMAVC9T7Yl+uV6QkhhBBCCCGEBMdEj8uICiIv8gQAjLt/hNqyvjnxgQ34W8qp0fyIzpndN+f00LjUaNZr6QBY8tsordytelJtsqdw7fGTWXr3Mdx/cg479CkAlO+S3mMDRu7b8MFV6KwNfGcbxzmNf+bcY6dx70k5He7qmzIBgPCqvJaltDYLLLweVj4PwBZ7knp41as9O/+eVLhC3SZPBcNhgUGTH2Q6FqjY+lXfzksIIYQQQgghHLoUHPvvf/9LWloafn5+TJgwgSVL2s9aeOaZZ8jJycHf35+hQ4fy+uuvN3t+wYIF6HS6Fj8NDQ1dmZ7oByZMmMJ6exp6zQqbFvbJOa3bvgNguX04M4bG9ck5PZUU7k+eQWWAVWxf2uqY6jzH/LVRTE4LJ9DXyDWz0rFEqICKff+mvpms6J7SbWhf3AHAAus8brbfxSPnT+X2udnompYUtiE5awy1mi++WgOUbXM/YamHdy6CDe9hwcDvG2/iDsvvANBt+Vw1ve+PCh3loUlTWn/eVVopfceEEEIIIYQQ3tHp4Ni7777Lbbfdxn333cfatWuZNWsWJ554IgUFBa2Of/bZZ7nnnnt48MEH2bRpEw899BA33XQTn332WbNxISEh7Nu3r9mPn59f116V8LrTxsTziX0mAOY1b/fJOes2q35jq43jGB4X0ifn9JROp6MmejwAhr2rWg7QNHS7FgNwIGoqgb7uDBt9nCqzC67c2uvzFK0wV8OzM+Ct8zoea2lA++AKdJY6frGN4J+mq3n9mmmcOS7R49ONTAxno5YGQG2+oym/psEnN8H2RTRoJq5rvIPipFOIyJxIrj0dvd0C6/7XlVfXuzStSXBscutjsuaBTq966lUU9t3chBBCCCGEEMKh08Gxp556iquvvpprrrmGnJwc5s+fT1JSEs8++2yr49944w2uv/56zj//fNLT07ngggu4+uqrefzxx5uN0+l0xMbGNvsRA1d0iB/7k07Gpunw3b8aDu3q3ROaawgsUUGnxtSj0es7ztDpa0EZapXCsNpd0FDZ/MlDuwhq2IdZMxI+7KhmT0Wmjcam6Qiylg+MlQkHovaawa99Ew5shO3fQPme9o/z7QPoDmyiTAvhHm7mnetnMDmtcwtDhAaYyPdRTfkrnE35l/0bNn6IRTNwpeUPWDPm8vrVk5k7PIa3bceqMasX9L+m9hV7oLYE9CaIG9v6mMBISJyktnf/3GdTE0IIIYQQQginTgXHGhsbWb16NfPmzWv2+Lx581i2bFmr+5jN5hYZYP7+/qxcuRKLxeJ6rKamhpSUFBITEznllFNYu7aNZtRNjltVVdXsR/QvR00YyVL7SAC09e/17snyf8GgWSmwR5EzfEzvnquLcjLT2WOPRo+GVtQ8e8y+czEAa7Uspg5LavZcVmIM+ZoKFmsHpCl/j9v4ITwcCWvfavmc3Ubj0v+67lp3/dL2cbZ8AStfAOD/LL/j5tNmMjQ2uEtTqo9S/4YN+3Jh5w9o3/0ZgIetlxCacywvXT6RAB8jU9Mj+Mw2jRrNXwWg+1twyZk1FjdG9RdrS7SjF1tF6xnIQgghhBBCCNGbOhUcKysrw2azERMT0+zxmJgY9u/f3+o+xx9/PC+99BKrV69G0zRWrVrFK6+8gsVioaxMNWofNmwYCxYs4NNPP+Xtt9/Gz8+PGTNmsH17G6v6AY899hihoaGun6SkpDbHCu84YWQsnzELgMY17/RqVot567cA/Gwfzazs6F47T3eMTgxjraYygupXNS+Bq9qs5v+bfjSjE8OaPZceFchWkgGozM/t9XkeUTQNbfHjYLeiLboP6subP7/1S3yq3QGbirwfWj9OZRH2j28E4AXryYSMOpHzJnb9PSkgVTXlj6jZgvb+Veg0O+9bZ7M+7jzmXzAWX6NaiTUrOgj/wBA+tqmsRFYv6NyJDu2Cty+CrV93ea7t6qjfmFOIo+y0qqh35iGEEEIIIYQQ7ehSQ/7Dm0prmtZmo+kHHniAE088kalTp2IymTj99NO54oorADAY1Be8qVOncskllzBmzBhmzZrFe++9R3Z2Nv/+97/bnMM999xDZWWl66ewUHrV9Dchfias2SdTr/ngW7UbSvJ67Vz1u5YDsDt4ArGh/bNXnb+PgV+GnIld0xGw5QPYrhrwY7fhV6Sa9NcmzMRwWEmoyaCnNCALgLrCdX0650Fvz1J0ZaqXm66+HJY82ezpxqXPAJBrzwDAp7D1DFnt2z+hb6hgnT2dt4Mv569njvSo+X5bUjJHUqUFYNIs6BrKybWnM9/vdzx36UT8TAbXOJ1Ox9T0CHdpZd5nnq8O21AJ/zsftn4BPz7S5bm2y7lSZdKk9seFJqjbSgmOCSGEEEIIIfpep4JjkZGRGAyGFlliJSUlLbLJnPz9/XnllVeoq6sjPz+fgoICUlNTCQ4OJjIysvVJ6fVMmjSp3cwxX19fQkJCmv2I/ueE8Zn8Zh8KgD2/nZK07rA2ElSpVvULzeggQ8XLwrKn86rtBAC0z34PDVWwbx1+1iqqNH8SR8xodT9r1HAATGW9F2A8EtlWvgxAnl1l5tl/fR7K89WTxbn4FC3Hohn4P+vvsGp6QszFLfuONdZhy1MrLT5ou4qnL5pCiJ+pW/MakTiEDY6m/KVaCLfY7uBfl05tNfA7NT2cTVoqO03ZYLdAbivloYez2+DDa92rYe7fADUl3ZpzC+Ya1asNINHdjF/TNK5/YxXHPrmY6gZHaX2IMzi2t2fnIIQQQgghhBAe6FRwzMfHhwkTJvDtt982e/zbb79l+vTp7e5rMplITEzEYDDwzjvvcMopp6DXt356TdPIzc0lLi6uM9MT/dCcoVHk6kcAULXlx945SclmjJqFCi2QocNG9M45esj1R2Xwouki9tij0VXthe8epHG7KtX71T6cmdmtL0QRkKh6UA2p3QU2S6tjRCfVlKDL+xSAOy03sMQ2Er29Ee37vwBgW6ayxj63T+WkObNZr6UDrfQd2/kDRls9RVokxx17AmOTwro9tSBfI98Enk6uPZ3rG+/gxtOPYkJK6439p6ZHAPBKwxz1wOoFHf8b+eERtcCA0c8dmNr1U7fn3UzxGtDsqmTSmRkGLNp8gG82HWBnaS0rdh1SD4Y6yyr39r9FBYQQQgghhBCDXqfLKu+44w5eeuklXnnlFfLy8rj99tspKCjghhtuAFS542WXXeYav23bNt588022b9/OypUrueCCC9i4cSOPPvqoa8xDDz3EN998w65du8jNzeXqq68mNzfXdUwxcPkaDdTFT1XbRct65YuvpWgNABvtqYzugcBEb4oK9uWPp03gj9Zr1QOrXsbuyF7a6DeelIiAVvdLTMumSvPHiBXK2s6oFJ2w9g30mpW19kxGT5rNP+yXYNd06DZ+CHmfo9v0EQCf+J3BLcdkkWsYBbTsO1a99kMAFtknc8m01J6b37CTOaPxEYZNPo4LJye3OSwzOoiIQB8WWqZi9QlRfcS+e7Dt4274AH55Sm2f9h8Yebba3tlGP7WuaqWk0mqz88Q3W1331xVVqA1ngM5S17LvmxBCCCGEEEL0sk4Hx84//3zmz5/PX/7yF8aOHcvPP//Ml19+SUpKCgD79u2joMDdwNpms/Hkk08yZswY5s6dS0NDA8uWLSM1NdU1pqKiguuuu46cnBzmzZvH3r17+fnnn5k8efLhpxcDUNTQ6dRrPvhbKqB0S48fv2LnbwDsNGYSG9I/+401dfrYeAKHHs3/rEcD4FdXDICWelSbfaqGxYWyRVMBksbiDX0z0cHMbqNxxSsAvG0/jv+bl83UmXP40KYWkNDevwK9ZmWFfRhTZx6Lj1FPbZwK8jbrO2ZtxGfnIgCK4+YS6t+9csqm/nDCMP53zRQeOX1ku+Ocfcfq8OObjPvVg8v/A5s+bjm4cCV8crPannErjD4XMo5R93f92LPB60L1/7JpM/6P1uxlR0mN6/66okq1YfKDAEeZvfQdE0IIIYQQDZXw+umw4gVvz0QcIbrUkP/GG28kPz8fs9nM6tWrmT17tuu5BQsWsHjxYtf9nJwc1q5dS11dHZWVlXz88ccMHTq02fGefvpp9uzZg9lspqSkhG+++YZp06Z17RWJfmdyZiyr7GqVRvvunu87ptunmtTXRIzqVhP0vqLT6XjkjFH823g5+zRVKrdPC2foyPFt7hMV7Eu+IRWAil1r+mKag9uO7/GpKaJCC8SacyaRQb7cdHQmL/tcTL3mg86uyhLf0k7iwkkqKBmRM7tl37H8n/G11VCqhZIx/ugenWKQr5HpmZHo9R3/m56arv4dvVU1Bqbfoh785GZ3lqGmwapXYcHJYK2HrHlw7J/Vc8nTVHll9T7PgtcrnodnZ0LR6rbHaBoUOVaqdPQba7DYePo71ePstDHxAKwvqkBzBuScpZdV0ndMCCGEEOKIt+0b2LUYvv4jlG7tcLgQ3dWl4JgQnTE8PoS1epX9Ut3TfcdsFsKq1Rdun6S2g0v9TWyoH7efMpG7LdfSoJn4wDabGZlR7e5TE6qCyrb9kjnWXdYVLwLwgW02501TK4GG+Jm4eO5UXrSdBECBPYqw8WcQGqCywSZlJ7XoO1ab+zEAi+wTOW5EfF++hGacfcdW7ynHPOcBSJkBjdXw7iVq9cqPb4TPbwNbIww7Bc55BfQGas1WXl6xj7o4R3bXzg7+f9qs8NPf4cAGeONM2NtGoPbgDlUeafSDWFWO+sbyPeyrbCAu1I+/njkSH4OeijoLBYfq1D6hSepWMseEEEIIIYTrIq8Nvr5H+tKKXifBMdHrDHodtY4v3z493XesJA+jZqFSCyApfXjPHbcPnDshETKPY5T5Zb6JuYYhgT7tjtfHqQBjYIVcOemWigIMO9WiIktCT2NKmrvR/YWTk/lqyCX83XI+N1pu5YqZGa7nMqODWNe075jdhn7bFwDsijyWqGDfPnwRzWVGBxEZ5IPZamfd3lo451UIilGZYPNHw7r/gU4Pxz0E578JvsE0WGxc+/oqHv58M+9XqABhh33HCpZBXZnaNlfCG2dAcW7Lcc5+Y/HjwOhDVYOFZxbvAOD247IJ9jORE69WGM4trFBjQyRzTAghhBBCOBxs0md55/ewfZH35iKOCBIcE30iatg0R9+x8h5Ni3U3409jVD9vxn84nU7HP84ZzanjU7j3xJwOxw9JHQtAiKUMag/28uwGL231a+jQ+MU2gjnTpzUrxTUa9Nx72lie184gddQM0qOCXM/pdLrmfccKV+DfeIhKLYCEcXP7/HU0pdPpmOLIHlu+8yAEx8C5C0BnAEstBEbBZZ/AzNtAp8Nis3PL22tZtlP9O3q7LFMdaM9SsJrbPtFmtbonI85UvcScvSD2rW8+rtBRUpmkSiqf/2knFXUWMqODOGu8CoKNTQwFYL2z75izrFIyx4QQQgghRJm6sOqsQuCbe8Ha6L35iEFPgmOiT0zOjGO1XWWn2Hcv6bHjVrqa8WcQH9r/m/EfLjrEj6fOG8v0zMgOx2YmxbLHHg2AdmBjb09t0DKv+wCAD3XHctb4xBbPz8qKYvk9x/DUeWNbPNe075h5yb8A+M4+gXmjWh6nrzlLK3/d5QicpkxXAbIJV8D1P0Oa6g1pt2v84YP1fLv5AD5GPWmRgWzRkqgzRajVIp1ZX4ez2yHvM7U99mK4+ANInAQNFfD6abDmDRUsA3dwLHEylfUWXvklH4C7jh+K0aB+7YxODANg3eGZY5WSOSaEEEIIcUSz21WbDoBT/6Uu9B7cASulOb/oPRIcE31ieFwIa5x9x7Yu7rkD78tVxwwfOSCa8XdHZnQQW1HN4av2rPPybAaoigL8qvKxanqCR5zQ5uqS0cF++Bhbvj1ObNJ3zHfHVwDkhR1F4pCA3puzh6Y5mvKvLignv6xWPTj8NDj1nxCi+qFpmsZDn21i4dq9GPQ6/nvReG44Kh3QsRzHVbm2+o4VrYSa/eAbCmlHgV8IXPIhJExQ/cU+vRmeyIJ3LnY39k+azJLtpdRbbGREBTJveIzrcGMcmZ4biyux2uzunmNVkjkmhBBCCHFEq9qrFpHSG1Xm2LF/Uo//9HfVT7f6AKxeAG+dB29fCJZ6r05XDA4SHBN9wmjQUxPbpCStJ/qO2SyEVjmb8Y/r/vH6OV+jgQP+qvyttkCCY11hdwR+1mkZnDK541LWwzXtOwZQq/kSOebEHptfd2REBTE6MZRGq52LX1rBvsrmHxIsNjt/+mQTry3fg04HT503huOGx3DCiDh8DHo+r3X8ebTVd2zzJ+p26AlgdPTH8wtV5ZrH3A+RQ8Fmhi2fAxoMSYWgaBZvLQXgmGHRzQLY6ZGBBPsaabDY2XagpslqlcVgt/XQn4oQQgghhBhwnP3GhqSBwaSqFmJHq563z82EJ4fCZ7fC9m9g65ew4zvvzlcMChIcE33G3XfsEJRtcz9hroFNC6HuUOcOWLoFk9ZIleZPUsaInp1sP9UYqRYdMJZt9vJMBqaqTaoR/0rdaMYlh3V6/6Z9xwB+tI/luNGpPTS77tHpdLx0+UTSIgPZW1HPxS+toKxG9Q87WGPm0pdX8MavewB4+PSRnD5WBaNCA0zMGRrFL3aV2cm+dS172mmaq6RyXfBR3P5uLodqHT0ffINh9l1w0wq44ReYcav68DLzdux2jZ+2qeDYnKHRzQ6p1+sY5eo7VgFBsWrRALsVakp6+o9HCCGEEEIMFM5+Y5GORaP0BjjxcbVdvQ/QVPVC3Fj1WMGvfT1DMQhJcEz0mUmZcaw5vO9YVTG8cjy8fwX8ewKsetXjrBFL0VoANtnTGJk4pDem3O/4JYwGIKxmB9isXp7NAGO341uo/t1Vxc/EZOja25+z7xjAuqBZZEYHdbBH34kO9uPNa6YQH+rHrtJaLn15Jct3HuS0/yzl112HCPI18uJlE7lkakqz/U4fm0ApQ9ihSwE02L24+YGL10BlIZopkN+vHMLCtXt5ctFhC2vodCrtfe5f4IYlMOEK8vZXUVptJsDHwMTUlv9HXX3HiirAYITgOPWErFgphBBCCDG4LP8vfPVH1U+sI87MsYhM92Mp09Wq66fMhzu2wLU/wNQb1XMSHBM9QIJjos+MjA9hjU5leFVv+RFK8uCluXBgI3Z0UH8IPr8NXjoWilZ1eDxnM/4dhnQSwvx7c+r9Rnx6DnWaLz5aIxza5e3pDCwHNuBvqaBG8yNuxKwuH2bS0CSesZ3Ot7bxBI0+rQcn2DMSwvx585opRAb5kLevigtf/JW9FfWkRgSw8MbpzG3S98vp2JxoAn0M/GhxZGAe3nfMUVJZFncUe6pVSfS7vxW6e5u1wVlSOT0jAl+jocXzY5NU5ti6QueKlY6FDWTFSiGEEEKIwWPTQvjmHljxLBR6EMg6eFjmmFPOqTDxSghxXFBNnqJu962Dxrqem684IklwTPQZo0FPtaMkza9gscoYqypilxbP0eYnechyKXW6QCheqwJkP/y13eNpxbnAkdGM3yknLowtmmpcbtnRRuN00SrLNtVL61d7DjOGxnX5OBlRQbwffBnXWe9k3ti0nppej0qPCuKNq6cQ4mcEYHZ2FJ/cNJOsmOBWx/uZDBw/IpYldkc/tU0LYcuXalvTYPOnAHxhmwyAXgdWu8ZT325r7XAuPzmCY0cdVlLp5Mwc23qgmvpGW5MVKyU4JoQQQggxKJTnw6e3uu87VzVvj7OsMiKr/XFhKarywG5RlQ5CdIMEx0Sfihg6nQbNhK+1BhoqWWUfypnmBwmKy+ZNTmJ2/T9YqB2lBv/8d/jtpdYPZLMSWqXKunySxvfR7L0vJsSXbw0q60lb+m+wWbw8o4GjevMiADb6jCM9MrDLx9HpdLx+1WTevW4aOXEhPTW9HpcTF8Jnt8zkmYvG8+oVkwgNaH1lTqfTxsazzD6CVYyAxhp450L4/mG1Imz5bjSjH/8uUMHAv5yu+pN9uq6YTcWVrR6vqsHC6oJyAOZkR7U6Ji7Uj6hgX2x2jc37Kps05ZeySiGEEEKIAc9mgQ+uVo30DY4FnToKjlnqobJQbR+eOXY4nQ6SHNljUlopukmCY6JPTc6MZZldlW59ZZvMxY33MGt0Fh/fNIMvfj+LlJRUbjdfzxOW8wDQvrwLti1qeaCyrfhoZqo1fxIyRvblS/AqnU7H1rgzKdNC8KkphA0feHtKA4OlnpASVaprTZvT7UzD9KggJqeF98DEeldKRCAnj47DoO/49c7IjCQ0MIALGu6mKPty9eCSf8AbZwJQHDmTgxYT6ZGBXDwlmVPHxAPwxDdbWz3e0u1l2Owa6VGBJIUHtDpGp9MxxtGUP7ewEkKkrFIIIYQQYtD48a+wdxX4hsIZz6rHilaqyoS2HNwJaOAXBgERHZ8jeZq6leCY6CYJjok+NTIhlAd1v+OKxru4yfJ7zpiYwT8vGIfJoCc7Jpj3r5/Gw2eM5EXdmbxrnYNOs6O9f4WqI2/CUqTSZjdrKUdMM36ns6dm85L1JABsS570rKnlka5gOUatkf3aELJHTvT2bPolk0HPyaPjsGLkScNVcPbLYAqAepX99bF5AgBnjU9Ap9Pxf3OzMep1LN5ayopdB1scz9lvbE526yWVTmMcpZXriyqk55gQQgghxGCx43v45Wm1fdq/VL8wgw/UlkL57rb3czbjj8xSmWEdcfYdK1wp34tEt0hwTPQpk0HPyKFZLLaP44oZGfzt7FHNslr0eh2XTk1h/vnjuM96Fb/YRqCz1MJb5zX7wly5U2UBbdNnkDjkyGjG73TCyFiWDjmdSi0Aw8HtkPept6fU79Vu+R6AX+yjmJEZ6eXZ9F+nj1VljZ+uK+bXwKPhmu8hMhtrcALP7Vdp7WeMU2NSIwM5b5Lqf/f3b7aiNbkCqGkaP21zBMeGtl5S6TQ6KQyAdYUVUlYphBBCCDEY7FsPH12ntideBSPOAKMvxI1Vj7VXWulpvzGnmFFgClSlm6V5XZ2xEBIcE33v8bNH8/ktM3nglJw2y9tOGhXHH04ayY2W29hmT4Ca/fD0CHgsCZ4eRejW9wCoGTLiiGnG72TQ67jymNEssJ0AgP2nJ9pPTRZYtqng2O6QSUQE+Xp5Nv3X+OQwTh8bj82ucdNba9jrmwY3ruCFcR9RrQUwLT2CxCHuEslbj83C16hn9Z5yPlzjDmhtPVDN/qoG/Ez6DstPRyeossr8g3VUmhxZZjUlYG3s+RcohBCDUdkO2P6dt2chhBDK1q/hlROgrgxiR8Hxj7qfS1KLO7UbHHNljmV6dj6DERIdlSFSWim6QYJjos8F+5kYmRDaYVDr2lnpnD51OFc2/oHdmmN1QXMVVBZgstVh0QyQMq0PZtz/nDYmnkXBp1Oj+aEv2QjbW+nLJpTaMsIq1VUk3+xjvDyZ/k2n0/G3s0YzPC6Eg7WNXP/GKhpsGh+s3Q+oksqmYkL8uGaWatJ/5/vreObHHWia5iqpnJYegZ/J0O45hwT6kBKhAm7rDpnA4AtoUF3cw69OCCEGIasZXj8N3job1r/v7dkIIY50vz6nFnWy1ELaUXD552BqUuXjSXCszBEc8zRzDNx9xwpXdG6+QjQhwTHRb+l0Ov586nCGDhvBseYnmNDwLMeY/8EZ5r9weePdnNL4V5LTc7w9Ta8wGvRcPGc8b9qOA8D+098le6wN2q7FAOTZkxg/fJh3JzMA+PsYeP7SCQwJMLFxbxWXvLSCXaW1+Jn0nDgqrsX424/L5orpqYBqzn/rO7l8u/kAAHOGtt9vzGlEvFr1c3tprbu0UvqOCSFEx9a/6y5F/+oPUFPq3fkIIY5Mdjt8+Qf4+m7Q7DD+MrjkQ/APaz4u0REcK9kEDVUtj6NpcNBZVulh5hi4+44VLO/01Dtt33p4+0LI+6z3zyX6lATHRL9mNOj594XjOGl0AsaQaAr1CeRqmfxkH0OhKW1ArBjYW86ekMCn/mdi1kzo966C3T97e0r9UtWmbwFYzmgmph5Zizd0VVJ4AM9cNB6DXseqPaoh/wkjYgnyNbYYazToefC0ETx65iiMeh2fritmtWOfjvqNOTlLNYvK6yDEGRzrJ33HJOgshOiv7DZY+k+1bfSD+kPw1V3enZMQ4siU9ymsfF5tH/cQnPovMJhajguJg7BkFUDbu7rl8zUlqlIIHYSne37+xEmg00NFAVT1YvVB/lJYcDJs/RI+uAqKWnkNYsCS4Jjo9wJ9jfznovGsuPc4tj1yIhsenMfPdx3N8j8eS1Twkds/ytdo4JyjJvCe7SgA7Ove7frB1r0Dz85osSroYKDP/wmAsujpHZb4CbfpmZHce5I7M/PsCYntjr9oSjJvXD2FsAD1QSg1IoCUiECPzuVcVGNveb17xcqqJpljdhuseB72runEK+gB3/4ZnsqBqn19e14hhPDEls9VhoVfGFzyEegMsGkh5H3u7ZkJIY40W79St1NvhJm3tb/KZJIjy6vot5bPOfuNhSWDyc/z8/sGQ8xItd1bfce2fgVvnqWCdz5BYGuE9y6VjN1BRIJjYkDR6XQE+5lIjgggNKCVqxFHmAsnJ7PaRzWgrN2xtMvHsa9aAAc2wofXgqWhh2bXD1QVE9ywD5umI3L4Ud6ezYBz1YxU7pibzZUzUpme0fEqn9MyIvj0ppmcPjae+08e7vF5nMGxovL61jPHVi9Q5UKf396Z6XffpoVQva9vUvSFEKIzNA1+eVptT74OUmfAjFvV/S/ugPpy781NCHFksdthp1r8iqEndjzeWVrZWn8wZ7+xyE70G3Pqzb5juW/DOxeDtQGyT4Tfr1Vln1V74YMrwWbt+XOKPifBMSEGMH8fA9kTVd+x4Np8qC3r0nGq9ztq+8u2wo+P9NDsvM9WoK5IbdWSmTIs2cuzGXh0Oh2/PzaLP586AoPes1VhkyMC+OcF4zhueIzH52lWVunMHHP2HLNZ3WVDZdv6rszRbnP38amWzDEhRD+z+ycoXgtGf5hyvXrsqLshMhtqDsDX96hs8PXvwXcPwUfXqfFCCNHT9q+H2lIwBULS1I7Hu5ry/6YCa025+o11JTjWS33HNn8CH98Amg3GXAjnvwFB0XD+WyqDLH8JfP9gz55TeIUEx4QY4CYMS2ebXWXbaF35ZWCpJ9TiTgfWlv1n0CyDXL5tGQAbdVkMiw3x8mxEWxLCVOZYVYOVOv9Y9aAzMLX5Y6jYo7YtdepLX1+o3g92x1XA3uxdIYQQXeHMGht/GQQ6MntNfnDafwAdrHsbnp8NH10LvzylGvd/+QevTVcIMYjt+E7dph8FRp+Ox8eMBFMAmCvVhfmmXJljnWjG7+QMzO3fAObqzu/fljVvqNtxl8Lp/3X3UoseBmf8V20v+zds/Kjnzim8QoJjQgxwY5LCWKOpVRirt3W+tFIrV4GHKs2fD2yz0aGhffw7aKzt0Xl6g61QZY6VDxntceaT6HuBvkaGOMqk9xOhHqwsal425HRoV99MqrLQvS2ZY0KI/mTvGti1WPUYm35z8+eSp8BMRwm6/xBIng4TrlBji1ZC6dbDjyaEEN2zw1FSmXmsZ+MNRkiYoLYLVzZ/ztlzrCuZY6EJEOpo9l+0qvP7t0bT3AsHTLwS9IeFT4afDjNuU9vf/rlnzim8RoJjQgxwfiYDJWFjAbDkL+v0/rWOkspCLZon9VeyTwtHd2iXKsMYyGxWhlRsAsCYPMnLkxEdcZZW7rE4VqBtqIBNH6leeD5BED9OPX5ot2cHPLgTFv8N6iu6NqGKJsExacgvhOgnas1Wlr3+AAD58SfSGNTKYinH/gnu2Qt/2A1XfQWn/hOyj1fPrX2jD2crhBj06ivcPb4yj/N8P1dpZZPgmLURHBftu9RzDCBlurrd+UPX9j9ceb5aCdjg4274fzjnBYnKAjDX9Mx5hVdIcEyIQcCUpn4RhFZsAkt9p/atLFZXaPYb4rj91EncbblWPbHyecj/pe0dV74Ivz7XsldAf1GyGR+tgSrNn6Tssd6ejeiAs7RyT60BfB0lsIv+pG4nXtkkOOZh5tgPD8Pix1RpUVdUFri3q6WsUgjRPyz88E2mNqgs8et3zeSoJ37k1aW7qW+0uQfpdOAb1Hy1uHGXqNt174DN0oczFkIMart/Ur24IrJgSKrn+zlXrGzaPL98tzqWTxAEx3VtPs4FAbZ83jN9ap1ZY7GjwOjb+hj/MAhwVD4c2tn9cwqvkeCYEINARvYISrQwjJpVlVt0grlUvYlX+ydy7oRETNlzedc6BwD7qldb36l6P3x5J3x9N3xwRacDcn2hYY+6ErXens7Y5HAvz0Z0pNUVK6uK1JW6qTfBkDT1WLmHmWPFueq2q2WYzgUBQGWO9dVCAEII0Ya8bTs4fuuf0Os0fos4jUNBWeyrbOChzzZz/PyfqaxrJ+iVNQ8Co1TT7O2L+m7SQojBzdlvrDNZYwCJjqqOg9uhJE9l/OcvUY9FZDQP7ndG5nFg9FOf/0o2d+0YTTmDYwkT2x8X4eiRdlCCYwOZBMeEGAQmpIbzmz0bgPqdnes7pivPB8ASkoxOp+Oxs0bxk1FlolXubj3QZtub676z+RN47VSoKW11rLdUbVclpjt8cogO8fPybERHmgXHQhPcT4y5EELiIDxd3fck2GWucQfRmga5OqNpWaXNDPXlXTuOEEL0AKvVSsP7VxOlq6TYJ41JN7zAkj8czSNnjCQi0IeCQ3V8v6WdBUsMJhhzgdpe+2bfTFoIMbhpWpN+Y50MjgWEu/uK/Xcq/Hs8fPF/6n5X+o05+QZBhqP3Wd5nXT+Okys4NqH9cRIcGxQkOCbEIBAR5Et+wCgAanZ0LjjmX6uCAIYIFXyIDvFj5qyjAQitzW81K+zgTvWLYoM9lUoCoeg3eOlYKN3W1ZfQ44z7VGCvPnqsdyciPJLg6Dm2t6IeQh09dHR6mHGr2g53ZI550nOs6ZXCpkGuzqg8bD9ZsVII4UW5b/+ZcZZc6vHF96LXweSPn8nAJVNTOHuCes9csetQ+wcZd6m63faNygAXQojuKN2iVhc3+kHqjM7vP+Fy0BtBb1KllP7hqlLAWQbeVTmnqNvuBsdsFti3Tm0ndpA55rqIK8GxgUyCY0IMEvZEtXxxUMlqz/uAaRrhZvWlPyguw/VwTmYmpVoIeuytpiQ3FuUC8KltOmeaH6JAi4aKPfDyXM8bpvem+grC69Q8AjOmenkywhPuzLE6iB6uHhxxpkqtB3cfi4YKqOvgC+D+De7tw4NcntA0d1DNJ0jdyoqVQggvKd30I+N2PANA7qj7iUgd3ez5KWmqdcCK3QfbP1DUUEicrHr6rHunV+YqhDiCOEsqU2eCyb/z+0+/BR4ogz+Vwb174e7dcGsuZBzdvXlln6BW6D2w0bNMru3fwrZWys0PbAJrA/iFuoNfbXFlju3o/HxFvyHBMSEGifhhk6jTfPG3VUOZh0u1V+/Hh0asmp6IBHdwbFhcKHlaCgBV+S1LK/0PqYBZTPYkho4Yzxnmv7DengYNFWhf3un9/kzFas4F9iiGZWR0MFj0BwmO4Fh5nYXakRfDuQvgtP+4B/gEQlCs2u6o79iBTe7thgowV3duMvXlYKl1TGy8upXMMSGEF2j1FRg+uhaDTuMnv2OZcsbNLcZMTA1Hp4P8g3UcqGpo/4DOjIy1b3r/d7UQondU7YOyHg7S2CxQU9L8se3fqtvOllQ21dXeYu0JCIe0WWp7y+ftjy1aDW+dC2+f714p06lpSWVH83RezJWyygFNgmNCDBIT0qJZa1dXLSy7l3m0T2OZ6t9UrEWQHBnmetzfx0Cxn6r3r85f23wnczURZtXHKSprIs9cNJ5zZ4/lVsvNmDUjuh3fwaaF3Xw13VO981cA1mmZjEoI9epchGdC/EyE+BkB2Fujqawxn4Dmg1wp6x0FxzY2v9/ZvmPObLPAaPdCAJI5JoTwgj3L3iPcVkqBFk3CJf9Fb2j50T3U38TwOLXK76+7OsgeG3EmmAJUE+zClb0xZSFEX7PboGAFfP8XeG4mPDUMnpkEu5f03Dk+uBL+kQUvz1OZp7UHoWC5eq47wbHeknOqum2vtNJugy9uBzTQ7LD+vebPe9qMH9yfUesPdVzhIPotCY4JMUikRASw2ZgDQOU2z34ZVuxVGWZ7dTFEBvk0e64hYgQA+sMCDdp+db9YCyczNQW9Xsc9J+Vw4lEzecZ6BgD2r+6G+oquvpRuq9+lloUuDhqJv4/Ba/MQnZPo6DtWVF7X+gBP+o7Z7e7MMVOguu1s3zHn+NBECIlX25I5JoTwAtuOHwFYN2QumYmxbY6bkhYBwMrdHXwp8wuB4Weo7bWv98QUhRDepGnw0nHwyjxY8qS7tYRmh2//1DMZotu/cweZClfAwuvh6eFga4SwZHdJYX8y7BRAp/oit/UZ7reX3T3FANa93fzPy9Nm/KAqHIIdnxkle2zAkuCYEIOETqejPk4ti2za69nV4NoD6s273DcB3WHpwn5JYwAIr97arIdZhWMFyy1aKlnRwa7Hbz0ui2/CLmCnPQ59bYm6euUNmkZQWS4AtngPfpmJfqPZipWtcQXH2lmxsiIfGmvA4AMpatVVKgs6NxFn5lhYEgTHqW3JHBNC9DVNI7pUXeypjmu/2fWUdGffMQ8yFpyrVm7/rlvTE0L0A3WHXO1EGHk2nPk8/G65ukBYvKYHmtJbYdH9anv85XDMAxCarHpxAWTO7Z3SyO4KjoWkyWp7yxctn68+AD88rLaPe0hl1B7aqYJpAA1VUOpoU+NJcAzcpZXSlH/AkuCYEINIWPYMbJqO0Ia9qt9AB+wHVQZOfVBSi+fi00dRr/ngqzU06/FUu0eVWR4IyMLH6H4L8TUa+PNZ47jPejUA2qpXoPC3br2eLinPJ8BaQaNmIDprUt+fX3SZs+/Y3raCY84Sx/Z6jjmzxqKGuYNpnS2rdGWOJTXJHJPgmBCij5VuIdh6kHrNB/+0ae0OnZyqgmM7SmooqzG3f1xnL8Wa/VL+I8RAV3NA3fqHwzmvqOB3zHCYdqN6/IeHVYCrq9a+DqV54D8E5j4Es+9UTfMveh9m3QlH39vtl9BrXKWVn7Z8btF9YK6C+HFqYYCc09Tj695Wt8VrAU1lxgVFeXY+V98xaco/UElwTIhBZGxmElu0ZADsBb92ON6nytF4Miy1xXM5CUPYqqmgWUOhu++YqUSVVTZGjWixz/SMSOLHzuV962x0aGif3aoaePYhmyMgt1lLZXRaTJ+eW3SPu6yyrcwxZ8+xdjLHHGW/xI5SwS3ofFmlM9MsLLlJ5piUVQoh+tiuxQCstA8jIz683aFDAn0YFquyuTssrfQNVu9v0OqK1EKIAcQZHAs67DPv9FtUQKtsmzvg01kNVfDDX9X2nHvU8QD0BsieB8c+AIGRXTt2Xxh2irrNX6p6pDnt+gk2vA/o4JSn1etxZtRu/BCs5s6VVDq5VqyUzLGBSoJjQgwiOXEh5DIMgMqtHfcdC2nYC4BfTMteAVHBvuwyqmBE+S7HLwibhfA6dTUkIGlcq8e876QcnjFdziEtCF3JJtjwQadfR3dUblfNQTfpssiICurTc4vucZdVdtBzrOYANNa2PsbZIy9mhCqLhJ7JHKs7qD4sCSFEH2nc/gMAS+0jPPp9NiXNUVrZUVN+gGjHBa4DEhwTYkBzriAZFN38cb9QmPV/anvx38DSZCXb+gpY/z4UrWq/J9kvT0FdmQr6TLyqR6fdJ8LTIGYUaDbVJ+3TW+Djm+CTm9Tzk65RmWMAabNVz7CGStj2dTeDY5I5NlBJcEyIQcRk0HMoQpVL2Pd0sGKluYZQWzkAofFZrQ6pCVMN/u3F69UDZdsxaRaqNX+SM3Ja3SciyJcbT5rCRza1hHLNnrWtjust9qJVAJSHj8Gg74c9EESbnMGxvRVtZI75D3FftWyrKb+zEW3MSHfmWGVnM8ccwbSwJHU+g6+6L33HhBB9xWbBUKB+j28LnEigr7HDXaakq6b8HvUdi3b8DpfMMSEGtlpncKyVaolJ10BIAlQVwaqXVVBs8d9g/mj46Bp46Vj411j4/mEoyWu+b/keWP5ftT3vETCYevNV9J7hp6vbHd/Cmtch9031uTAwGo653z1Ob4DR56ntde90bqVKp3Bnz7FdPbMQguhzHf+mFUIMKL4ZM2AVDKnaqtKh/UJaHaeV56MDKrRAEuLiWh1jiB8NhyC4Qv3CrC1YSyCQpyWTkxDW5hzOnZjICz9nQDWUFeTRZ/lbVjNhlWquPimT++qsoockhqmyyrKaRuobba2vNDokDerLVd+x2JHNn2uoggpHqXDsKHdJb/U+te3JB7vGOnWVFNRqlTodhMRBeb7qOzYktUuvTQghOmXvGgyWGg5pQWgxIzseD0x2ZI5t2V9NeW0jQwJ92h4c48gck+CYEAObq6wyuuVzJn+Y80eVMbX4cfjpcZUZBerzVM0B9flmyT/Uj28I+IWBfyiYq8FmVhlV2Sf01avpedNuBJOfqjjQG0BvVD+Zc8E/rPnYMRfA0vkqc0yzg84AcaM9P9eQVNDp1cJQNQfUogBiQJHMMSEGmZyhw9hjj0aPHQrbXrWyct92AAq0GBLC/FsdE5E+DrumI8R6EGpKqNitrqIU+mQQ4td2oEGn0xGZoso7/Wv2dPWldN6BTRg1C4e0IFIyW/ZEE/1biL+RYEd2RJvZY+31HXN+yQuOh4BwCIxSq1Zq9raX8T6cM2vMJ1h9QHQeD6TvmBCi7zj6jS2zjyAzpvWLXIeLDPIlM1pdjlqZ784eW7azjDP/u5TvNh9wD44erm5L8iTDQYiBrKadzDGAMRdBRBaYK1VgLCoHzl0At6yBu3bA2S/D0JNAb1IN6isLVBZ+eb4K9Mz7a/9cjdJTPoGq/9qcP8Lsu2Dm7ep+9LCWY6NzIG6s+twI6n3SJ9Dzcxl93P0cpbRyQJLgmBCDzPjkIfymqXKJqq2L2xxXXayCY6XGuGarTjY1NDmO3Zq66mErXg/7VMlazZDhHc4jOG4oAOHmYrDbPJ5/dzQUrQNgkz2VcclD+uScoufodDrXipUd9h1rrazSVVLpCIzq9Sr7CzwvrXQ1409yfxgMcWRWyoqVQoi+4giOLbWPdAW8POHuO6aCY9/nHeCKV39jbUEFb65ocrEqIlNlT5irOt+XUQjRf7TVkN/JYISzXoARZ8I5r8LvlqltvV4FfkadAxe+DXfnw82r4Zrv4eIPVdDsqkWdy5waDMZe5N5O7ES/MSdpyj+gSXBMiEEm0NdIcahqLtm4a2mb4xpLVeZNdUBim2NSwgPYpksF4NCuVYRVbQHAJ3Fsh/OISkinUTNgwuJ51k43VToWDijwySA6xK9Pzil6lrspfxcyxw5sUrdNyy1dwTEPv/w1bcbv5FqxUoJjQog+YK6BIrXy8i/2kWR1Jjjm6jt2kM/XF3P9G6tptKosiB0lNe6BRh+IzFbbUlopxMDlyhyLantMwniVLTbyLBUUa41vEERmQuJEyDpOBc2SJvX4dPu9kWerCwfQuWb8Ts6+Y5I5NiBJcEyIQciYNh2AsPINYGk9yKCvyAfAFprS5nH0eh2HglUGmG7bNwTaqrBoBmLSx3Q4h9SoEAo11f/AXNJHvyAOqMyhuvCOM9tE/5Q4RPUda7Oscogjc6y8lcwx10qVTYNjjvT2Ck8zxxzjwpoEx5wrVvZRkFcIcYQrWA52C4X2KAq1mE5ljk11ZI5t3lfF799ei9WuMXe4yigpKq+nrtHqHuwsrXReWBBCDDwdZY6JzgmMhKm/UxlgXem15swca+0iruj3JDgmxCCUNWwMB7QwjJrFvdrKYQLrVCaNISKt3WPZo1WgIfLQGgB2aPEMS2rn6pRDWICJIr3KuCkv2uLx3LvMbiesaisAPglHWAr4IOJx5lhlEVgb3Y/b7XDAkf0QO8r9eKfLKoua7weSOSaE6FuOkspf7COJDPIlLKCdxvqHiQ7xIy0yEE0DuwYXTk7iuUsmEBmkjrGzpLbJYFmxUogBzWaBuoNqW4JjPWfeI3DL6tYXOehIhONzqmSODUgSHBNiEJqUFs5Ku2o0Wbv955YD7DaGNKov+sHxWe0eKziteUrxDn0acaEdlyzqdDoq/VT2Tf2B7Z5Mu3vKd+Nrr8esmYhNH9XxeNEvOReHaLPnWFA0mAJVs9SKAvfj5bvBUgsGX3dKO7gzwDwNjrVWVimZY0KIvtSs31gnmkE7zBuhviRfPTONR88chUGvIyNKZZ9tL6l2D3StWJnXrekKIbyktlTd6gzgH+7duQjFlTm2u/s9lw/ulH63fUyCY0IMQmEBPhQEjQWgfvuSlgOqijFhxaIZiIpPb/dYGWkZlGqhrvuVocPQebhqjTnEUbLZB6nF1uL1AGzRkhieGNHr5xO9w1VW2VbmmE7XpCl/k39XzpLK6BzVfNbJGeTqdFllMj9tK+Uvn22mMcCxFHf1flnVTQjRu2pKXe9ny+wjyIoO7vQh/nD8MH65+2geOGW46/d1VowzONak75izrLJ0q8pAEUIMLK6Syui2e4mJvhWapFZKt5m7t9hJTQk8NwtemQc2a8fjRY+Q/0VCDFJasuo7FlK2tsWH3oZStYJKkRZJSmT7S8RnxQSRpzXpSxbreVaWPkJl8PhVF3Qwsvsqdqvy0e26VFdpnhh4nH93JdVmGixtXHEbkqpum/Yd2+8IjjVtxg/NG/J3FNiyWd3ZYaFJ/PWLzbyydDdLDxgcz5uh7pBnL0QIITqjogDWvQOf3waohWUOEdKpfmNOBr3OdaHByRlk236gSXAsNAl8gsBukRIgIQYiVzP+LpT/id6hN7j743bnfbVguaqIqChQ26JPSHBMiEEqJWcCFVogPvZ62Le+2XMVRdsAKNbHEhpgavc4fiYD+/zdpZdh6eM9nkNAnNov3OxBYKKbLHvVa+xMZpvof8ICTAT4qGBUcVtN+Q9fsVLToHit2o45LHjrDI5Z6919OdpSXQyaDfQm7IHR5B9UpZ2FVTYIiHSPEUKInpL3GTw9EuaPgoXXw5bPAfhRUy0NOrNSZXucx9nRtKxSr+9e3zFLvWTTCuFN0oy/f+qJpvxFq9zbeZ91bz7CYxIcE2KQmpweyW+OvmMNO5v3Has7oDLHKn0TPDqWOUL1JdmrRZCV0vbqloeLTszEqunx1cyqJK09mgYfXgOvnQa1ZR6fwynwkLMZuzTjH8h0Op0re6zNFStdZZW7VVDstVNhx7fqsfhxzccafd0fGjvqO+Zqxp/A/upGGq129zxCHE35pfeDEKIn/TJfvTfpjZA4Cab/nsbz3uHhmtMAupQ51ppMR1llwaG65lm5rhUrOxkc27MMHkuCxX/rkfkJIbpAMsf6p55oyr93jXs77zO18JTodRIcE2KQignxY7u/yqKp2do8OKY5ytEagpM9OpYt+wQ+tU3jafsFZER53hw4JTqMIk2tbNlY2sEviJoS2PA+7P6p8wGy2oOEWFRT0iHp4zoYLPo7ZzlQhytW7voRXpgD+UtUI/4590DylJbjPe071qQZf/5B94puxRUNEOxoyi+ZY11Xdwh+e1llmwgh1Iq7+x2Z3Tf+Ctd8B/MeZueQGVg1PSF+RqKCfXvkVFFBvoT6m7BrsKu06YqVjuBYZzPHVjyvyjEdiwcIIbzAFRyTzLF+xZk5dnBn1/a329wVETq9+uxZvKb9fQCsZjiwqfsLARzBJDgmxCBmTZgKQOCBVc2uOPhV71EbzgycDkzMTORW6y0cSD0do8Hzt42IQB+KdKqZeXnR1nbH2vZvct8p2dSpAJm2fwMAu+0xZCfFeTw/0T85V6x86tttnPHMUi584VeuXvAbP25xfAh09nKwNarb0efDLatgzh9bP2DTvmPtqXT0xgtLZs9B92qZxZI51rr6CvVBzFPfPQhf3AE/P9FbMxJiYDmwUb2P+Q9xf5nC3TQ/Kya4x9oE6HQ6d2llaZO+YzFdCI6Zq2Hb12q7am+PzE8I0QVSVtk/OVdN72rmWOkW1W/MJwhyVBYxeZ+2PtZqhq1fw0fXwxOZ8Ox0+OWpto+dvxR2/9z280c4CY4JMYjF50yhVvPF31YFpXmq4fjX95JQtwUA39gcj44zKjGUT2+ayfzzx3bq/DqdjnI/FZio37+t3bGH8tcBkGtP54AW1qkAWVW+upqyhVTXilxi4BqXHAZAabWZ3MIKlu86yPdbSvjrl3lqQFgyjDwbsk+A636Cs15Qj7UlzJE51lFZZZuZY/WSOXa4yiJ4KgfePNuznkOaBjt/UNubFkqfIiHAlQmgJUxQK/E67HAExzKjevb3mfP3444DTfqOOTPHyvPBXNNyp9Zs+RKsDWq7ep9kKQjhLc7MscAo785DNOe82FFRoDKEO8vZbyx+HIw4Q23nfdbys9O6d+CJLHj7fFj/Dpir3I+3pqIAXj9NtSN591KoPtD5uQ1yEhwTYhCbnBHLans2AJZNn8P/zoVfnwHgH5ZzGZI8wuNjjUoMJSKo8+Ud5uBUAOwH229KWb9XZY79ZB/LhY33Nw+QNVS1u29tgUo9LgnIwtdo6PQcRf9y5rgEvr5tFm9dM4WXLpvIY2ep8uD8slosNrv6EnnOK3DRuxA/ttVjvLeqkMl//Y7cwgoIdQTOKjpYNdUZPAtLYk+ZO3PsQFUD1iCVASmZYw67fwZLnSpp3fpVx+PLd7v/fA/t8ixLRdPgo+vUhzgpxRSDkaOnzCu7wvhpW6nrYWfT/J6+2JPpXLGypEkQLDASAh39ikq3eHagjR+4t+1W9xd0IUTvqChUF7gPJ5lj/VNwLPiHq0WeupKltXe1uk2cCJlzweinPjsdaFJlc3AnfHYrmCshKBam3ACXLgS9SWWslW1vedzNn6r3bFCZaM9MVoE0uWDpIsExIQaxpHB/8nxGAmD6+VHY+QP1+PK7xlt5x/8CRiWG9vocdI6mlK5SzjYYD6oP5WGpYxg1ZiIXNt5PiTNA9uuz7e7rU6p+WViiPA/2if5Lp9MxLDaEGZmRHDc8hvMnJuFvMmC1axQequv4AMCHq4soqTbzQ94Bz8sq28gcs2tQbohQd6olOAbAvnXu7cWPdfzB6vAPh56svFS4Eta/C7t/xrLug47HCzHQOL4ALW1I5XdvrlbBfNyZYxk91IzfyVlW2Sw4Bp0rraw75M4CNfqp2yrJqBWi12z6GOaPhCVPtnxOeo71TzqdavkBsPrVzu/vDI4lTADfIMg4Vt13fnbSNPj8dpXBm3YU3JEHJz4OGcdA2iw1ZssXLY/rLM2cdI1awKyhQq2S/L/zoFJK5EGCY0IMajqdjoZ4d4PyYi2Ss81/ZmvEMXz4u2kE+5l6fQ4BsVkADGkoavsLtKYRXqPq8gOTRvHkuSpA9rDlEgCsy59pO3vM0sCQOrXAQECyNOMfjPR6HamRaiGIZo2k26BpGpv3qX8vheX1npVVbv8WKlQAVwtNdPUc83H02Nuvhatx8iVQaRoc27++9Q9hTTmDY84+HB4Ex6p/fsa1Xbf0uc7OUIj+zVwNpaoX53p7BnWNNq5a8BvbD1Szu0y9z2X1dHDMkYmWX1brWo0XgGjHhSVPVqzc/LHKPIgdpX4Aqjq48CCE6LpVr6jbnd83f7yxFhodJdKyWmX/M+Fydbv1K6je7/l+jbXuCxUJE9Vtzqnq1vnZKfd/agEzox+cOh/0TUI6Q09yn7epqn1QuEJtz7wDrv0Bjv0TGHxg+yL34jBHOAmOCTHIRebM4ld7Dj/YxnKq+WFC0yaw8HczSInwfNXJ7ohIzMKu6fDX6truH1ZZiJ9Wj1kzEpc6HKNBz5PnjqEh+1R22uMwmivht5da37c0DwN2DmlBpKVl9d4LEV6V7lgl1fmlsT1F5fVUN6i08YJDde7MsbqD0HhY5ln5Hnj7InjrHNUYO3YUpYYY6i029DoYk6SyKwssjizL+kNgaeiZFzVQ2e2wz/Ehatgp6nbx39peZlzT3MGxuX8BnUE1Im9vFaeqfQTs/BwAi2YgtHwjFK3uoRcgRD9QnAto7NUiKSOUzOggDtU2ct7zy7HYNPxNBuJD/Xv0lLEhfgT5GrHaNfY0yY4l2tF/tGRT6zs2teFDdTvyHAhJUNty0UCI3lFTqtoXAJTkNb/I7MwaM/qDb3Dfz020LzoHkqao0sq1b3q+X3EuaHbV69a5GNTQE0BvVO/RBb/CovvU43P+6F7B3Wnoieq2cIX69+O0RX2mInEShCaAwQSz/g9u+AWOud+93xFOgmNCDHKTM2K4oPEBrrL8gWMmjOC1qyYTGtD7GWNOKTHhFKNK0qxlra/a0lisPpDv0uLIih8CgNGg548nj+S/ttPVvkv/ra6mHKauIBeAzfYUhsf3fpmo8I4MZ+ZYWccNo51ZY4Aqw/QLAx/HB0dnaaXNqlZNfGYybP1CBWym3QxXfEl+uWqemjDE3xVEzq/1cZcQHemllYd2qlWUjP5wynz1Z3tgg/uD1+FKt0BtqRqfNded8t9e9tjqBRg0GyvtQ/nUPg0A+8oXe/Z1COFNjrKZXHs6PkY9b187ldSIAMrrLABkRgeh1/fMSpVOOp3OVarZrLTSVVaZ1/4Bqophz1K1PfJsd3Cso5J1IUTX5H2qAiWgmq03/b/mKqmMbragh+hHJlyhbte81vYFxMPtdTTjT5zgfsx/CKTNVtv/Ox/qyyFmlPrcerjQRIgbA2iw/Rv345s/UbfO1S+doobC7Ls8m9sRQIJjQgxyWTHB/OmU4Tx+9ij+fs5ofIx9+98+OtiXAlQz8/Kira2OObQ7F4B8fTJRTZr+Z0QFYRtxDnvs0RgbDsGqlnX7VbtVQ+MCn8w+DfqJvpXuWLVtpwdllZuL3cGxkmozDVZ7k75jheoL3munwg+PqH4NKTPVlbPj/wp+Ia5+Y6kRgcSHqcyN4soGCHZcwTvSg2POksrYkRAUBVNvUPd/erz1D3+OrDFr4hQ+23QQ27DDygMOZ23E9psqI3nDNo8P9Y6rmRs/hNqDPfUqhPAux0qV6+0ZJIT5ExXsy+tXTSHS8Tuwp0sqnVx9xw40CY5F5agmzrWl6v9ZWzZ+BGiQNFWVq4dK5pgQvWrTwub3mwawpRl//zf8DPANVQtC7frRs32a9htrylla2VABOj2c9i+V/dUaZ2nlli/VbW2Z+8LG8NNa30cAEhwT4ohw1cw0zp+UjM4LV5Z0Oh3lviowUbtvW6tjGvepzLGK4KwWc7zp2GE868we++WfLUvaDmwAoC48pyenLfoZZ1mlJz3HNhU3709XVF7n7ju2/l14biYULFMZT2c+D1d87s6cAFe5UUpEAAlhKlusuKIeQuLVgCP9i+C+XAC02DFomgbTbgLfEFUquaWVgJcjOLbYMoxb3l7Ly2XDAZ26OtpaA9i8TzHUlXBAC+Ng0vHEDp/BensaensjrH2j916XEH3JsVLlOi2DeMf7THJEAG9dM4UzxsZzzaz09vbuMndT/mr3gz4BMONWtf3ZbVCe3/rOzlUqR52jbl3vidLIWYgeV33AHdBInKxum5Y+u4Jj0m+s3/IJgDHOxvwLPNvH2ULC2W/MadgpgOM70pTfQcL4to/hDI7t/EGt9r3lc5WBGDcGhqR6OPkjU5eCY//9739JS0vDz8+PCRMmsGTJknbHP/PMM+Tk5ODv78/QoUN5/fXXW4z58MMPGT58OL6+vgwfPpyFCxe2ciQhxEBUH5wCgP3grlaf9zmkMsrsUcNaPJcVE0zd8HMp0iIx1pU0/3JstxNWpfb1SRjTw7MW/Umao6yyrMZMVYOl3bF5jrJKg6MkSfUdaxIcqzuoGklf/xOMuaBFOUK+oxl/08yxvRX17syxHd97nh4/GDn6jf11rYlb38lV6f5Tf6eeW/w3sNvcY+02V7+UTytVT8B3NjeiJU9Vz7dWirnieQDesh7H3FGJzBsRyxu2uQBoq15pfnwhBqKaEqgsREPHBnsaCWHu3mJDY4OZf8E4hseH9MqpnU35dxy+YuWcP6ov4OYq+PAasB32PntwJxSvVSXow89Qj4U4MnKP9AsGQvQGZ0llwkTIPl491jRzrNbRT0oyx/o3Z2nl1i/dpbBtqd7vWOBEB/Fjmz8XFK3KH4edAkff2/5xYkepz73Wetj1E2x2rFJ5eEmlaKHTwbF3332X2267jfvuu4+1a9cya9YsTjzxRAoKClod/+yzz3LPPffw4IMPsmnTJh566CFuuukmPvvMfXV5+fLlnH/++Vx66aWsW7eOSy+9lPPOO48VK1Z0/ZUJIfoPR7NI36r8ls/ZrEQ4VpsMTBzd6u43HpvDc1aVTmz5+SmoKFQ/hSvws9dh1kzEZozqlamL/iHYz0RUsCo3ai97rKKuUQWygKnpaoXJgoNNMscAJl4NV38HERmtHqPAERxLaRocK69Hc6a0r/sffHQNWM3dek0Dkqa5yiqX1yXx6bpi1haUq+CYX6haYWnlC+7x+9dDQyWabzBfHVRXt3eV1XIgXgW7WpRWFq+FopU0agbeth3D8SNimZ0dxSLdDCq0QHQVe2DHd33xSoXoPY6ssRLfFGrxJyEsoM9OnRWt+i/uKqvFamsS5DeY4OyXVAlQ0W+w+DH3c6Vb4Ys71Hb6HFVODc2zaSVoLUTP2vSxuh1xJkQ7+wI2WVFWyioHhpgRqgm+3Qq5b7U/1llSGZ3T+iILx9wHF7wFvh2U3et07gb7uW+plS0Bhp/eubkfgTodHHvqqae4+uqrueaaa8jJyWH+/PkkJSXx7LPPtjr+jTfe4Prrr+f8888nPT2dCy64gKuvvprHH3/cNWb+/PnMnTuXe+65h2HDhnHPPfdw7LHHMn/+/C6/MCFE/+EfozJGQhtaadpbvhsTFuo0XxLShra6/7DYECqyz+eAFoapphjmj1Q/r54AwFYtkZyEiF6bv+gf0p1N+UvbbsrvbMafFO7P8DiVeVFYXg+jzlMNpM99DU55Ckx+re6vaVqTnmMBrtXiahttVKWfrMow9UbVl+eNs1RT1CNJRQE0VGDTGdmmqayRZxfvVNljxz2kxnz/sBoHrpLK8qjJWDSD6zCfmMepjT1Lm69i62i6/4V9KvGJKcSH+RPgY2RSVgLv2eY0GyPEgOX4ArTVkA2oxT/6SkKYP34mPY1Wu3pvbGpICpz2T7W95ClY/x58cjP8dyrsWqz63Ey70T0+OFZlkmm2jjMihBCeq97fpEfU6e7WD6Xb1IJC0KQhf1Tfz090jjN7bHUHjfnb6jfWFc7gWN6nKjAXlQORWd0/7iDXqeBYY2Mjq1evZt68ec0enzdvHsuWLWt1H7PZjJ9f8y8h/v7+rFy5EotFpWwvX768xTGPP/74No/pPG5VVVWzHyFE/xSRqL4ABNmroe5Qs+fqi1TPsG1aAtkxba82+bu5w/mb5ULqNF9sOiNWnQ+Nej8qtQA+1h9HYh9+uRDe4WzKv7us7cwxZzP+4XEhJIerbAxVVpkA57wCI85o9xzldRaqG6zodJAUHoC/j4HwQB/A0XdszAVw8QeqX9meX+Dl44+sldocWWMFxhQsGAFYtPkAO0qqYfzlkDxdrWT5xZ0qy8wRHNvkOxaAED+1z9vbdGhxY1XJyMIb4K3z4N8TIfd/ALxuncfxI2Ndp503PJa3bMeqOzu+g0O7++DFCtFLHF+A1thUVrWz51hf0Ot1ZLqa8le3HDDiTPV/GQ0+ula1MtDsqpTnd8sg87gmBzOoABlI3zEhetLmTwFNlTqHJUFoMpgCwWaGQ44WJZI5NnCMOFP1Zi3frYJVbSlyrFTZE8GxlJnqnE7SiN8jnQqOlZWVYbPZiIlp/p8wJiaG/fv3t7rP8ccfz0svvcTq1avRNI1Vq1bxyiuvYLFYKCtTV4v379/fqWMCPPbYY4SGhrp+kpKS2hwrhPCupNhI9mmqxM12WN+x8j3qy3ahMbXd1SZHxIdSM+wchptfJaP+dTLrF5Bd9wpjzC+xOf5sryw2IPpWhgdN+Z3BsRHxoSQ6gmOFh+o8PoczaywuxA8/k8p0im/alB8g42i46msIjoeyrbDogc69kIHMERxb06j6CDq/ZD/30y7Q6+HU+WDwUcuHr38P9iwH4PsGlRV6zax0fI168g/WUZLouCi241s1/uB2QOMn+xjWapmcMMIdHDsmJ5oCYlluGw5onq/6JER/o2mulSqX1KrProl9WFYJ7tLK7Yf3HXM64W8qywDUF6yrv1OlPNGtLHwT4lyxUoJjQvQY5yqVI85Ut3o9RDv68jqb8rsyxyQ41u/5BMLka9X2F/8HNaUtx9jtqrUEQOLEls93ltGn+cUMKan0SJca8h/+JVTTtDa/mD7wwAOceOKJTJ06FZPJxOmnn84VV1wBgMHgLrHozDEB7rnnHiorK10/hYWFXXkpQog+EBfiRwHql3dF0ZZmz1kdK1VWhWR2eJxHzxzFnfOyuXZWGhdPSeascQmcNiaeu45v2chfDD7OFSt3elBW2TRzrPBQnVpV0QPulSoDXY85SytdwTGA2JFwxn/VtjMN/kjgCI6ttabgY9Dzt7NUr7+P1+5Vvd6ihsKs/1NjP70FLLVoARF8sT8MgBmZkRw9VPUee1s7HiZfB9NuhpOfgss+4Zu533F54x/Ijgl2ZQoCRAb5MjFlCOu1NPXAgSZ9V4ToCk1TpYLle/r2vOW7ob4czeDDemsSOh3EhvZd5hi4g9otmvI7+QTA1YvghqVqNd+kSW0fzNl3rLWVZ4UYbD6+EZ6Z0vaKrj2hqhgK1IWlZgENV9+xPPX+JatVDixH3a3+DuvK4LNb1d9hU2Xb1IIopgD3xYnuyjlF3UZkuf/9iHYZOzM4MjISg8HQIqOrpKSkReaXk7+/P6+88grPP/88Bw4cIC4ujhdeeIHg4GAiIyMBiI2N7dQxAXx9ffH19e3M9IUQXqLX6zjokwiWPGr2badpdzD/im0A6KI6ftOOCvbl5mOkXv5IlR6pvtDlH6zFbtfQ65tfQGmw2Fxf9obHh7jKIWsbbRyqbSQiqOPfGflljpUqI92ZHO4VKxuaD451LAJRsQcaa9WVwcHOERzbZE9lWEIwE1PDmZ4RwbKdB3lpyS7+fOoImHk7bPxIZdUB9fHTKd1oxWTQMSI+hJNHx/H1pv0szKvm1jv/3uxC2MKlqwFds6wxp3nDY8krcGSJl0hwTHTTunfg4xsAnVoJbtK1kHGMytBoi90O5krVY6899eWQv1Q1Qd6zXAWQZt+lgkyOZvx14SOw1BqJCfbFx9ila9VdluUsqyxppazSyS9EXQToSKhzxUoJjolBzmZVK17braoVwNXfdPxe0BXOksqkqaolhJMzuHFgEzRUgK1R3Q+U4NiAYPSFs16AF46GrV+oNhLjLlbPHdoN71+hthMmgKFTIZq2DT8TTi5X/5akwsYjnfpt7OPjw4QJE/j222+bPf7tt98yffr0dvc1mUwkJiZiMBh45513OOWUU9A7PoBMmzatxTEXLVrU4TGFEANHfVAyAD57l7uvllgaCG9QWZ9BKa2vVCmEU+IQf0wGHQ0WO8WV9S2e31FSg9WuERZgIi5UlUXGhqiMjBaNp9vQWuZYQlgrmWMAgZHuD6WlzTMiB6Xq/VBbgh09eVoyIxNUj8DfzVGrfr6zspBDtY3qA+Cp/3Tttj1oPKCy+fxMBo4ZFo2fSc+eg3VsKnb3Cy2tNrN4myoTadpvzGnu8Bi2aup9xH5gU8urrkJ0xto3HBsabPsa3job/j0eti1qe5/Pfg9/T4fFf2t9dcbClfDSXDXm3YvVyq0HNqiy4ZePg/+d7yqXKg1RX3Sd7y99yZmVmV/meVZtm1wrVkpwTAxyVUUqMAbq4s87l/TOqtVbv1S3h5fBxTTJHHOWVPqFtrnAkOiHYkfB0feq7a/uVosX7V4CLx4DpXkQFAvHP9pz59PrYdI1nl3oEEAXyirvuOMOXnrpJV555RXy8vK4/fbbKSgo4IYbbgBUueNll13mGr9t2zbefPNNtm/fzsqVK7ngggvYuHEjjz7q/ou/9dZbWbRoEY8//jhbtmzh8ccf57vvvuO2227r/isUQvQL+xOOp1EzEHdwhfsX/8HtGLBToQWSkpzu3QmKfs9o0LtKJVvrO9a0Gb8zGykpXH3xLPCw71j+QTUuJbxl5liL4Bi4e/CU5Hl0/AHNkTVWbEyiAV9Gxqvg2MzMSEYmhFBvsbFgWb4amzINjv0TJE3hG20qAOOS1RX2QF8jxwxTQcUvNuwDYG9FPec/v5wGi53M6CDXSqNNpUYGokVmY9X06BsqoHpfL75YMahVFLpXgrv0Y5jyO/ANVSWPH14N1saW+9gssOlj1Zx+8WPw1jnulVatZvjuQXjleChaqcZEZqtstHNegXGXqFUdt30NWz4HYLePageQMKRv+42Bel/U6aDGbOVgbSuvtTNcPceKuz8xIfoz50IwAZHuRXk+uan91Qc7y1IPBb+q7ab9osCdOXZol7scXPqNDTwzboWkKdBYDW+cCW+cAfWHIH4cXPcjxEmygDd1Ojh2/vnnM3/+fP7yl78wduxYfv75Z7788ktSUlRz3n379lFQUOAab7PZePLJJxkzZgxz586loaGBZcuWkZqa6hozffp03nnnHV599VVGjx7NggULePfdd5kyZUr3X6EQol+ISh3OCzZV+27/8g/QWEt1wXoAtmpJZMUGe3N6YoBwZjzsaqXv2KbiSgBGxLsDK0mdbMrfas+xwxvyN9W0B8hg5wiO5VrV7/uRCerPWafTceMc1TPwtWX51JodV9Zn/R9cvYhlxSozZVxymOtQJ49S2SZfrN/HztIazn12GbvKakkI8+fFyya22XN0WGIU+Zojq0z6jomu2viBuk2ZoRbYOPFvcMdm9aXXXKUCXIfbu1p9mTEFgtEfdv4Az82C3LdVmcwvT6ug2OgL4I48uPk3OPkfMPJsOP0ZuGml2gYw+LJO773MMV+jwdVLcc9BzxcsaUrTNH7ddZA6f8eXc+k5Jga7ckdwLGE8nP866I2w4X348ZGeO0fBr2pFyuB4iDysjUhgFAREAJoq2QYJjg1EegOc8az6XXJwh8pGHHUuXPmVOxNXeE2XmhzceOON5OfnYzabWb16NbNnz3Y9t2DBAhYvXuy6n5OTw9q1a6mrq6OyspKPP/6YoUOHtjjmOeecw5YtW2hsbCQvL4+zzjqrK1MTQvRTp42J59OQiyjSItFXFcHP/6BqjwqOFfukEuDTQ/X1YlBzNuXfVdZK5pizGX+T4FhyJ4JjlXUWyussAKREuLM5nF9e91c1YLUddoXYlTl2BARqnCtVWlIw6nUMbRLQPn5ELGmRgVTWW3h7pfsCmdlqc2X0jUty92Y5elgUfiY9BYfqOOM/SymubCA9KpD3b5hGWmTbvdvSIgPZojn7jm3qyVcnjiTr31e3o851P+YbpHqOAez4vuU+Ox0rpGbPg2u/Vw2Oq4tV37KSTSqwdv6bcNbzrX/BicxUWWQ3r4brfmRTfRgACWHeKYlyvsc5Lwh01tcb93PBC7/y1ArH/tX7Wi81FWKwcDbhH5Km3iuc7QOWPAlf3Kmyvrpr12J1mz6nZY8onc59QW6XMzgm/cYGpIgMOO1fKvP2uAfhrBfB1PcXSkRLfdsBVAhxxPIzGbj39PE8aLkcAG3ZvwnY8x0AtaHZ3pyaGEAyHE35dx8WHLPbNfL2qebSw+NCXY8nOUqWCss7Do7tOaSOGRXsS6CvO1gbGeSLyaDDrsGB6sP6ixyBmWMb7alkxwTja3SvOG3Q67h+tiqNfmnJbsxW9SV5U3EVjTY7EYE+rhJXgAAfI8cOU1e8q81WRsSH8P7101wlrG1JjQxkq90RHJPMMdEVBzapYJbe1LKnT+ax6nbHdy332+UIjqUfDTEjVPmLMxNs2Clw46+Qc2rH54/MhJgR7HX0QUwY4p0vRM7gWH4XM8eW7lQlpbmHfFTJqGZzr54nxGDkLKsMd6yaPO4SOOYBtf3bi/DCHNi/oXvncGaEpc9p/XlXU37HeSRzbOAadY7KWJ55uzTL70ckOCaE6DNzhkZjGn4y39nGobNbGFKzAwBDrCwvLDzjyhw7rOdYYXkdNWYrPka9awxAsuMLoCc9x5xfElMjmvcA0ut1xIW20XcsypEJXb0P6g55/kK6w1zTd+dyqj0IlWrxjM1aiquksqkzxycQE+LL/qoGPlmr+g/lFlQAqqTy8FLJ8yepINfk1HDevm6qR6uJpkUGslUyx0R3bHBkjWXNg4Dw5s85M8f2r3c3vAZoqISiVY4xR6tb32C+HvYoV8a8z5upj2ILiGz1dPsq66l0ZKQ2tdfxXpIQ1vc9x8BdOt7VzLHcwgoASmptEBynHpS+Y2Iwc5ZVDkl1Pzb7Trj4Q7U4T+kW1Vh9+TNd60NWdwiKc9V22uzWxziz1Z0Cozp/HiFEmyQ4JoToUw+cMpzHdVfRoJlcj4XKSpXCQ86eY3sr6qlvdJfwOEv3hsUGYzK4f7U5M8eKK1opiTxMQSv9xpza7DvmFwKhjmBNX61Y+eZZ8M+xKmDVV/arrLH9xgRqCGBUQmiLIb5GA9fMVNljz/28E5tdY63jC7SzGX9Ts7OjWPbHY3jnuqmE+JlaPN+a1MhAtjhWrNRKt4HN2pVXI45UdjtscPQbG31uy+eDoiHW8fto5w/ux/N/UZlR4RkQloymafznh+3c8OZqftxj4f6PN3Lqv39h5W4VtNY0jSXbS7nslZVMe+wHznp2KXa7e1XIGrOVynoVMIv3UlllqqussvOZYw0WG1scmbplNWZZsVIMfprmboI/JK35c1nHwY3LIftEsDXCN/fCiuc6f478JYAGUcMgJK71MTEjmt+XzDEhepQEx4QQfSo+zJ9zjp3Bf6xnALBfG0J6cpJ3JyUGjCEBJkL9VSClaWnlpiYrVTYVHeyLj1GPza6xr7Kh3WO3lTkG7hUr97a7YmUflPmV74HCFWCuVMt+9xXH1ez1NtWMf0QrwTGAC6ckE+JnZFdpLYs27WdtQTkAY5PCWh0fH+aPXu95OUGQrxFzYCK1mi86mxkO7fT8NQhR+KvKgPQJhuwTWh/jKq1s0nfM2W8s42gaLDZufzeXfyzaBsC84TGE+BnZvK+K855fzg1vrObEfy7h0pdX8vO2UrV7aa2rJyK4g+whfkaCPQwM97TuZI5tKq7E6gj21TXasAY7gmPSlF8MVnWH1GIdAENSWj4fGAkXvg0z71D3t3zR+XM07TfWlqhhze9LcEyIHiXBMSFEn7tqZho/hF/Af6yn82frVaQ7+kgJ0RGdTtekKb97xcqNjpUqmzbjB1USmejo6dNRaWVrK1U6OZvyt75ipTM41gfBqt0/u7er9/f++ZwcJWUrGtPQ6yAntmVZJajg1eXTUwF44putFJXXo9PB6MTWg2ldkRIVzHYtUd05IKWVohPWv6duh5/WdvPjzOPU7c4f3KVRjn5jVfEzuejFX/k4txiDXscjZ4zkhcsm8uOdc7hwcjI6HXy9aT9b9lcT4GPgiumpTE5VpZtLtpe5TuHuN+adkkpwL1ZSXmdpteyzPbmFlc3u1/k6vqBL5pgYrJwllcFxbb936HQw+ny1Xbym85nNngTH/EIgNNl9XxryC9GjJDgmhOhzJoOeP505jqftF1CeNBcfo7wVCc85g6m7SmvRNI2nvt3G4q0qQ6O1DCVPV6x0Z461VlbpDI61kn3Wl035nc16oXlPpN6kabBXBcdy7ZlkRQfj72Noc/gV01PxM+ldK4pmRwf3aHZMemQgW5xN+Y+EVUJFz7A2wuaP1faoVkoqnRIng08Q1JWpcuKKQji4A3R6HtoQwZqCCkL8jLx+1WQumaoySCKCfHnsrFF8dvNMTh8bzx9OGMryPx7Lg6eN4OTRqjxqyfZS1ync/ca8tzpZoK+RqGDV58+5GImnnP3GnCp9HH2PJDgmBqumK1W2JzIbfEPBUte5vpgVBXBol1rcImVG+2Ob9h2TzDEhepR8IxVCeMXU9Ai+v+MoXrxsorenIgYYZ+bY1v3V/N976/jX99sBuOnoDEYnhrUY7+w71l7m2ObiKkqrzRj0OlIj2y6rbD9zbLMKJPUWTWueOVbTR5ljlYVQcwCbzsBGLY0RrTTjbyoiyJcLJrmvbI9LDuvR6TRryi8rVgpP7fgO6sshKLbtZtcARh9IO8qxz/fuVSoTJrBsr8qweubi8czIbNmAf2RCKP+8YBw3zskkNEAFhGdlqXGr8supa1SZJO7gmHf6jTl1te/YOkdwzOAoiS7TO4Nj0pBfDFKHr1TZFr0eEieo7cKVnh9/l+PCV8IElR3WHudnDp1elXMKIXqMBMeEEF6TGhno+gIhhKcyHMGxLzbs46O1ezHodTx21ijuOn5Yq+OdmWPtBcdeXLILgJNGxbWa5eT8Ettqz7HIbPUhtb4cag506rV0SunW5sev7sVzNeUoqSw0ZWDGh5HxHZdIXjMrzfXFuaeDY02b8suKlcJjGx2N+EeeDfq2Mx8ByHSsWrnje1e/MUvKUa6+hZ78H3BKiwwkIcyfRpudFY6G/e6ySu9ljgEkh3e+79jBGrPrvXRiilpo4wCOVT+l55gYrFpbqbItiZPVbdFvnh/fk5JKJ2dT/oDIjt/LhBCdIsExIYQQA4pzxUqAQB8DL18+kQsnJ7c5PslZVlneSmALlQ322TqV8XDdrPRWx8SFqi+x1Q1WqhoO689j8odwx369WVrpKql0NLDvzUBcU47g2CprBgCjPOgfljgkgNuPy2JMUhhzh8f26HTSIwPZ6iyrLM8Hc02744WgsQ62fq22R57d8fgMR1P+opWuVSsLh0wBIDzQhyGBPh6fWqfTMTtbZVYt2ab6jhW7Mse813MM3Jlj+Z3IHFtfpPqNpUcFut6LC22O4Fj1PrDb2tpViIHL07JKgKRJ6tbTzDFNc/9+9yQ4ljIDTIGQPNWz4wshPCbBMSGEEANKWmQgqREBJIT5894N05gztP2GtEnhKrDVVs+xV5fuxmrXmJYe0WbgJ9DXSJgjy3Ffq33H+qApv7OkMnWmuu2r4Jij39jShlR0OsiJ66Dkw+HmY7L45KYZhHcikOCJpPAAynUhlGqOv6vSLT16fDEIbV8ElloIS4aE8R2PD0+D8AywW6GhAnyC2GQYCqjgbGfNdpRW/uzoO+Yqq/Ry5lhKZOczx9Y6SirHJoURFaT+bxc0BqleSZqt796XhOhLnpZVAiQ42oWU74basvbHgmrJUFsKpgBInNTx+NAEuHMbnPtax2OFEJ0iwTEhhBADismgZ9HtR7H4rjmM8KC8yZk5dqi2kRpz89WjqhosvL2yEIDrZreeNeYUH9pe3zFnU/5e6oFlt0H+ErU9+jx12xerVVoboTgXgLVaJmmRgQT5Gnv/vO3wMxlICPN3N+WXFStFRzZ9pG5HnKlWlPNE5rHu7dSZ7DxoBtw9DztjekYkeh3sKKmh4GAdB6pUgD3eyz3HUsI733NsXZPgWKSjoX9pjU2t4gfSd0wMPpZ6qHb8u/akrNI/DKIcbR48Ka10llSmTFc9Dz3hG6T6mwkhepT8rxJCCDHg+Bj1mAye/QoL8TO5sr4Ozx57e0UBNWYr2TFBzBka1e5xnE35i9ptyt9LmWP7cqGhUq2ClXW8eqz+kApe9aYDG8Fmpt4YSr4Wy6gEz3st9aZmTfllxUrRHnMNbFuktkec5fl+mce5t9OPZlepyq5qWtbtqdAAE2McK+l+sLoQu6bewyIDfTt9rJ7kXJm3pNrsWiygPZqmsa6oAnAEx4KcwTEzhMSrQZVFvTJXIbymokDd+gRDQIRn+yQ6ssc8Ka3sTL8xIUSvkuCYEEKIQa+1pvyNVjuvLs0H4NpZ6eg6yCjJilFfiv+3ogCLzd78SWfmWOkWsB/2XE9wlVTOgMAo0Duyt2pLev5cTTn6jW03DQV0jHV8wfe25itWSuaYaMe2r8Far3oFxY3xfL/UmWB0lD1mHsuuMtXbritllQCzslTw/b1VKniUEOaPXu9hFlsvCQ1wXzjwJHtsz8E6Kuos+Bj1DIsNcQXHymrMqtQL+n/mmNUMb5wJH17Tu6sLi8HDVVKZ6nnmqadN+a1myP9FbTtXyRVCeI0Ex4QQQgx6SUMcTfmbBMc+W1fM/qoGooN9OW1sfIfHuHpmGmEBJvL2VfHSkt3NnwxPB4MPNNZAZWGPzh1wL/OedpQqpQiKUfd7e8VKV78x1WelvwTHUiMC2WJ3rli5Wb7kirZtWqhuR57l+RdbAJ9AuOgdOPc1tIjMbmWOgbvv2P5+UlLplBLhed+xXEdJ5Yj4EHyMeqIcZZVl1WYIcQbH+vmKlblvqUUWNrwPjZ73WhNHsM6sVOmU5AiO7V0NtnayMgt+BUud+p0eO6rLUxRC9AwJjgkhhBj0nH3HXlyyi2te+437P97Av37YDsCVM9LwNXa8HHpkkC/3naTKJ+d/t635l0mDCSKz1XZPl1ZazeoDNEC648pykGMRgt5ufu246r3cnIqPQc/weM+a8fe2tKhAtmsJ2NFB3UGo6eUMOjEwNVTB9m/V9ogzO79/+hwYcQb7qxqoa7Rh0OtcWaidNTYpjOAm/foSwrzbjN+pM33Hcpv0GwOIdDTkr2200Rjo7DnWj4NjNgv88rT7fm9n3orBoTMrVTpFDlVtECx17Zf+7/hO3WYc07ngvRD/z959h7dVX38cf2tZ3ntvO8PZe5MQRggbAmWvsqG0zE5K+yulg0JbCoVCoWUUStl7E1YmBLK3s+O997ak+/vjqyvbsS1LsjzinNfz5JGQrqRriI11dM7niAEhxTEhhBAj3qyMKABK61r5bFcZ//0mj8OVTYQEmLhsbrrHz3PBzFSOGx1Dq83BL9/ahta5Y8mVO+bnDKyC79RYWEh8R8hvaKK6bBjAUP6mKqg6AMBmxygmpoR7VEQcDFkxIbRg5bDm7KArk9FK0YPcj8DeCjFjIGGSz0+jd42lRwcTYPbtV2ezyciC0R15RSmRvhXZ/C0zRp3HIR+KY6FWM1bnv486i7NgXzuMi2NbX+nIjwLPNgkK4c2mSp3RCKkz1fUCN7lj+79Ql50zDoUQQ0aKY0IIIUa8JRMS+OLHi3n6+7P4w3mTuPWk0Vw8K42HL5lORJDF4+cxGAz8YdlkrGYja/ZV8sbGTm8E9cKVvzvHXCOVx3d8shw2CGOVzryxCms6dYQOm5FKgNSoIMxGQ6eNlRLKL3qgj1R6s6WyBwfK+5c3ptNzxwBSooZJ55iHY5VtNgc7i+qAjuKYwWDoCOU3qbFRKvaoJQjDjd0Gq/7a9TbpOBWe8GWsEjpyx/J7yR2rK1ZLbzBA9om+np0Qwo+kOCaEEOKYkB0XysnjE7h8bgY/XprDAxdM4ZQJCV4/T2ZsCHcsUSOUv/9gpwqjho5Q/oLv1DiXv+hh/Nmdwnr1zLGBHKt05o1tYQwA09OjBu61vGQ2GUmPDmav5sw5qtw3tCckhp/mmo6RpUlebKnswX5X3lj/imPHdyqODZ/MMc/GKncV19FmdxAVbOkyWhrrzB3LDxgNkRnQUgMrHhiw8/XZ9jdUJ2xQtPqgAaCxfGjPSQx/DgdUH1bXvRmrBEidrS576xzTu8aSp0GIh1swhRADSopjQgghhJeuX5TF+KRwapra+fvnKruM1NlgCVGfMj+1GEq2+fbkbU2qIPbVA/D8uR2/WOtv6GBwimPOvLGVTRkATB9GnWOgNlbma85RrprDQ3syYvjJ/RAc7aqjUx959tGBiv6F8evSY4KZkxlNZLCFCUnDI79P7xwrqm2m1Wbv9bgtBTUATE2L7LLZN86ZO1berMHpD6obv3nc/x20/eGww6q/qOvzf9jRASTFMdGX+mI1mm0wQUSqd4/VxyqrDvQ8wrv/c3UpI5VCDBtSHBNCCCG8ZDEZueWEUQBsL6xVN4bGwVXvQHiq+mX4XyfDhue826RYngt/GQv/ORu++iMc+Ao0B2Qucr2hq2tpp9IYrY4fqOKYwwEFGwBYbxtNbGgAqcNkDEyXGRtCgebsxKmW4pg4wo631eXE/nWNgf/GKgH+c+0cVv/8JCKDA/r9XP4QGxpASIAJTYP8quYej2lpt/PiNyqra8YRHaQdGyvbIOc0yDkTHDb44MfDZ4vsznfUuGdgBMy5UeU3ghTHRN/0kcrINLV4xxtBUSqYH1wxBS4Oe0fn2KiT+3eOQgi/keKYEEII4QO9WFRa19pxY9psuHkVjFmqPm1+73Z491bP3yTuehfa6tXoz6TvwRl/gR+shavedR1y2b++4aa38tU/DFTmWOU+aK3FZgxkt5bGtLSoLt0iw0GXzrHafFXQEwLA1tYxjjzuzH49VUu7ncIaVTQaFd+/zjGAoAAToZ22Vg41g8HQZ+7Y35bvIbe0ntjQAC4/YoGJnjnmGi8//U9gDoLDa1QA/lBzOGCls2ts7g8gMBxCnEV1yRwTffFlU2Vnab2MVhZthuZqtdFSH78UQgw5KY4JIYQQPkiMUJlBpXUtOBydil/B0XDpK7DkXjWKsekFyPvGsyc9/LW6POFuuOAZmHMDJExUm6+A6sY2thfWUWSLUMc1lA5Md4ZzpPKQdSx2TExPj/T/a/RTVmwIxVo0doxgbxvYzZ3i6FK4Xm14DY5V3z/9cKiyEU2D8EAzMSHDo9vL3zLcbKxcd6CSp1aprbX3nz+FGGcxTNetOBaZDot/pq5/+iuV/TaUDq5Q22wDwmDezeq2UGdxTLZVir5U+RjGr9ND+XM/UkV7nT5SmX08mIZPsVyIY50Ux4QQQggfxIVaMRrA5tCoaGzteqfRCAvvhCkXqX/e8WbfT2i3Qf46dT1jfo+H7CxWQf8VOItjjnb16bO/HVQbMr9tU5+WD7e8MVDFMTsmijRnkLGMVgqd3jWWtahfWyoBDpR35I0Nt+5Jf9E7x/KO6BxraLXx49e2oGlw0azUHheYuLZV1nf6GTj/RxA7Vo0tfvH7gTtxT+R+pC4nnafG3KCjc6xROsdEH/SxymgfO8fGnqbGect2wvJfd9yuLwuRkUohhhUpjgkhhBA+MJuMrrydktqWng+a9D11ueMtVfxyp2QrtDWoX6T1zZdH0PPN2rDQYnYWyOr93DFVvAW2vgrA603TMRhgyjAsjiWGB2I1G8l3ON/oSii/0LmKY8e7P84Drryxfm6qHM4ye+kc+/37OymobiYlMohfn9Xzz6RYZyC/q3MMwBygRsIBvvv30BWuNQ32OItjY0/vuF0yx4Sn+jtWGZYA5z2prq/7J2x/U3VT6hlko6U4JsRwIsUxIYQQwkeJESp3rNfiWPYJqluhsRwOr3b/ZHnOkcr0+WA09XjI9qI61/Vasx7K78fimKbBhz8DNIrTzmKjNpax8WHDKiNJZzQayOocyl+TN7QnJPzL13HhtibXWDBZi/t9Gnrn2Kh+bqocztKdxbGDFY3sK2tg5Z5y/vHlPl7+Lh+DAf560VTCAnsOI4/VA/kb2rrekb3YWZzUYNurA3n6vSvfrX4umKzqfHR651hzNdjbh+bcxNGhv2OVADmnq05yUBmk3/0LNLvqroxMd/9YIcSgkuKYEEII4aPEcGfnWF0vxTGTBcafo65vf8P9kx1eqy7Tex6pBNhRVOu6XoFzRMifodLbXof8b8ASzFuxNwIMy7wxXWZMCPmysXLkWfOI2tq6633vH5u/TmXQhadAdHa/T2V/hXOs0g+bKoerTH2ssqqJJQ+t4KpnvuXPn+QCcN1xWczLjun1sfpYZUOrjZZ2e9c7p1yiLre8MjSbK/d8rC6zjoeATv/9gqJUHiRI7pjoXXM1NFep676OVepO/BVkLFTd4fqo8egl/XtOIYTfSXFMCCGE8FFSX51j0DFaufPdroG8nWlaR3Es47geD2lotXGwoiMTqNTh57HK1oaOTJRFd7GyVI1LDefiWFZcp42VMlY5Mhz+Gpb/RuVBvXkDlGzr+bjqw9Ba3/32ziOV/cwI0zSNA2X6WOXI7RxLDA9kQlI4AGFWMzkJYZyQE8cdS8bwk1Nz3D42PNBMgFm9neiSOwYw4Ry1ubJyLxRuHJBzd2vPJ+py7KldbzcaISRWXZfcMdGbb55QlzGjwRrWv+cymdWSndBOuX2SNybEsDP85iSEEEKIo4S+sdJtcSxzocq4aSyDA1/B2KXdjynPVZ9Qm4MgaWqPT7OruA5NU+/3NQ3y29WbWRpK+/lVOK36K9QXQ2QG9nk/YutnKpR/WlqUf55/AGTFhvCtJpljI0ZrPbx1E6BBQKjqsnjpMrjxy45ihsMOX/5B/X1NnAI3ftV1DNmPeWPlDa3Ut9owGDo2Oo5ERqOB929dSGObrdfxyd4YDAbiQq0U1jRT0dBKWnSnf0/WMBh/Fmx7Dba+DKkz/XzmbjRVdSw4ObI4BupnckOp5I6JntXkqw5WgJP/zz/PGZagCmT/OQesoZCxwD/PK4TwG+kcE0IIIXyUGK6KY8XuimNGE0w8T13vbbQyz9k1ljZbhVn3QA/jn5muilV5bc5Psv1RHKvcD18/pq6fdj97KttparMTajUzOn74dsx0yRyrLex76YEY3j75pSpyRqTBLV+rscjaPHj1+yobqqUWXrpUFcZALbHY+U7H41tqocjZoZS5qM+X21/eQENr739n9Lyx1KggAi095wCOFEajwevCmE4P5e/WOQYdo5Xb3xjcfK99n4HmgPiJPec66cXWBimOiR4s/z+wtaifI3o0gj9kLlTF/ms/hYCRW3AX4mglxTEhhBDCR3rnWGlvmWM6fbRy9wfQ3tz9flfeWO+fJO9whvEvGB1LkMVEuRap7qjvR3HM3q5yxl6+XOU0jToJcs5gU14NAFPTIjAZ+zeaNpCyYkMoI5JWzaICjusKhvqUhK92fwgbnwcMsOwJVdC49GUICFPLLN66Gf51Muz9BMyBHSNJK/8CDoe6fvhrVRCJzobINLcv92VuGSf/dQXz//g593+4i+La7t+XenEsO3b4FoiHAz13rFsoP6ilJCHx0FSpClaDRc8b66lrDCBUNlaKXhxeCzveBIMRTru/3+PZ3SRNhfhx/n1OIYRfSHFMCCGE8FHnzjHNXeB06mzVDdNWD3uXd72vS95Y78UxvXNsUnI4SRGBlBGp7vBlW2VjJaz8Mzw8Gd64Dsp3gTUcTvsTGAx8faASgGlpkd4/9yCKCQkg1BpAgebsApGNlUenxgp47zZ1ff4PIcvZ9RWXA9/7N2CA7a+r7KrwFLj2Y7jgaVU4K9sBuR+o4/WRSg+6xj7aVgxAfauNJ1ceYNEDX3LHy5vYWlDjOuZAuZ43NnLD+P2hozjWQ+eYyQyTL1TXt7w0OCdkb4e9zkLc2NN6PkbfWCmZY6Izhx0++rm6PuP7kDh5aM9HCDGopDgmhBBC+EjvHGtut1PX4makz2jsfbSyJg/qCsFoVkW0HrS029nrDAaflBJBYkQgZXrnmLfbKm2t8MQCtTGrvlh1dSz+BfxoPcTlUFrXwsfbVeHglAmJ3j33IDMYDGTFhVCoF8dkY+XRxd4O29+EF5apDp74CXDSr7sek3MaLLlXXU+frzLGkqerjYNzb1K3r3hQFZm9yBvTC8C3nDCKednR2Bwab28u4pzH1nDm31fxwteHXN2aIzmM3x9iw9RYZY/FMYCpztHK3I+huWbgTyjvG2itheAYSJ3V8zGu4phsqxSdbPqvGte2RsBJvxrqsxFCDDIJ5BdCCCF8FGgxERlsoaapnZLaFiKC3GT2TPoerP272qDWWt+x/Srva3WZPL3XDJI9pfXYHRrRIQEkRQSSGBHIVr041loHbU2e55eU7VTdZgGhcOZDMHEZmK2uu59be4h2u8bszKhh3zkGkBkTQn6JbKw8qjRWwIZn4btnoL5I3RYQCuc9CZbA7scvvEN1H4UlqUKzbv4P1Ua5kq2qK6nUudmyj+JYflUT+VXNmI0GbjlxNKFWM9sKavn36gN8tK2EHUV1/PqdHa7jR8VK55g7bjvHQHXfxE9QP3t2vg0zrx7YE9JHKscs7bqsoTO9OObthwti5GisgO+eVv8PtbeDo70jw/CEX3Tk0okhVdfSTmiAGeMwjngQI4d0jgkhhBD9oI9WlvSVO5Y0FaJHga0Z3v6B6uACOLxGXabP7/Wh2wtVB8vE5HAMBgNJEYE0EESbwVnU8ma0stT5pj9lBky9uEthrLHVxovfqALT9YuyPX/OIZQVG0K+HsovnWPDX5fOxSJVpDj+p/Cj7yBpSu+Pi0jpWhgDCI6GOder6x/8WF3Gje/Ik+rF1/tV19iU1AhCrepz4smpETxyyXTW/fJkfn3WBMY4F1EEWUyMSwr3/us8hsSFqZ8hPQbyg8psmnKxur7llYE/oT2fqMve8sZAMscEfPE7+OqPahnNt0/C+mdUNl7MGJhzw1CfnUAtTZn5u+Wc+481fWe7CuEH0jkmhBBC9ENiRCC7S+op6SHQuwuDQWV6vXI57HoPXrwQLnmxU97Ycb0+dHuRyhubmBzhfM0gwECNKZp4W7Hqfoj2sJhVsl1dJnTPUnl1fT51LTYyY4JZMj7Bs+cbYlmxIXymF8ckc2z4q9ynNqxaguGsv6lx404FWq/NvxXWPQXtTeqfvRipXDCqe2dIVEgA1y3M4trjMtlWWIvVbCI6pOcNskJxG8ivm3whfHav2sxbfQiiMgfmZCr3q2w6o1ktGOmN3hUkxbFjk70ddr6rrk+/AkITwRSgtkVPOBdMvm1uFf719f5K2u0a2wprWfaPNTz9/dlMSJYPK8TAkc4xIYQQoh+SIjpC+fs0dilc9ipYQuDgCnj2dFUswADpc3t9mJ59NClF/VKY7HzNCqLUAfWed45ppao49tSeIGqaOt7M2h0az6w5CMB1C7OG9ZbKzrp0jslY5fBXuU9dxk9QWVT9KYwBhMbB7Os6/rmP4pimaazdr3KmFoyK6fU4g8HAlNRIchLD+nd+xwBXcay3zjFQnX/Zi9X1TS8O3Mlsf1NdZiyAwIhud7fbHVz2r2+4+1Pnlt/GcpVXJ44th1ZBcxUEx8JZj8DJv4YTfg4L7/T8gyYx4PaW1gPqs8Xi2hYu/Odavtjdjw3dQvRBimNCCCFEPyQ4xyo9bvkfdSJ8/z0IioYSZ0ZSwkQVMN6DdruDXcX6WKXeOeYsyNmdn6B6mpujaTiK1Wu+UxzFJU9948oJ+mRHCflVzUQFW7hgZppnzzcMZMaGkK85R6Tqi6FdRi+GNb04FjPaf8+54FZVcLaEQGbvHZgAByoaKa1rJcBkZEZGz99zwjtxzuJYfauNlnZ77wfO+L663PQC2N0sMPFVfSmseURdn3pZj4esP1TN2v2VvLHb+XPCYYPmav+fixjedrytLsefrTaqimFJX0R0zxnjWTAqhsY2O9f/Zz0/eW0Lv3xrGz99bQt3vLyJv36ai8MhRW7Rf1IcE0IIIfrBq84xXepMuPZjCEtW/5y5sNdD95c30GZzEGo1kxEd7HzNIADy2/XimIedY/XFmFprsGlG9mkp7C6p56Inv6aktoV/rToAwBXzMggK6CXEehiKCLJgCI6hUXN2INUWDO0JCfcq96tLfxbHwhLhxi/h+s96LTLr9LyxGRmRBFqOnr/nw1l4kJkAk3pL0WsoP8C4s1SnTn0x7P3E/yfy+X3QVg/JMzoyzo7w1R71QUIbFhxWZ2eZbKw8tthtKtoA1EIaMWzpxbFZmdE8d80cLpqVikOD1zcU8L91eby2oYC3Nxfx6Bf7WLNfvo9F/0lxTAghhOiHRGehqsSb4hhAXI56M7/kXlj0k14P08P4JySHu7Y1RQVbCDAbKdM3VtZ7OGbgDOM/oCWxZHIGyRGBHChv5KxHV7Mpr4YAk5Er52d493UMA5lxoRS4RisPDem5iD64OsdG+fd543IgYUKfh+nFsfnZsonOXwwGAzGhKpfNbe6YOQCmX66ub3jOvydRuAE2/1ddP/3B7ssbnFbkdmSMtQc6x2obZWPlMeXQSudIZQxk9P7BlBhaNU1triUfo+NDCTAbeeB7U3j00un86MTR3LlkLD87LYeZzg7gdQeqhvJ0xQghxTEhhBCiHzzeVtmTiBSVcRIa1+shO1xh/B0htPrGynIi1Q0NHhbHnGOcu7QMzpqSxKs3zyc9OtjV7bFsejLxYYHefx1DTDZWHkUGYqzSQw6H1hHGP7r3vDHhvT43Vur00cq9y7sv0KgvhTdvhNyPvHtxTYOPfq6uT7kE0mb3eFhpXQu7S+pd/9wSoBfHJJT/mCIjlUcFvWssOSLQtVXYYDBw9tRkfnJqDrcvGcMtJ4zmolmpAKw7WDlk5ypGDimOCSGEEP2g53/VNLW7z9vx0Q5n59ik5K7h0kkRgZTrnWMejlXanZsqdzvSmZIWSWpUMK/eNJ8x8aFYzUZuPP7oDCLO6pw7Jhsrh6+mKmhyvoEZgtDrPWX1VDW2EWQxMTU1ctBffyTr2FjZR3EsZhRkLQY02PhCx+32dnjt+7D1FfjwZ96F5G99FQq+U5lzS+7t9bDOXWMATRbnCG6DFMeOGXYb7H5fXZ+wbEhPRbi3t1QVx8YkuF+KMjdLFbm35NcOyO9g4tgi5XIhhBCiH8IDzQRZTDS32ympbSEzNsTn59I0jRV7yimobqapzUZTm53tzs6xSSlHFseCyPVyrLKtcCtBQJE127XxMjEikA9uW0RDq43okACfz30oZcWGsEFzjsnJxsrhq0rl2hGWDNbQQX/5tftUYW5WZhQBZvl82J9i9bHKvjrHAGZerbb1bnoBFv9cde98/lvI+1rdX5sHRRshZWbfz9XaAJ/9Rl0//scQntTroSv2dC2C1ZmjSALpHDuWHFqlCvTBMZC5aKjP5pizs6iOioZWjh/be7e8bm+Z6vIcE+/+/xUZMcHEh1kpq29lU14N891sIRaiL1IcE0IIIfpBH3E8UNFISV3/imOr91Vw9bPfdbs91GpmVFzX502MCGS1XhxrqgCHHYxuAsbbW7DWquKEOXkKBoPBdVeA2Ui0+egsjAFkxoTwjt45JmOVw9dA5Y15aK0zb2zBKMkb8zePO8egezC/5oC1j6r7orNVEXXH254Vx9Y+qp4nKhPm/bDXw2x2B6v2qiLY2IRQ9pQ2UGPQA/klc+yYseMtdSkjlYNue2EtF/xzLa02B5/dtZhRce6LXh2dY+6PMxgMzM2O4b0tRXx7sEqKY6Jf5GMzIYQQop8S9Nwxb0P5j7AprwZQn4SePz2Fy+emc8OiLJ66ciZmU9f/ZSdFBFJJOA6M6s1lX90P5bsxanaqtVAyMoemODFQMmODXZljDhmrHL4q9qrLIcgbszs0VybNAnnz5HcdxTE3gfy6zsH8K/8Cb9+irs//EZzs7ALb+bZno5Xb31CXJ/4KLL3nJW7Or6GuxUZEkIXFzq6VCk22VR5TZKRyyJTXt3LD8+tpaXegabB2X9/fc3rn2Oh492OVAHOzogHJHRP9JyVzIYQQop+SnCOKxf0sju0pVb8MXjI7nR+c4L6AlRgeiAMjNcZIoh1VUF8CYYm9P8C5qXKXM29sJAkOMNMWmgbtYGyqUKNWQzC2J/owhGH8O4pqqW+xEWY1d1luIfwj1tNAft2M78OaR9T4JED6fJUXZm8HS7DKDizaBCkzen+OumKo3AsGI4w5xe3LfeXMG1s0JpboEOe5Opx/Dxqkc+yYICOVQ6LVZucH/91AcW0LBoOqea87WMWV8zN7fUxtczuldepnSV+dY9BRHNuYV02bzSFj88Jn8jdHCCGE6KcEZ3Gs1JeNlZ3oYwRjPfhlMCkiCIAyh7P7oY83eK2FWwHYraWPyDDyuLh4arVg9Q/DrXusdAc8vgA+vluF0h+rKveryyEojukjlXOzo7t1YYr+c2WOeTJWCZ2C+YGQOLjgWTBZICAYxixVt+982/1zHFqlLpOmQlCk20P1vLETcuKJCrYAUGxzdqRI5tixQf/7NO4sGakcJJqm8Zt3drD+cDVhgWb+sGwyAN8erEJz0xm6z7mpMjE8kPBAS5+vMzo+lOiQAFraHWwrrPHLuYtjk/x2IIQQQvRTR+dYs8/P0W53cKBCL471PUaQFKles8Du3LhWtMnt8c35WwAoDR5D1FEavO9O5nDeWLnzXSjbAd88Dn+fpnKSbB4WEUYKhwOqnMWx2DGD/vJf7FbFY8kbGxgZMSoT8VBlo+fdY0vuhewT4eIXuwbpT1ymLne87X608uAKddlHF1B5fSvbCtVik+PHxhIZrH7+FbQ5cxylODbyaRrs+VRdn3DO0J7LMeT5rw/z8nf5GA3w6KXTOX9GCgEmI2X1rRyqbOr1cXudXfSedI2Byh2bk6m6x745cAx/ACX6TYpjQgghRD+5MsfqfC94HK5spN2uEWQxkRIZ1Ofx0cEBBJiMvGufr2749klo6+WXTU3DWrkTAGPiRJ/PcTjLjg1x5Y4Nu42V1QfVpSUYWmrh01/BY7Ph0JqhPa/BVF8M7U1gNENk+qC+dHl9K98dUm+Ylk5MGNTXPlakRAYxLS0ShwbvbSny8EEz4Kq3IX1u19vHLAVzkPo+Lt7c++MPrlSXegdaL/Qg/onJ4cSHBRLp7Bw73OZ8493W0PvPTjEyVOyB+iIwWSHjuKE+mxFP0zSeWrmf376n4hx+cfo4TsiJJ9BiYmqa6nb/1k0+2F5n59joPjZVdjY3O9r5vANfHMuvauKnr21h7X7JKxxppDgmhBBC9JPeOVbSj86xPZ02MxmNhj6OBqPRQEKElQ8c82gJS1dZKpte6Png+hKCbLXYNQPx2VN9PsfhLDM2hAK9ODbcNlZWH1KX5z4G5/4DQhPVG/+3bvIsdHwk0PPGojLV+NwgWr6zFE2DKakRpEYFD+prH0vOm54CwNubC/v3RAEhHRliO97u+ZjqQ6pD1GiG9Hlun07PG9OD+KOcnWNFTWZVLAHpHhvp9n+pLjPmg6XvD5+E71ra7fz41S388cPdODS4an4GNyzKdt0/N0stRFnnpoilF8c86aI/8nnXH6rCZnf4cuoeKa9v5Yqn1/HahgJufH4DhyoaB+y1xOCT4pgQQgjRT4nO4lh5favPv5TpYfxjPNjMpEsKD8KOiT2jrlE3rPk72HrYFucM4z+gJTMxw01o/1Esq1PnmFZzaGhP5khVzs6x6FEw/Qr44TowmKA2H2oLBu88ynNh7WMd2V+DaQjD+D/aXgzAqRNH5t/94eKsKUmYjAa2FtS6MoN8po9W9ra1Uu8aS5nldvmG3aG5OsdOyFFj13rnWE1zO1qos6AuGytHtgPO4lj2iUN7HiNcaV0LFz/1DW9uKsRkNPDbcyby23MmYjB0fOA3R98s6Wb80TVW6UXnWE5iGOGBZhrb7OwoqvPxK3CvodXGNc99y2HnSGhDq41bX9pEm23ginFicElxTAghhOin2BArZqMBhwblngZSH8GbMH6dXpRbH3E6hCZAXQFse63bcfWHVR7Zbi2NSSkjc1NfenQwhag3v7bKYdQ51toAjc5lCdFZ6jIoEpKmqOv56wbvXN64Hj69Bx6dAS+cB7s/ALttcF57iML4a5va+doZxn/6JCmODaSYUKurO+ud/naPjTkVzIGqQ6xka/f7XSOVx7t9mu2FtVQ3tRMWaGZGeiQAEUGqOObQwB7kzKBrlI2VI5a9HQ6tVtezTxjSUxnJSutaOOex1WzJryEiyMLz187h+wsyuxTGAGZmRGEyGiisaaaguvs4c31Lu2vztzcfFpqMBlfhbSBGK1ttdm56YT3bC+uICQngfzfMJTLYwrbCWh78eLffX08MDSmOCSGEEP1kNBpcuWP6L3Xe0jvHvBkj0EP58xscMO8WdeOah1X4eSf1eZsBqAgZS3DAyNzSFWA20h6WBoBhOGWO6SOVQdEQGNFxe5ozZ2mwimNVB5xFBoP6s/8LePkyeGQq5A3CObg6x0YN/Gt18vnuUmwOjbEJoWTHeV54Fr5Z5hytfGtTodttdH2yhvY+WqlpnYpj7sP4tzqD+GekR7m2lAZaTARZTAC0BerFMRmrHLEK1qtcueAYSJwy1GczYr36XT6lda1kxYbw7o+O47jRPS8/CbGamZSi/l+oZ0F2pnedxodZiQj2bgS/Y2Sz9zwzXzgcGj9+dQtr9lUSHGDi2Wtms2BULH+5QMVU/Hv1Qb7YXerX1xRDQ4pjQgghhB8khKvsmlIfimPtdgcHnbkVnm5nAkjSFwHUtsCsa1XxpWIP7H6/y3GW8l3OkxyZYfw6a2wmAOb2emiuHtqT0elh/FGZXW9Pm6MuB6s4tvNddZl1PNy2CY67XRXs6grg9WvUooCBNERjlR9tLwHgtElJfRwp/OGU8QmEWs0UVDez4XA/vwcnLFOX21/vOi5esQcaSlVeWOoct0+RW6LGq8Yldf3QIcr5prvF4tz22yCdYyOWPlKZtRiM8tZ3oHy2SxWHbjo+27W9tjdz3YxW6nlj3vwupOvcOeZw+C/P85k1B3l/azEWk4Enr5zJlNRIAJZMSOCa4zIB+MlrW9XvYuKoJj8hhBBCCD9IilAhv750jh2qaMTm0AgJ8GxTpS6x82sGhsOcG9Udqx/qyOmxtRLVfAiAiKzpXp/b0SQlPpZyzTk2WpM3tCejc+WNZXW9Pc0ZIl6yXY1eDrRd76nLCeeocznlPrhjK0RlQV0hfPQL/7xO4QZ4bA7s/rDjNnt7RwfdIBbHGlttrNyjOoJOk7yxQREUYOI05/jqW5v6OVo59jQIiVPfy6sf6rhd7xpLnwuWQLdPkVuiOnLHJXYtjkU6Q/nrzerNtGSOjWB6GP8oyRsbKKV1LWwpUB+wnDQ+vs/j57oZf9zrQ/6qbmJyOCEBJupabOx2fu/7w8fOD1l+fto4Fo2J63LfL04fx8TkcKoa2/jlW9v89ppiaEhxTAghhPADfayytM774pi+qXJ0Qli3fA53OrZkOl9z7s1gDoKiTfD8ufDvU9D+MQczdmq0EMaOzvH63I4mWbEhFGjOX8yHy8ZKV+fYEcWxiBQITwXNrgpKA6m2EArXAwYYd1bH7dYwOO+f6vYt/1MZZP21+SWoyIUP7oJ25/bW6sPq67QEQ9jgdXB9lVtOq81BRkww45O8f6MlfKNvrXx/a3H/gqqtoXD6g+r6yr9AmbMD1sO8MU3TXG+QcxK6Zi3qofx1pkh1g2SOjUwttR0/XyWMf8B8vkt9/0xLiyQ+zH3BGmBWZjQGAxyoaKSsvuvvTHrn2Ggvwvh1ZpORmZmq8LZ6n39GpVttdtd49snjE7rdbzWb+LNzvHL13grsfuxYE4NPimNCCCGEH+iFKl86x1x5Y17+Mqi/Zll9i9qSGRILM69Wdx5cAQXfYnB27KzQppOTNDLD+HWpUUEUaM6ck+GSO9Zb5xiozhcY+NFKvWssbS6EHdFBlT4PjrtNXX/v9v530JQ7g4nri+G7f6vrnfPGvCj+9tfHO5wjlRMTvSo6i/6Zlx1DQriV2uZ2vsrtZ9Fp4nmQcwY42uGdHznD1Vep+7IWu31ocW0L9S02zEYDo+K7jnlFOTvHqnDmAErm2Mh0cJUqzMeMhsi0oT6bEUsfqTxlQvfiUU8igiyMT1S/jxzZPdaxnMi3DzROcXaufbCtxKfHH2l7YS1tNgcxIQFkxgT3eExOYhgBZiNtdkePSwbE0UOKY0IIIYQfJOhdXD50ju0t8z6MHyA2tIctmSf/Gs74C5z9CLuOf5yruY+TW//MWxm/xmIa2f/bjw8LJF/vHBsuY5X6OOGRnWMweKH8u5x5YxPO6fn+E34JceNVgeD9OztGcn1RtrPj+uq/QWv9kOSNtbTb+cL5hu002VI5qExGA+dOU91jb/d3a6XBAGf+Fazhqvvx3dtUnmBAKCS7HxPXRyqz40Kwmk1d7tODviv0MewGKY6NSHremGypHDBNbTZW71MfqizpobOqNz1tlmxstVFYozqOx/jQOQYqX9JogC35NeRX9b9Qtf6Qyk6cmRHV64csJqOB7FhVgD9Q3tjv1xRDx6ffkh9//HGysrIIDAxk5syZrFq1yu3xL774IlOnTiU4OJikpCSuueYaKis7tkg899xzGAyGbn9aWiTUTgghxNFB7+LKLaln3QHvNiXpY5XeBtB23pJZVOP8f2ZACNrs63muZTFnfRbFVy2jCU+byIMXTfPquY9G8eFW8jWVB6INh7FKuw1q89X1njrHXMWx77ptGPWbhjI4vFZdH392z8dYAuH8J8FoVoW0ba/5+Frl0FQJGFQxsKkSvnliSIpja/ZV0NhmJzE8kKnO8GQxeJY5i2Of7SqjrqW9f08WngxLf6eub/mfukyfDyb3m+xcI5WJ3Ttm9UD+Uod0jo1oet6YjFQOmFV7K2izOUiLDmKsF7/D9JQ7pm+qjA0NICokwKfziQuzMi9bba38YFuxT8/R2XfO4ths57hmb7LjVHFsf/kgZIiKAeN1ceyVV17hjjvu4J577mHTpk0sWrSI008/nby8nj+hXb16NVdddRXXXXcdO3bs4LXXXuO7777j+uuv73JceHg4xcXFXf4EBvY9syyEEEIMBxOTw0mJDKK2uZ2Ln/qGW1/aRHFtc5+Pa7M5OOTcVOnLGIFelLvjlU3c/MIG/vJJLj9+dQv3vrcTu0Pj/BkpvHTDPI9yQI52MSEBFDqLY/aqQ0N7MqAKYw4bmAMhtIfupYRJKoertbZjHNHfdr8PaKrLJjK99+OSpsLin6vrH98NzTXev5beNRadBSf9Sl1f+6gz74xBLY597NpSmYjRKCOVg218Uhij4kJoszlcSxH6Zcb3IXNRxz/3kTcGsFvfVJnY/eeqPlZZ1O4ct2yqVMVsAFsr1Bb073zF0KvJg6r9YDBB1qK+jxc++dzZobtkfIJX4+t659juknre21LEc2sO8sRX+wHf8sY6O3OKyrb8YGv/imOaprExz9k5lhnl9tjsWHXOByqkc+xo5nVx7KGHHuK6667j+uuvZ/z48Tz88MOkpaXxxBNP9Hj8N998Q2ZmJrfddhtZWVksXLiQm266ifXr13c5zmAwkJiY2OWPEEIIcbQIDjDz3q0LuWxuOgYDvLeliJP+soKnVx90+7iDzk2VYVazq9DljePHqmJQflUzH+8o4bEv9/HmpkIMBrjnjPH89cKpBFpMfTzLyGA2GakPUh0rxtq8/o0H+oMrjD8TjD38ymUyQ8pMdX2gRiv1vLHeusY6W3gnxI6FpgpY8aD3r6UHpsdPgInnq+Jfax2UODd4DVJxTNM0Vu1VYz6eZuAI/zIYDK4RKz2su59PCGc/ogrN4NHmwVxXGH/34lhEkOocK2gNBgyABiv/DC+cD3/KgL9NhE3/7f95i6Fz4Ct1mTITAiOG9FRGKrtDc31/n+LFSCVATKjVVQS79aVN3PveTldO5OSU/v33Om1iIiajgW2Fta4PH905UN7AvrLu2y0PVDRS1diG1WxkUrL7c3J1jpVJ59jRzKviWFtbGxs2bGDp0qVdbl+6dClr167t8TELFiygoKCADz/8EE3TKC0t5fXXX+fMM8/sclxDQwMZGRmkpqZy1llnsWnTJrfn0traSl1dXZc/QgghxFCKDgngj+dN5r0fLWRWRhTN7XZ+9/5ONhzuvq5cp4fxj04I9Sk0/LaTx7DmFyfxn2vn8OuzJnDpnHROm5jIc9fM4Ybjs4+5IHJ7WCoOzYDR1tz/cPn+qupUHOtN+jx16W1xrKEMXrnS/YbJ5uqOzX7jz+37OU0WOO1+df3bJ6E817tzKncWx+LGqWLgifd0vT8627vn81FRbQsldS2YjQZmpLv/tF8MnJPGqfy/L3PL1MKQ/ooZBd9/Dy78DyROdntou93hGm/KcdM5Vt1sh2A1gsWKP8H+z8Hm7Pj9+G6oK+r/eYuhoY9UelBIFb7ZnF9DZWMbYYFmZme5HzvsyQ2LskiNCmJqagSnTUzk2uOyuPfsCfzopDH9Oq+YUCsLRnk2WnmgvIEz/r6Kcx5bQ4We3eq0wTlSOTU1kgCz+7LJqDjpHBsJvCqOVVRUYLfbSUjoWhlOSEigpKTnjRALFizgxRdf5OKLLyYgIIDExEQiIyN59NFHXceMGzeO5557jnfffZeXXnqJwMBAjjvuOPbu3dvrudx///1ERES4/qSlyQYSIYQQw8OklAheu3k+501XXUzuusf2ujZV+raZCSAlMojFY+O4bmEW958/mX9eOZPFzo6yY01keCglOAsiQ72x0tU51kPemM7XUP5vHlf5YK9+v6MAdqTcj9RYZ/wEiPWwa2v0Ehh7unrcx7/wrvvO1Tk2Xl3mnN7RGRccA8Hev3nyxYbD6g3NxORwggKOja7J4WhmRhQRQRZqmtrZlF/jnydNmwMTl/V52IHyRtrtGqFWM6lRQd3uj3RmjtU0t8Ok76nOotGnwKl/hJvXQMos1fX4/l1D34EqvKdpHT8Xj7G8MYdD47tDVXy52w8dm33Qt1SemBPv08Kfi2ens/rnJ/HOjxbyzytn8n9nT+Dq47JcnZ39ceZkNVr5vpvRSodD4+dvbKWl3UFTm53X1ncdp/7ukPpgc1YfI5XQ0TlWXt9KfX9zFsWQ8SmQ/8hPoTVN6/WT6Z07d3Lbbbfxf//3f2zYsIGPP/6YgwcPcvPNN7uOmTdvHldccQVTp05l0aJFvPrqq4wdO7ZLAe1Id999N7W1ta4/+fn5vnwpQgghxIAwGAzcvHgUoPKPetua5GsYv+hZfJiVAmfumGtT5FDRO8d6CuPXpc52Hnug68Y8e3vvG/Q0Dba9oa472uHlK6Bsd/djtr+pro/vZUtlb079A5gCYP8XqsDmCU3rXhwzGOCU+1TQf8Zx3p1DP2x0FsdmZEjX2FAym4ycmKO+F/U30YNFzxsb20tHbqTeOdbYBmc8CL/Igyteh/k/hMRJcM6jYLTAno9gx5uDeu7CDxor1Hg4hj63mo4EDofGhsPV/Pa9Hcz/0+dc+M+vuea573h3i/86H19bn8+VT6/j5W/zaG6zA/DZTmfe2DAcXz91YiJmo4FdxXW9huT/d91hV+A+wEvf5uFwdBTD9Q9aPCmOhQVaiAuzArKx8mjmVXEsNjYWk8nUrUusrKysWzeZ7v777+e4447jpz/9KVOmTOHUU0/l8ccf55lnnqG4uOdKrtFoZPbs2W47x6xWK+Hh4V3+CCGEEMNJTmIYi8bE4tDgubWHejxmjzPnwpcwftFdfFjHxkpqel4WNGj04py7zrGgSIhzFpP07rGC9fDoDHh4EhRu6P6Y/G+hNg8CQiF1jgr0/9+FatQSoGIfPH8u7Fuu/nmCl8WxmFGqSADwyS9VQHlf6opUp43RDDGdRmIyF8LtW+D8p7w7h37Q39DMlOLYkDvZn7ljXsh1s6kSOrZV1rXYeh75TJgAx/9EXf/wZ9DU+2g8Dge8dzu8e+vAbZ0V3ql0voeMTFPbeEcwTdO4+Kmv+d4Ta3l2zSFK61oxO5eQPPjxblpt9n6/xv7yBu55azur9lbwize3Mf9Pn/Ort7ext6wBs9EwLDvVo0ICOG50LNBzMH9BdRMPfKQ+VPr5aeMICzSTV9XEmv0qjqGyodU1IunpeH52rGysPNp5VRwLCAhg5syZLF++vMvty5cvZ8GCBT0+pqmpCeMRIbQmk2px13ppU9Y0jc2bN5OUlOTN6QkhhBDDznULVWHkle/yu7Xat9rsHK5UHWVSHPOPuDArBZrKOhrSsUpN86xzDCDdOVqZ9zWsfhieOVUV9mwt6p+PtP11dTnuTLj0ZZXlVZMH/7sYvnoAnlgAB1eo8PJT74eEid6f/6Ifqw2b1Qfh63/0fbzeNRYzGswBXe+LSAVL99G2gdDUZmNnseoakuLY0Dt+bBxmo4F9ZQ0crhy8bgq9ONbTpkqgy9hWXYut5ydZeJcqXDdVqPyx3mx7DTY8Bxufh/xvfD1l4U+V+9TlIG7IHSo1Te2u7qfzpqfw76tmseFXp6gu6upm/vtN/z4k0jSNX7+9nTa7g/FJ4aRGBVHT1O563rnZ0X4ZgxwIZ03RRyu7dtBpmsbdb26jsc3OrIwobjo+m/OdMRgvOr8u/UOWsQmhrk7TvmTruWPSOXbU8nqs8q677uLf//43zzzzDLt27eLOO+8kLy/PNSZ59913c9VVV7mOP/vss3nzzTd54oknOHDgAGvWrOG2225jzpw5JCcnA/Db3/6WTz75hAMHDrB582auu+46Nm/e3GX0UgghhDgaLR4bx+j4UBpabbzyXdcIgAPljdgdGmGBZhLCrUN0hiNLfFggBZr6tJjqISyONVZAeyNggMh098fquWPfPAGf/UblfY1eom7b/X7X8VC7DXa8pa5PvhBCYuDy1yEoGoo2wld/BHsrjDoJbvka5t/i2/lbw+CU36rrq/7a+4inrmynuowb59vr+cmW/FrsDo3kiECSIganICd6FxFkYXamypr7bBC7x3b3URwzm4yEBZoBqG5q6/lJzAFw7mOAAba+DHs+6X5MWyN8dm/HP299tR9nLfzGVRzrX7D70UAPkY8IsvC3i6exZEICEcEW7jplLACPfrGX2mbfM7De2VzE2v2VWM1G/nnFDFb89ESeunImx42OIcBk5Kr5mf74MgbE0gmJWEwG9pQ2uJYfAby+oYBVeysIMBt54IIpGI0GLpubAcDyXaWU1bWw3tWB7HlW5ihn7tiBCukcO1p5XRy7+OKLefjhh7nvvvuYNm0aK1eu5MMPPyQjQ/2FKi4uJi+vo0J99dVX89BDD/HYY48xadIkLrzwQnJycnjzzY75/ZqaGm688UbGjx/P0qVLKSwsZOXKlcyZM8cPX6IQQggxdAwGA9cepzqHnlt7qMsIj/7L2tiEsGNuq+RAiQ+3ku/qHBvCsUo9jD8iFcx9FD714phmB3MQnP13VfAadRJoDljXaSTx0EpoLFfFsOwT1G0xo+DSl8ASAiHx8L2n4Yo3+78dcvJFKq+nrQFW/tn9seXOzLP4Cf17zX7amCd5Y8PNyePV9+Png5Q7VtfSTmGN2jg5rpexSugUyt/kpnCQOgvmOQvMb93cPcdw7aNQX6S+90AVrm29FNvE4Kk4djrHyp3FMT3vSnfBzFTGxIdS09TOE1/t73JfQXUTv3t/Jx9uK+6SsXWk2qZ2fv+B+uDj1pNGkxETgsloYOnERF68fh57/nA6p05M9PNX5D8RwRaOH6NGPs/8+yom/eYTpv72U3751jYA7lwy1rVlMicxjJkZUdgdGq+uz2e9Hsbvxf9LRknn2FHPp0D+W265hUOHDtHa2sqGDRs4/vjjXfc999xzfPXVV12Ov/XWW9mxYwdNTU0UFRXx3//+l5SUFNf9f/vb3zh8+DCtra2UlZXxySefMH/+fN++IiGEEGKYOX9GClHBFgqqm/l0ZykOh8bbmwr58ye5gGrbF/4RF2ol36F+GdZq84cuA0gfqYzK7PvY6GzIOUPlh934Jcz8vgqzn+fM/dr4PLSoUUG2OUcqJy4DU6dRlvR5cNcOuHM7TL5APb6/jEZY4uweW/9Mx9fUE71zTA/jHyL6GxoZqRw+ljhzx749WEXdIGxx2+PsGksMDyQiuPdxryjnqFRNb51jupP/D5JnQHOVWn7R5nzjW1vYMfZ8zt8hLAlaajqy/sTQcXWOjRra8xgE5fWqOBYb2nX0z2wy8ovTVSfvM2sOUljTjKZpvLY+n9MeXsXTqw9yy4sbOePvq/h4e89Fsgc/2U1FQxuj4kK44fh+ftgyRC6fl47RAO12jYZWG7XN7bTbNaalRXLDoq6RB5fNUV3e/1uXx/ZC9f9cT8L4ddmuzjE1FSCOPuahPgEhhBBipAu0mLhiXgaPfrGPvy3fw98/3+sa+4kNtbo6y0T/xYdbKSEam2bEbG+D+mKISOn7gf5W7UVxzGBQnV9HGn0yxOZARS5s+i/MuhZ2vafum3RB9+ODBqAglL1YdbDt/wK+/AN879/dj3E4oFwVegerOLZqbznp0cFkxIR0Og2NjXk1gBTHhpPM2BBGxYWwv7yRlXvKOWtK8oC+3m5XGL/7HEc9J6naXecYqED3i/8LTy2G0m3wzo/ggmfg8/vA1gxp82DS96BoE3z9GGx9ReUBiqHhsKvtvwCxx8JYpSruxoZ271A+aVw8c7OiWXewit+/vxOHpvHJDtXBOT4pnIKqJnaX1HPzfzcyISmc82ekkBoVREpkMDXNbfzvW9V9/ftlk7GaTYP3RfnRSeMS2PTrpdS3tmOza9gcDuwOyIgJxmzq2id05pQk7nt/J0W1LYDqxkuPDvb4tVKjggkwGWmzOSiqaSbNi8eK4cGnzjEhhBBCeOfK+RkEmIzsLWtgd0k9YYFmfnpqDit/dgJjJIzfb4IDzARZrRRpMeqGoRqt9DSM3x2DAeb9QF1f9wTs+VhthAxPgfRB7LBfcq+63PYaFG/pfn/NYWhvApPV/WZOP/lydxlXPv0tlzz1DS3tHZvYDlQ0UNvcTqDFyPgk2WI+nAzm1sq+wvh1HneOgSqwX/S82sa64014+xaVQwZw2h/V9+qUi50n8DG01Pp8/qKfag6Do139PApPHeqzGXAVvYxVgop1uPsM9YHFR9tL+GRHKRaTgZ+dlsP7ty5k1c9P5NaTRhNqNbOzuI7ff7CLm/+7kbMfW82VT3+LpqnO9/mjYgb1a/K3iGALqVHBZMaGMDo+jJzEMAIt3Yt9gRYT58/o+DBtVkaUV5EXJqOBzFhVEJONlUcnKY4JIYQQgyA+LJCbFmcTGWzh5sWjWPWzE/nhiaMJDpAmbn+LC+ucOzZEofyuzrF+FoumXqLyxWry4ONfqNsmna9GHgdL0lQV/g9dw8d1+qbK2LFg6v/f59rmdj7eXtLjWIqmafztsz0AFNe28L91HcVPfbvY1NRILCb5FXc4OXmc+n78MresS+7iQMj1sHMsypPMsc4yFsDpD6jrW/6nLqdeCikz1fXEyWohhb0Vdr7r9XkLP6l05mvFjBrcn5NDpGOssudsy2lpkZw7TXVr5iSE8fYPj+OWE0ZjMhqIDA7gx0tzWPWzE/npqTmcOSWJaWmRrkJbYngg95wxtKPyg+3yuR0LdHzpQM6Oldyxo5n8Ri6EEEIMkh8vzeHHS3OG+jRGvLgwK/k1KndsyDZW+qNzDMASpMYpV/1FjYhCzyOVA+3Ee2DH22q88sBXHcsAAMqdxTE/jFRqmsYPX9zI6n0V3LAoi3vO7Brw/8XuMrYWdHTlPP7Vfi6Zk0ZwgNlVHJORyuFnZkYUEUEWapraWbO/ksVj4wbkdTRNY3eJygrqc6xS7xxr9iJAf9Z1qnty4/NgCVZ5ZDqDAaZcpMYtt74CM670+vyFH1QeO2H80KlzrJfiGMCfL5jKRbPSmJkR1WPHVFRIAD88seu/r5Z2O2ajodvo4Ug3Oj6MUyYksHJPuavj1Rt67ph0jh2djq2/7UIIIYQY8eLCrBRozjffQzFW2doAjc7xMX+MGc65AYzOYPGY0aqTa7BFZ6kiHcDy33RddFDmv+LYJztKWL2vAoCnVx90bZ8EVfh4+LO9AFy/MIv06GAqGlp5/mtVAJXi2PBlNhlZ5uxeue+9HbTa7H08wjcldS3UtdgwGQ2Mjne/6ETvHOszc6wzgwHO+Ass/gVc+ByEH5GfpndYHlqtAvvF4KtQPyOOteJYbFhAr8cEmI0cNzq2x8JYbwItpmOuMKb7x2Uz+PaeJWTFhvR98BGyZWPlUe3Y/BsvhBBCiBErPsxKvqs4NgSdY/prBkVBUGT/ny8ssSPPaOql/tlE6YvjfwoBoVC8WXWy6fxUHGtpt/O799VzxYZacWjw09e2uHLFvthdxrbCWoIDTPzghFHcfrIK235yxX7yq5rY73wzMj1dimPD0V2n5BAbamV/eSNPfLV/QF5jd7EaqcyKDekzQDzSWRyr9aY4BmC2wol3w9hTe3jSdEhfAGiw/XXvnlf4xzHWOaaPVcaFBg7xmYwcAWaja2GHt0a5NlZK59jRSIpjQgghhBhR4sMCOzLHhmKssqr3vLH6lnaeXn2Qcx9bzd8/3+v5c575F7jkf3DcHf45R1+ExsHpD6rrX/4R9i4Huw0qVAZYf4tjT644QGFNM8kRgbx363HEhalCyiOf7+3SNXbV/ExiQq0sm55CdlwI1U3t3PnKZkCNtESH9N5BIYZORLCF35ytxmQf/3I/+8r8/+bxu0NVQN8jlQCRzrHKak8C+b0x5SJ1ufU1/z6v8IyeOXYMbKp0ODQq9W2VbjrHxODRO8dK61ppaLUN8dkIb0lxTAghhBAjSpexyroCsHvZGdJfrjD+TNdNBysauffdHcz74+f87v2dbCmo5bEv9lHX4uG5WYJg3Jl+Cbzvl+mXO8crNXjjOpVBZm9T+UsR6X0+vDcF1U08/pXq+PjlmeNJigji98smAaoz7OHP9rq6xm5YpIqOJqOBO5eMBWC9PlIpXWPD2llTkjgxJ442u4NfvrkNRw9LF3xVWtfCs2sOAXD6pMQ+j+/YVunnnw8TzlVj0KXbOroqxeBoa1Q/8+GY6ByraW7H5vweignpPXNMDJ6IIAuxoepny0EZrTzqSHFMCCGEECNKfJiVciJowwKaA+rcZP8UbYaPfwltTf47Ab2TyhnGv2ZfBSf/9SueW3uIxjY7o+NDSQwPpM3u4ItdZf573cFy2p8gZRa01MJrV6vb4sb1azPc/R/uptXmYG5WNGdOTgLg1ImJnD01GYcGj3zetWtMd+bkJMZ16hKSvLHhzWAwcN+5kwiymPj2UBWvrs/323M/+HEuze12ZqRHuv4OuRMZpG+r9HPnWHA0ZC5U1w+t9u9zC/eqDqjLoCj132GE0/PGIoMtBJjlbf1woW+slFD+o498FwkhhBBiRIkPt6JhpJA+NlZqGrx9C3zzD9j6sn9evCYftr6qrqfNBVRxzKHB+KRwXrhuDsvvPJ4LZ6UC8NH2Yv+87mAyW+Gi5yEkDtqdn4zHT3D/GDfW7Kvgg23FGA1w7zkTMXTKVPvtOROJcY5JBgeYuPH47C6PNRoN3OHsHgMpjh0N0qKD+fFS9d/sjx/ucmUm9ce2glre2Kg6hn591oQuf4d6o3eONbbZabM5+jjaS6mz1WXhBv8+r3DPlTc28kcqASqc3zuxbjZVisGnb6w8IMWxo44Ux4QQQggxougr7fPsseqG3kL5S7ZC2Q51vTzXPy/+2W/A1gIZC2HMUnUeVaor7fzpKSwaE4fBYOA059jXV7nlNB6NuSQRKXDBs2Bwhp7Hj/P4oc1tdr7KLeMPH+zkjEdWcfm/1wFwxbwMxieFdzk2OiSAP31vCgFmI7efPKbHPLFTJyZw2dx0Lp2T1ueGQjE8XL0gk0kp4dS12PjJa1v6NV6paRq/e38nAMumJXu8kCEs0IzRWUOrafZz91jqLHVZsN6/zyvcqzjGwvj1TZWhkjc2nIxy5o7tr5CxyqPNEAdXCCGEEEL4V1RwAGajodPGyryeD9zSqVtM7zjoj8NrYfsbgAFOu9+1VTLfWRxLjwl2HTohKZyMmGAOVzbxVW45Z07pewxs2MlaBOf+Aza/CJO+59FD2mwOlj68gvyq5i63z8+O4a5Txvb4mFMmJJD7u9N67QYyGAz88bzJ3p27GFJmk5E/XzCVZf9Yw4o95Tz6xT5uX+Jbt8/H20v49lAVgRYjPzvN8yKt0WggIshCdVM7NU3txIf5cdtfykx1WbkXmqvVmJ8YeK7OsVFDex6DxLWp0p9/d0W/dXSOSXHsaCOdY0IIIYQYUYxGA3FhVvcbK+3tHeOPABVebI7sicMOH/1cXZ/5fUia4rpL7xxLj+4ojnXuHvvwaByt1E27FK5+H8KTPTr8UGUj+VXNBJiMXDI7jb9fOp31v1rCSzfOc20P7IknY3Li6DI+KZw/OIuaD3++h5V7yr1+jpZ2O3/8SIXe37gom+TIIK8er49WVjf6uXMsJLZjIUfhRv8+t+idXhwboE2Vf1u+hxueXz9sun0r9E2V0jk2rOgbKw9WNPh16Yg/rNhTztp9FUN9GsOWFMeEEEIIMeKo4pjeOdZDcWzf59BUAQHOMPeaPGhv8ezJD62BXe+BrdMb6s0vqjFNawSc9GvXzXUt7VQ7t+GldSqOAZwxSXWLfbm7jJZ2u2evfZTTP0kflxTGn743hXOmJktezjHsgpmpXDonDU2D21/eRGFNc98P6uSFrw+TX9VMQriVmxZ73y0UEewM5W8egI22Kc7RSskd6z+7Dcr3qJzI3mia6tSDARmrtNkdPPHVfpbvLOWfK/b7/fl9US6ZY8NSWlQQAWYjLe0OdhbXDfXpuJTUtnDtc99x+dPr/LoMZSSR4pgQQgghRpz4MCsFenGscr/qFOtsy//U5YwrVUELrWPTmTtNVfDCefDKFfC3ifDF76FsN3x+n7r/hJ+rrhGnvErVNRYTEkCotWuaxZTUCFIig2hqs7PCh66Zo9FBZwZLVmzIEJ+JGC5+c/ZEJqWEU93Uzg9f3OhVOL6+0OJHJ40hxOp9WozeOeb3jZUguWP+9NX98I/Z8OYNXT+U6KypUm3QxQDR2T0f0w8F1c202dXfzadWHvC6kDsQ9G2VcWFSHBtOzCYjp4xPAOA1H4tQzW12lv1jDT98cSOau6Kwk6ZpLN9Zyi/e2Or6/+yRVu4tx+7Q0DT4+RtbeX1DgU/nNpJJcUwIIYQQI05cmJW9WgotpjDVIfbF7zvubK6G3I/U9amXQqyzy8CT3LHS7WB3btdrLIOVf4bH50JjudqQNvuGLof3lDem6zxa+fH2Eu++wKPUwQq1vUuKY0IXaDHxxOUzCQ80szm/hoeW7/HocZqmsadU/X2amxXt02tHBjk7x5oGuHPMgze3ohcOB2x2fpix7TV48QJo6aEbR//5HZEGFu/Gaz2xv9PmwVabgwc+2u331/CWqzgmnWPDzsWz0wB4a1OhT53hK/eWszm/hg+2FbM5v6bX4zRN48vcMs79xxpueH49L3+Xz5+co+ZHWrVXjVMmRQSiafDT17fw1iYpkHUmxTEhhBBCjDhxYYE0E8gbqb9QN6x5GPZ+pq5vfxPsbRA/ERInd4zgVHqQO1aqtuIxZilc+JzaSqk77X4wd81+6SlvrLPTncWxz3aW0mob+aOV0jkmepIWHcz956ucvlfX52Oz9909VlDdTEOrDYvJ4PPfJz3nrnogimOJk8FoUcX53jbmir4VfAf1RWAJhoBQOLgCnj0D6o7IatRzIwcojF8vjo1LDMNggHe3FLHhcNWAvJanZKxy+Fo4OpaUyCDqWmx8ssP7D7++2FXmuv6/dT0vFcotqed7T6zlmme/Y2tBLQFmVdpZuaeiW0HO4dBYvVd1qD9yyXQum5uOpsGPX93CO5sLvT6/kUqKY0IIIYQYceKdYyZfGufB7OvVjW/dpN5Q6Vsqp12qNkrGOMObKzzoHCtzFscSp8DE8+CaD+CWdXDdchhzSrfD+yqOzUiPIj7MSn2rjbX7Kj3/Ao9SenEsOzZ0iM9EDDdLJyYQEWShqrGNDYer+zx+T2k9AKPiQrGYfHtLE6lnjnUaq6xtbuf9rUXY+xukbQlUBTKQ0cr+2Pm2uhx3Flz9AYTEQ+k2eHoplOd2HOfaVOn/vDGAfWWqOHbapEQumqm6gu57f9eQBa47HBqVzkUSMlY5/BiNBi6YmQrAy996N1rpcGh8kdtRHHtvaxG1R+Qi2uwOfvDiBjbm1WA1G7lhURZrf3ESyRGBNLfbWXNE6P6Oojqqm9oJs5qZnh7J78+dxCWz03BocPvLm7n1pU0crpTtmlIcE0IIIcSIo79ZKG9ohaV/gITJqoPjfxdBwbdgMMLki9TBsV50junFsYQJHbfFj4O0OT0erhfHjgzj1xmNHaOVHw3w1sp/fLmPBfd/zn+/OTwkb+hqm9td29UyY3v+9yGOXRaTkZPHqw2zn+wo7fP43SWqODY2Iczn14wK7jpWqWkaN7+wgR/9bxMvrvNDt1eqhPL3i8MBO99R1ycug+RpcN2nED0KavPgXyd3jMgPcHFsv3OZyKi4UH586lhCAkxsya/hnS0D23VTUtvCz17fwvbC2i631zS3uwq4MbKtcli6cFYqBgN8faDSq8LT9qJayutbCQkwMTo+lJZ2B29v6vr37I2NBRwobyQq2MLKn53IPWdOIDbUypIJKuvs0yN+hq50do3NHxWDxWTEaDTwx/Mmc/WCTAwGeG9LEUseWsG97+5wjesei6Q4JoQQQogRR+8cK69rUR0cFz4LlhC1URJg1MkQpn6J7Bir7KNzzOGAMmeWR/xEj85DL45l9FIcA1zFsU92lFI7EONdqDf9z609RFFtC796ezsX/HMtu0sGd4vWIWfXWFyYlbBAy6C+tjg6nDpRfS98urOkzxBqvXMsJ9H34ljHWKUq2n6yo5SvD1Q6r/shBzBFQvn7pXA91BWqrcKjTla3RWepTt2M46CtHl66BL56oGOsMtb/xTFN01ydY6PiQokPC+SWE9XrPPBRLvUtA/NzG+B3H+zk1fUF/P3zrh/e6COVkcEWnzsnxcBKjQpm4Wi1oMeb7ZCfO0cqF42J44q56YAardR/Jra023nkM/X34YcnjiYhPND12KUT1M/Qz3eXdul+XeUsji0aG+e6zWg0cO85E3n/1oUcPzaOdrv6PWHxg1/658OBo5B8JwkhhBBixIl3/rJY3tCqfqGMHQNn/rXjgGmXdlyPdmbUNFdDo5vRxto8aGtQOUIe5NrY7A4Kq9VGs54C+XVzMqPJiAmmtrmd217e1P9xrh7sL2+kvL4Vi8lASICJjXk1nPX31dz/0a5ByzqTvDHRl+PHxBFoMVJQ3czOYvfF21xn51hOPzrH9LHK2uZ2Wm12/vhhR5D1tweraGi1+fzcQEfnWPGW3rcsit7teFtd5pymPuTQhcTAVe90LED56o9Q4RyxHIDOsarGNmqb2zEYOn5+Xbcwi5TIIErqWjjt4VWsHICNw3tL6/lwm+ooPvL7QcL4jw6XzFbFrdc3FHiUpQjwpXOk8qTx8Zw3I5VAi5Hc0no25qlx8xfX5VFU20JieCBXzMvo8ti52dGEBZqpaGhjc746vrHV5hpVP35MLEeamBzB89fO4cXr5zI5JYLGNvsx+wGWFMeEEEIIMeLEOsdM2u1axya6aZfCiffAlEtUfo0uIFhtOAP3o5V611hcDpj6/sWxuLYFm0MjwGQkISyw1+PMJiOPXz6DQIuRFXvKefCTvregNbXZeODj3ewoqu3zWMDVDTMrI5rPfryY0yYmYnNoPLniQLeOhIFywJU3JsUx0bOgABPHj1GdDe5GK9vtDldAen86x6I6dY79Z+0h8qqaiA+zkhoVRLu9I8DaZ9HZEBSlNtyWbu/fcw2E+lLY9jrUD8NtuZ1HKiec2/1+kwXO/Auc8xiYnGOFJmvHz3I/0kcqUyKDCAowAWrL6uOXzyA1KojCmmaueuZbfvraFr92/z725T7XotOC6mbqOnWo6cUxCeMf3pZMiCcq2EJpXSsrPCigltW1sLVA/X/9xJx4IoIsnD0lGVBFsYZWG49/qbrcb18yhkCLqcvjLSYjJ+ao8fRPd6qfod8cqKTdrpEeHUxGTO///z1udCzv/PA4nrl6FmdNTvL+ix0BpDgmhBBCiBHHaja5ukLK6jvlZyz+GZz/JJiPeEOhdxtUuCkUle5Ql/ETej+mk3znSGVqdBBGo8HtsROTI/jzBVMBeHLFgT63R/1r5UGe+Go/P351S5/jZwDf7FfFsfmjYkiKCOKfV87kd8smAfDKdwW0e/iJdn9I55jwxFJ9tNLNWOPBikba7RohASZSIoN8fq2IIPUzoqqxjUc/V284f3pqDqc4c3u+2F3W62M9YjBAykx1fTjmjn38C3jjOnhoPLx4oerUsg2TvKHCDVBXoDZUjl7S+3EzroRrPoLYseoDEKOp92N9pBdiR8V1XSQyNS2ST+443pXb9NqGApb8bYVfRtb3lzfw3pYiAIKdBbndxfWu+12bKiWMf1izmk2cP0MF87/yXd+jlXrX2NS0SFd26mXO0coPthbzt+V7qGxsIys2xBX4fyT959dyZ3Fs1V4Vzr+oh66xIxmNBk4al9Dn7ywjlRTHhBBCCDEiuXLH6j14s+dJ7lhPYfxuHO5jU+WRzp6azM2L1bjmz9/Y2i2AWadpGm9uKgBUKPnm/Bq3z+twaK7OsfmjYly3XzI7jdjQACoaWlmR6/+RoCMdrFBvMKU4JtxZMj4ek9HA7pJ68iqbejxGH6kcmxjWrzdxUSEdHab1rTYmpYTzvRmpnDROdV58mVve/+UVwzl3rGSbutQcsPdTeO378NdxkPfN0J4XdGypHHsqWPoogKbOgh99B2c/MiCnsr+s5+IYQIjVzL3nTOS1m+aTHRtCeX0rT6040O/X/McX+3BosGR8AvOy1c/tXZ1GK8tlrPKocfFs1c34xe4yyupb3B6r542d7PwZBDAtLZLxSeG02hw8vfogAHeeMrbXrLkTcuKwmAwcKG9kX1mDK4x/0Zi4Ho8XHaQ4JoQQQogRSf/U1d0vo1WNbXy4rRiHJ8WxUmdxzMPOMU/C+I/001NzOCEnjpZ2Bze9sKHHEZ2NedUc7lQ0eOnbPLfPuaesnqrGNoIsJqamRrput5iMLJuWAngXFuwLTdM46BxNyo6T4pjoXWRwAHOzogEVzN8Tf+SNAYQEmLCYOopr/3fWRIxGA3OyogkOMFFe38qOon52Abk6x4ZZccxhhxpn6PYVb8LCuyAsCZqr4IvfD+25aVqnkcplQ3oq0KlzLL73n12zMqP5+enjgI5Nqr46VNHI287u4dtPHsP4JPX3vHNxrKJeZdjFhsmmyuFubEIYM9IjsTk0Hvui998xWm12Vu9TXV4ndSqOGQwGV/cYwISkcLdjj2GBFuaPUl1i/1l7iAPljZiMhi4fjomeSXFMCCGEECNSvDPnq8xN59idr2zmlhc3sqHB+Utjb2OVtraOPDIvi2NpXhTHTEYDj1wyncyYYAprmvn36u4dCG9sVG+axjmzlt7bUtwli+ZIXztHKmdlRhFg7vqr34WzOj7R9qjDzkfl9a00ttkxGrz79yGOTUudY0G9bYzM9cOmSlBvOiOCVHHhjMmJzHEW5axmk2vLXL9HK/XiWOU+tfRjuKgrAnsbGM2QtRiW/Aau/Vjdd3iN++UkA61wA9Tmqw3DY04ZuvNw0jPHeuoc60z/mbyvvMHj8PWe/ONL1TV20rh4JqdGMD4pHOi5c0wyx44OPzk1B4D/fnPYVdw/0roDVTS12UkItzIxObzLfcumJRPiHK/9yalj++yY1Ucr9a2T09IiXWPkondSHBNCCCHEiBTXx1hlcW2za9xgS7PzU9qqA6qj4kiVe8FhA2sERPSc83GkfC/HKnURQRZ+4exAeG7NIWqbOwpfLe123nfm0Pz6rAmMiQ+lud3OO5uLen2+r/d3H6nU5SSGMTVNfaL99ib3OWf9oYfxp0YFYzX7PxNIjCx67tj6w9Wu4PHOXJ1j/SyOgXoTmRwRyN2nj+9yu9658UVuP4tjITEQlaWuD6fcsWo1nkVkOpjM6npUJiROUWOWuR8M2amx4y116clI5QBrabeTX61+lvdVHEuLCiY4wESbzcGhykafXi+vsok3nT+Lbz1JdTTrxbHc0nrXNuOKehmrPJosGBXLaRMTcWhw3/s7eswK1QvxJ42Lx2DoWvwKC7Tw7DVzeOSSaa7AfXdOGa+KY/pUuCd5Y0KKY0IIIYQYoeJdY5U9F8fe2Vzk2gSW2xwO5kBwtHeMGnXmGqkcr0K2PaB3jqXHeN8ptXRCIjkJYdS32nhuzSHX7V/sLqOuxUZyRCDzs2O4dI4atfjfurwef9m2OzTWHawCYH52zyMVF81Sxb5X1+d7FO7vCwnjF95IjgxickoEmgaf7ey6tbKpzeb63urvWCXA/edPZvXPT+rW0Xiiszi2taCmxwKdV9LmqMvPfju0HVmdVR9Sl3rhTjfhHHW5891BPZ0u9i53nksPWyoH2aHKRjQNwgPNri3IvTEaDYxx/p3MLWnw6fX+teoAdofG8WPjmJ4eBUBmTAiBFiMt7Q7Xz1L972ScBPIfNX55xngCzEbW7Kt0heXrNE3j893qtpPGJfT4+DlZ0Zw7LaVb4awniRGBTEmNcP2z5I15RopjQgghhBiRXJljdd0zxzRN462NHZ1SBTWtEK3C8KnoIROkTN9UOb77fT2obW6nxpkX5m3nGKg3WT90dg08s+Yg9c6xyTc3qiD+ZdNTMBoNnD8jhQCzkV3Fda71753tKq6jtrmdUKuZySkR3e4HtQjAajayt6yhz3B/X0lxTHjr1InqDeKnR7yJ3FOqig6xoVZi/NQ109OIUkJ4IBOTw9E0+Kq/CysW/QRC4qBkKzx3BtT3volz0FQ5O8eijyiOjXcWxw58Bc01g3lGSksdVOxR1zMWDP7rH2F/mXOkMj7Uo6JEToLqLsv1YWNlu93Be1tVF/ANizr+u5iMBnISO0YrHQ6NykZn5ph0jh010mOCXf9d//DhLlptHV3qn+0qI7+qmQCzkeNG+ycbTO8eCws0MzW15///i66kOCaEEEKIEck1VtlD18fO4jpXbhFAYU0zxOqh/D3kjpXtUpcJEz16bX2kMjbUSnCA2Yuz7nDm5CSy40KobW7nhW8OU9HQ6nqTfv4MFaQfGRzAmc5g3p6C+fWRyjlZ0Zh72WwVHmjhDOdzvLq+wKdz7csBCeMXXjrVOVq5am85xbXNrtv1okNOovsRN39wba3sb+5Y3Fi4+kMIS4by3fDMaVDjfpHGgNPHKqMyu94elwOxOaqLdu+ng35alGwFNAhPhdC+x8cGmiuMv4+RSp1exPIllH/13gpqmtqJDbWyYFTXMbgJnUL5q5vaXOOVMX10s4nh5ZYTRhMfZuVwZRPPrjnEvrIGrnvuO254Xi3sOGV8gs+/MxzpglmpjI4P5fqF2b3+/190Jf+WhBBCCDEiJYSrQP6immaqnJ+y6/SusWlpkYDKH9Oi3Wys9HFTZXq073k5JqOBH52ozunfqw7yynf52BwaU1IjGB3fMU6mj1a+u6XI1WGm+/qAM2+sl5FK3YUz1Wjl+1uKaG7rIXOtnw5WqDeY0jkmPDUmIYx52dG02zX+veqg63Z9XC0nIby3h/qNPlq5ck857f0IWAdUgezajyAyQxWmnjldZRwOFb1z7MixSug0WvnO4J2PrmiTukyeNviv3QNvi2N6KH/nD1889a4zT/KsKUmYjuhm7BzKX9Gg/n8WFWzBIkWPo0qI1czPT1OZon9bvodTH17J57vLMBsNXL0gkz+eP9lvr5UUEcRndy3m9iVj/PacI518NwkhhBBiRMqKCSEnIYyWdge/fW+H63ab3cE7zjchNy8ehclooN2uURuSqQ44cmNlSx3UOrs8PByrzPMxjP9I50xNJiMmmKrGNv62XI0anT89pcsxszOjGBUXQlNb12B+m93Bt3reWB8r3Odlx5AaFUR9q42PdxT365yPZLM7XP8+pDgmvPGDE1Rx+KVv86h2FrhzSwevc2xqaiQxIQHUt9pYf8gPmyajMuGajyBmDNQVwKe/7v9z+qq6l7FK6Bit3Pc5tPkWLO8zvTiWMmNwX7cXHcUxz3526Usi8qqaaGqzefw6zW12PnVuZz17anK3+zuKY/WuJTMyUnl0Om96ClPTImm1ObA7NE6ZkMCndx7PvedMlI2SQ0yKY0IIIYQYkYxGAw9eMAWjQYXv6wG4a/ZXUl7fSlSwhZPGxZPo7DArtji3UB7ZOaaPVIYlQXC0R6/dEcbfv2KQ2WTklhNUFprNoWE2Grq9cTIYDK7usX98uY+1+yoA2F5UR0OrjYggi+uNVW+MRgMXzkwD4PmvD/s1mL+wppl2u0aA2UhyxNBunhNHl+PHxDIxOZymNjv/+foQ0KlzLHHgO8dMRgOLc1SQ9Re7S/s42kMRKXDeP9X1Q6vB0c+ONF80V0OLM6PwyLFKgMTJ6nZbc0c4vr80VcHfZ8CHP+35flfn2HT/vu4RtuTXcMETa7ni3+u469XNPPDxbp7/uut2YIdD65I55onYUCsxIQFoGuwt9TyU/4vdZTS22UmNCmJGemS3+/WOtJK6FvaW1bteSxx9jEYDj14ynavmZ/C/G+byr6tmke1hZ6IYWFIcE0IIIcSINTUtkhuOzwbgnre2UdvUzlvOUPuzpyYTYDaSEqUKNoc0lbtFfTG0dhqJcYXxezZSCZBX6Z/OMYDzpqeSEqnO8YSc+B5DyC+cmUZqVBDFtS1c5nyz9+E21QE2Nyu624hOTy6dk4bVbGRTXg0r91b0+7x1B/Qw/piQHoPPheiNwWDgB87i8HNrD5Ff1eTa0jfGw2JFfy2doEKt399ajMPhp6Jx0lQwB0FLTUf4/GDSRypDEyCghwK+wQDjz1bXd/l5a+XhNVC1H9Y/C61HFI+aqztGTZOm+fd1j/D814dZf7ia1fsqeHNjIU98tZ//e2cHF/3za9psqmBZUtdCc7sds9Hg1c9yvXss14vcsXe3qFH/s6cm9xj8HxZoIc05pr/K+fNZNlUevdJjgrnv3EndsuXE0JLimBBCCCFGtDuXjCU7NoSy+lZ++fY2PtmhOkDOn6E6xVKdhadDjQEQ7PxFtXJ/xxO4wvi9KI75aawSIMBs5HfLJpKTEMatzg2WR4oItvDh7Yu4an4GBgO8ubGQp1aqN5l9jVTq4sMDuXJeBgAPLd/jt+6xg+WyqVL47vRJSWTEBFPT1M5v31PZf+nRwYRY/RNa3ZcTcuIJCzRTXNvCOueYcr+ZLJAyU13P/8Y/z+mN3sL4Oxt/rrrc8wm0d9/46/trH1KXjnY4tKrrfcVbOs7Lwy5dX+0oUp1z1y/M4uenjePqBZlEBVvILa3nH1+q7mF9pDIjJtirbK8cL3PH6lra+dK5bOWcHkYqdeOd3ZLfOLMkpXNMCP+S4pgQQgghRrRAi4kHL5iCwQAfbC2mud1OdmyIa7W53jlWWNMEsc7g2s6jla4wfs82VdrsDrX9Ev8UxwBOGpfAJ3cez1TnAoGehAdauO/cSbz5gwWuERzAq0+mb1o8iiCLiS35NXyZ6/2Gvna7g9V7K7pk7RzUO8dkU6Xwgclo4KbjVffYZ7tUYXtsQpi7h/hVoMXk2gj79qbCbvfbHRq3vLiBG55f79og6JH0ueoyb50/TtM77sL4dSkz1XbNtgY48JX/Xxtg32dd7yvcqC4HeKSy1WZnX5kqfF2zMIsfnDCKe8+ZyO+WTQLUePrukjr2l3kXxq/LSfCuc+yT7SW02RyMjg/t8rP7SPp4fJNzaUpsmGyqFMKfpDgmhBBCiBFvVmY0Vy/IdP3zedNTXKMr+shiYXUzxDg7szb9Fw5/rfKAXGOVnoXxF9e2YHdoWM1G4odg7GV6ehTv3bqQ3507kV+dOd7VxeCJuDArVy3wvXvsv98c5oqn13H+42tdBUJXcUw6x4SPvjczpcv3krsCwkBY5lyC8eG2Ylrau25z/Wh7MR9uK2H5zlK+3l/p+ZOmzVOXQ9I5dkhd9hTGrzMaB2a0srpzcezzrvcNUt7YnpIGbA6NyGALyRGBrtvPnJzE0gkJ2BwaP3t9K7nOzDBP88Z0+s/c3R4Wx97bqkbgz+llpFJ3ZHZknHSOCeFXUhwTQgghxDHhp6fmMCouhJAAE+fPTHXd3tE51gxpc9SNB76EZ0+Dv01QOTgGI8TlePQ6h515Y2nRwUOWsWUxGblyfibXL8r2+rE3HT+KkAAT2wvrXEsMPKVvx9xdUs+yf6xhW0GtqziWLcUx4SOr2cR1CzsKOd4UfP1hTmY0yRGB1Lfa+GJ3R0elw6Hx2BcdXabvbO7eWdartNnqsuoANHjfpdkvenHMXecYQM5p6vLQav+/NqhCmZ4xBlC0WV0OcHFsZ7EaqZyYHN6lGGUwGPj9skmEB5rZWlDL6xvyAe87x/TOxoqGViqdGXm9qWxoZY1ziYq7kUqACUcUx2Ilc0wIv5LimBBCCCGOCcEBZt790UK++umJrm4xgOROnWPatCvgijdg6mVgDVfh/AAxY8CijvtsZ6nbzXX+zBsbCtEhAVx9XCYAf/tsr1ch5NudOT7RIQGU17dy0ZNfuzrIpHNM9Mdlc9OJCLJgMMDklIhBfW2j0cC5zu6xtzqNVn66s5TdJfWuhRcfby/p1lnWq6CojiUf+YM8WqmPNrrrHANInKouaw53XVLiK7sNavLUdb0wp3ePNVZArfO+pKn9fy03dhTVATAxufvfo/jwQH51lvrv0m5XP/tGeTkSHmI1u37+95U79uG2YuwOjSmpEWT28TMyNSqI0E5Ze9I5JoR/SXFMCCGEEMeMEKu524YvvVDW2GantsUGo5fAeU/AT/bCJf+D2TfAGQ8CUNXYxk3/3cC1z63nTefWyyMdrlKdUkdrcQzghkXZhFrN7Cqu45MdJR49prapnfwqVQh790fHsWhMLM3OQkF4oJnoEMnHEb4LC7Tw6k3zeeHauX0WEQbCec7i2Fe5ZVQ3tqFpGo9+sReAm47PdnWWfbnbiy6wND13bBBHK22tUOcs8LkL5AcIiYHQRHVdX0zSH3UF4LCBKQCmXa5u2/+FutS7xmLGQODAFj/14tiRnVi6C2emsmhMR1ZjtpedY9DRPdZX7th7WzpGKvtiNBq6jBTLtkoh/EuKY0IIIYQ4pgVaTMSGqsJNQXVzxx2WQBh3Jpz5F8g+AYDdxXWu0O2fvb6VVXvLuzzXyj3lvPD1YQBGe5lTM5xEBgdwrXOM7cFPcqlqbOvzMTuco0pp0UGkRgXzzNWzuWxuOgBT0yLdZukI4YmcxDAWjvF8wYQ/jU0IY0JSOO12jQ+2FfPF7jJ2FNURHGDi+kXZnD1NFTfe2Vzk+ZOm67ljg9g5VpMHaGAJgZC4vo9PcC4iKd3R/9fWRyojM2DMKer6wZVgaxu0vDG7Q2NXsd451nNxzGAw8MfzJhMbamVGeiQRQRavX0cvYrkrjlU1tvHdYTWKfrpz6UNfOueOyQcOQviXFMeEEEIIccxzhfLXNLs9Th+RMRkN2BwaN7+wge2Fqij09qZCrn3uO5ra7CwcHcv3ZqS6e6ph77qFWSSEWzlY0chl//qmzwLZjkLnG84k1fVhMRn5w7JJvPGD+TxyycC+4RViMJzXabTy786ssSvnZRAdEsC5U9V9X+wuo7a53bMn1DvHijZDu/ufPX7TeaTSk4J1gnP00x/Fsc6vnTgFgmPVNsyCbwetOHa4spGmNjuBFqPbjrC06GBW/exEXrt5gU+vo+fiuRurXLGnDE1TBa/Oo/7u6MWxqGALFpO8lRfCn+Q7SgghhBDHPFcof7X7N6h7nG90rl+YxfzsGBrb7Fz97Hf8+ZPd3PHKZmwOjXOmJvPM1bMJCjAN+HkPpIggCy9eP4/YUCu7S+q5/N/rqHZTINPzxialdHQ2GAwGZmZES4eDGBHOmZaMwQAbDlezJb+GQIvRtfRifFIYYxNCabM7+GS7Z6PIRGVCaAI42juKQwNN3xbZ10ilLmGSuizb6d/XNhph1Enqn/d9PmjFMX2kclxiuCsrrjdBAaY+j+mN3jm2p6S+19zGz3epEdyTxnnQwec0JysKg6FjbFMI4T9SHBNCCCHEMc/jzjHniMyklAievGom4xLDqGho5R9f7gdUt9XDF08jwDwyfsUaHR/KyzeqAtmu4jouc1Mgc4VcD3JYuhCDJSE8kONGdYx1XjYnw5X7ZDAYOHea6h5729OtlQbD4OeOVXlbHNPHKreD5vlyjh4duSVz9MnqcttrUF+ktgInTu7fa/TBlTfWy0ilv2TGhmAxGWhss/f4/5V2u4MVe9RY/knjEjx+3tHxYXx0+yKeunKW385VCKGMjN/chBBCCCH6ISWy784xTdPYU9oAqJGZ8EAL/7l2juuxd58+jl+dOR6jj50Gw5UqkM11FciueHodrbauG/ma2mzsL1f/bib1sAFOiJHiXGe2WIDZyE2Ls7vcp4eqf32gktK6Fs+ecLBzx/QCVV+bKnWxY8FggpZaqPMiT60nR27J1DvHavOdr5UD1oHNatzh7HDtLW/MXywmI6OcY5u7e8gd23C4mvoWG9EhAUxLi/TqucclhhMR7H0OmhDCPSmOCSGEEOKYlxKlNku66xwrrGmmodWGxWQgM0Zty0sID+STO4/ny5+cwE2LR43Y0PnR8WG8dMNcIoMt7CiqY0Vu10UEu4rr0DSID7PKBjUxop0zLZkr52Xwp/MnkxAe2OW+tOhgZmZEoWnw3hYPC0lpnYpjDofnJ1J9GN69DWryPX8MdBpt9LA4ZraqAhn0L3dM0zp1jmWqy9D4rp1iAzxSqWkaO/UO10Eo4rtGK3vIHfvCudX0hLFxPo9uCiH8S4pjQgghhDjmeTJWqb/ByY4N7TI2GWo1kxUbMrAnOAyMSQjj/OlqycDHR2Qq6aNKk2SkUoxwVrOJ3y2bxPm9LNxY5u3WyqQpYA6C5mqo3Ov5iaz6K2z8D6x80PPHOBzed45Bp1D+7Z4/5kjN1dCqfk50GekcdXLH9QEujpXVt1LZ2IbR0FG4Gkhjna/RU+fY57tKAThpfPyAn4cQwjNSHBNCCCHEMU8P5K9qbKO5zd7jMbklamxw7CC8qRquTp+cCMBnu0pps3V0uegbOycN8KiSEMPdGZOTMBkNbCus5YBz1NgtkwVSZqrr3uSOFW1Ul4fXev6YhhKwtagxyYg0zx+n5471J5RfH6kMSwJLp82Mo5d0XE+Z4fvze0AfqRwVF0qgZeAXpugj5l/tLqOs05jt4cpG9pc3YjYaWDTG8zB+IcTAkuKYEEIIIY55EUEWwqxmoPfuMb1zbDA6DoarGelRxIZaqWux8c2BStft2wsljF8IgJhQK3OzogGVPeaRdGcov6e5Y+3NUOosVFXug3oPt2PqBaqIVFWU85S+sbI/Y5W9bclMm6tui0jreJ0BskP/OTVIRfzjRscyJTWC+lYbv/9gl+t2faRyVmYUEUGSHSbEcCHFMSGEEEIIOrrHeiuO6ZsqxyYcu8Uxk9HA0olqs9rHO9Qb8lab3VU4HKw3nUIMZzMzogDYeLjGswfouWOH13i2EbJkO2idOlwPr/HsdXwZqQSId45VVuwBW8/bavt+7V6yzswBcNNK+MEasAR2f5wf7SwevLwxUD8v/7BsMkYDvLuliFV7VVajXhw72YstlUKIgSfFMSGEEEII3G+stNkd7HOOSOUcw8UxgNMmqtHKT3eUYndo7C1twObQiAy2uP4dCnEsm5GuimOb8qo9e0DGfDBZVfHKk1yvok1d/9nT0Upvw/h1EalgjQCHTRXIfFF1SF32VJgLjFB/BtiOosHtHAOYnBrBVfMzAfj129upbGh1dd2eOE7yxoQYTqQ4JoQQQghB586xpm73Hapsos3mIMhiIjXq2C4AzcuOITzQTEVDKxvzqjvljUWM2G2dQnhjenokAAcqGqlu9KDTyhoGY05R17e/2ffxet6YPoZ4yMPOMX2s0tvOMYOhI5Tf19wx16ZKL1/bT+pa2smrUj/bJwxyh+uPl44lPszKocomrn9+Pe12jYyYYEbFjfxFLkIcTaQ4JoQQQgiB+84xfWxwbEIoRuOxXQAKMBtZMl6NA320rYTtzpBrGakUQokMDiDbWfjYlO9h99ik76nL7W/0PVqpd47Nu0Vdlu+CRg/yzXztHIOOUH5fN1b2ljk2SHY6u8ZSIoOIDA4Y1NcOC7Twm7PVv79NeTUAnDQuXj5MEGKYkeKYEEIIIQTuM8f0vLGcYziMv7NTJ6nRyk92lEgYvxA90EcrPc4dG3sqWIKh5nBHZ1hPWhugPFddH70E4sar657kjlX1o0Cl5475Esrf3gJ1Req6t11rfqIXxwa7a0x3xuREFo/t2Ex5koxUCjHsSHFMCCGEEAJI9qhzTIpjAMePiSPIYqKwppnN+TUATJLOMSFc9NHKjZ7mjgWEwNjT1HV3o5XFWwANwlMgLAEyj1O395U71lwNzVXqenS2Z+fUmWtjpQ9jlTV5gAYBYRAc4/3j/WAo8sY6MxgM/O7cSYQEmEgItzLHudFUCDF8SHFMCCGEEAJIdRbHSupaaLc7utyXWyqdY50FBZg4IaejCyIkwERmjOTnCKHTO8e25Ndgd3iwgRJg0vnqcsfb4HD0fIw+Upk8XV1mLFCXh1e7f+7KA+oyLAmsoZ6dT2fxzg61+iJoqvLusZ1HKodglLDd7nBtipySOnQdrukxwXz248W8f+sirGbTkJ2HEKJnUhwTQgghhABiQ60EmIw4NCipbXHd3tJu51BFIyCbKjs7zTlaCWpU6VjPYhOis7EJYYRazTS22V2dp30afYrqrqorgILvej7GVRybpi4zFqrLku2qO6w3lfvUZfQoz87lSIHhEJmurnsbyu9aBJDp22v30+e7yiirbyU2NICFo+P6fsAASooIIi7MOqTnIITomRTHhBBCCCEAo9FAcmQg0DV3bH95Aw4NIoMt8qamkxPHxWMxqYLYxGTJGxOiM5PRwNQ09X3h8WilJRDGnaGu7+hltNJVHJuhLsMSIGY0oEHeut6fu2q/uozxsTgGnUYrvcwdG+JNlf/7Ng+AC2elEWCWt79CiJ759NPh8ccfJysri8DAQGbOnMmqVavcHv/iiy8ydepUgoODSUpK4pprrqGysutGlTfeeIMJEyZgtVqZMGECb731li+nJoQQQgjhM1cof6fcMVcYf0KYbBfrJDzQ4gqVnpct+TlCHMnrUH6AiZ1HK+1d72uu6Shy6WOV4Nlopd451p/imK+h/EO4qTK/qsk1Unnp7PRBf30hxNHD6+LYK6+8wh133ME999zDpk2bWLRoEaeffjp5eXk9Hr969WquuuoqrrvuOnbs2MFrr73Gd999x/XXX+865uuvv+biiy/myiuvZMuWLVx55ZVcdNFFrFvn5tMPIYQQQgg/S3HmjhV0Lo5J3livHvzeVF64bg6nTkzs+2AhjjF6cWyTp51jAKNOgsAIaCiBvK+73le8WV1GZkBwp4K0Plp5yM3GSldxbLTn53KkhInq0tvimGuscvA7x176Ng9Ng0VjYkmPCR701xdCHD28Lo499NBDXHfddVx//fWMHz+ehx9+mLS0NJ544okej//mm2/IzMzktttuIysri4ULF3LTTTexfv161zEPP/wwp5xyCnfffTfjxo3j7rvv5uSTT+bhhx/2+QsTQgghhPDWmHhVAHty5X4+3l4CwJ4S2VTZm4hgC4vGxElHnRA9mJYWCcCBikaqG9s8e5A5AMafra5vf6PrffpIZcqMrrfrnWPFW6C1h3wzTesI5O9Xccw5VlmyzX0hrjOHA2oOq+uDPFbZbnfw6voCAC6bI11jQgj3vCqOtbW1sWHDBpYuXdrl9qVLl7J2bc/rgxcsWEBBQQEffvghmqZRWlrK66+/zplnnuk65uuvv+72nKeeemqvzwnQ2tpKXV1dlz9CCCGEEP1x6dx0Fo6OpanNzs3/3cCjn+/tGKuUzjEhhBeiQgLIjlVbXDfle9E9po9W7nwHWhs6bi/cqC47j1QCRKapsHzNDvk9TN40lEFbPRiM/RttjBkN6QvA3grPnwub/tv3YxpKwNYCBhNEpPn+2j74bGcpFQ2txIVZWTIhYVBfWwhx9PGqOFZRUYHdbichoesPl4SEBEpKSnp8zIIFC3jxxRe5+OKLCQgIIDExkcjISB599FHXMSUlJV49J8D9999PRESE609a2uD+sBVCCCHEyBNqNfPcNbO5ekEmAH9dvoci5+bKsfFSHBNCeGe6L7ljWYshLBmaKuG1q8Herm4v2qwujyyOgfvRSj2nLCINzP1YKmI0whVvwIRzwdEO7/wQPv21ykar3A+r/wZPnQi/T4Cnl8Ln98G219VjI9PAZPb9tX2gB/FfNCsVi0mC+IUQ7vn0U+LI1nlN03ptp9+5cye33XYb//d//8eGDRv4+OOPOXjwIDfffLPPzwlw9913U1tb6/qTn5/vy5cihBBCCNGF2WTk3nMmcv/5k13bGJMiAokItgzxmQkhjjYzMiIBLzZWgioiXfQ8mINg33J4/w5orIBaZ8Zz0rTuj3GF8vcweeOPMH5dQDBc8Bws/rn657V/h4fGw6Mz4LN7oWij6hTLXwer/grLf62OG+SRysOVjazaW4HBAJdIEL8QwgNele9jY2MxmUzdOrrKysq6dX7p7r//fo477jh++tOfAjBlyhRCQkJYtGgRv//970lKSiIxMdGr5wSwWq1YrbJOXQghhBAD49I56YyKC+Wet7ZxztTkoT4dIcRRSA/l35Jfg92hYTJ6mM+XNhsueAZeuVyNL9Y4C2MxYyAwvPvx6fPUZdEmsLWp7DJdpbNzrD95Y50ZjXDiLyF2LLx9CzSUqrHJrEUw/hxIm6uWBxxcBYdWQV0hjDnFP6/toZe+VY0Ti8bEkRYtQfxCiL55VRwLCAhg5syZLF++nPPOO891+/Llyzn33HN7fExTUxNmc9eXMZlMgOoOA5g/fz7Lly/nzjvvdB3z6aefsmDBAm9OTwghhBDCr+ZkRbP8rsVDfRpCiKPU2IQwQq1mGlpt5JbUMyG5h8JWb8adAWc+pDrHDq5Ut/U0Ugmq8BUUBc3VKjA/dWbHfXrnWLQfOsc6m3wBxE+Asp1qy2bnDZqJk2D6FWoZQFsjWEP9+9puaJrGGxsliF8I4R2vxyrvuusu/v3vf/PMM8+wa9cu7rzzTvLy8lxjknfffTdXXXWV6/izzz6bN998kyeeeIIDBw6wZs0abrvtNubMmUNysvoU9vbbb+fTTz/lgQceYPfu3TzwwAN89tln3HHHHf75KoUQQgghhBBikJmMBqamRQBejlbqZl3TMcII3TdV6gwGSJ2jrh8Zyl/lh02VvUmYoIpknQtjR57XIBbGAIpqWyivb8VsNHDiuLhBfW0hxNHL6+LYxRdfzMMPP8x9993HtGnTWLlyJR9++CEZGRkAFBcXk5eX5zr+6quv5qGHHuKxxx5j0qRJXHjhheTk5PDmm2+6jlmwYAEvv/wyzz77LFOmTOG5557jlVdeYe7cuX74EoUQQgghhBBiaMzMUIWj9YeqfHuCE+6G+T+CyAwYd2bvx6U5i2MF33bc5nB0Ko5ldzn84+3FXPavb9hWUOvbeQ1Tu4rqABgdH4rVbBrisxFCHC0Mmj7beJSrq6sjIiKC2tpawsO9aFcWQgghhBBCiAGyZl8Fl/97HUkRgaz9xUlul471y8GV8J+zITwV7tqhbqvJh4cngdEC95R02Rh58ZNfs+5gFSEBJv511SwWjI4dmPMaZI99sZe/fLqH86an8LeLpw316QghhpA3dSLZaSuEEEIIIYQQA2R6eiRmo4Hi2hYKqpsH7oWSZ4DBCHUFUFuobtPzxqIyP6ODHAAAKcJJREFUuxTGAIpq1bk0ttm5+tnv+Hh78cCd2yDaVVwPwPiksCE+EyHE0USKY0IIIYQQQggxQIIDzExJVblj6w76OFrpCWsoJExS1/XRyqqeN1U6HBoltS0AzMuOps3u4JYXN/LSt3kc7XYVq7HK8UkyTSSE8JwUx4QQQgghhBBiAM3JigFg3YHKgX0hPXcs31kcq9SLY103VVY0ttJu1zAa4D/XzuHSOWk4NLj7zW3c/+Eu2myOgT3PAdLUZuNgZSMA4xKlOCaE8JwUx4QQQgghhBBiAM3NUqH83/oayu+pNOdCsz6KY0U1qmssPiwQq9nEH8+bzC0nqGOeXHmA859Yw76yhoE91wGQW1KPpkFsqJW4MOtQn44Q4igixTEhhBBCCCGEGEAzM6MwGuBwZZNrnHFApM5Wl8VboL2lI3MsumtxrLhG5Y0lRwYCYDAY+Nlp4/jnFTOJDLawvbCOsx5dxX+/OczRtL9td4nkjQkhfCPFMSGEEEIIIYQYQOGBFiYkqzG/dQcHcLQyKhNC4sHRDoXroeawuv2IzLEiZ4EuKTKoy+2nTUrkkzuOZ+HoWFraHfzq7e088HHuwJ2vn+l5YxMkb0wI4SUpjgkhhBBCCCHEAJuTqXLHvh3IUH6DoSN3bNtr4LCBOQjCkrocVuTsHEs5ojgGkBAeyPPXzuEnS8cC8PqG/KOme0zC+IUQvpLimBBCCCGEEEIMsLnZztyxgSyOQUdxbPtb6jJmFBi7vu0rrlXFsaSIwB6fwmg0cP2ibExGAxUNbRQP5Cion2iaxu5iNVY5TsYqhRBekuKYEEIIIYQQQgyw2ZmqOLa3rIHKhtaBe6FUZ3GstVZdRmd3O6TQGcifFNG9c0wXaDExNkEVmbYW1Pr3HAdAQXUz9a02AkxGRsWFDvXpCCGOMlIcE0IIIYQQQogBFh0SwNgEVbT5biC3ViZPA6O545+PyBuDjkD+nsYqO5uSEgHAtsIaf52d1z7fVcppD69k7b4Kt8ftdI5Ujo4PxWKSt7lCCO/ITw0hhBBCCCGEGARzs1Tu2LqBHK20BEHS1I5/jum6qbLN5qDc2bmWFNnzWKVucqoqjg1l59gjn+9ld0k9N/93A4crG3s9Th+plLwxIYQvpDgmhBBCCCGEEINgTpYarVx3YIBzx/TRSujWOVZa14KmQYDZSExIgNunmZoaCcC2wtohCeU/UN7gKszVtdi48fkNNLbaejy2I4xf8saEEN6T4pgQQgghhBBCDIK5zuLYrpI6apvbB+6F0joVx6K7do7pmyqTIwIxGAxun2ZsYigBJiM1Te3kVzX7/TT78u6WIgCmp0cSF2Ylt7Sen72xtcdC3a4S2VQphPCdFMeEEEIIIYQQYhDEhweSFRuCpsGGwwPYPZZxHFhCICoLQmK73KVvnnQXxq+zmk2uzY9bBzl3TNM03t2simNXzc/gictnYDEZ+GBrMU+uPNDl2IZWG4crmwApjgkhfCPFMSGEEEIIIYQYJHMyB2G0MiwBbloBV78PR3SHFeqdY32E8esm66H8fsoda7M5PBrR3F5Yx4GKRqxmI6dMSGRWZjS/OXsiAA9+vJuvcstcx+Y6u8YSwq1E9zEqKoQQPZHimBBCCCGEEEIMktnO0cqNedUD+0KxYyAitdvNxbV6ccx9GL9uih9D+cvqWpj9h8+469UtfR77zuZCAJZMSCDUqrZvXj43nYtnpeHQ4JYXN7LhsPp3uMsZxj8uUbrGhBC+keKYEEIIIYQQQgwSvdi0o6gOu8O3kPuimmYe/Hg3Jc4RSe8e6/lYJcAUZyj/9sJaHD6er+7bQ1XUNrfzyY4St91jdofGe1vVSOW5U5NdtxsMBu5bNpFFY2JparNzzbPfsqOotlMYvxTHhBC+keKYEEIIIYQQQgySUXGhBFlMNLXZOVjR4NNzPLXyAI9/tZ9n1xz0+rGuQH4PO8fGxIdiNRupb7VxsLLR69frbF+Z+nqb2uwUuSnsrTtYSWldK+GBZhbnxHW5z2o28eSVM5mVEUVdi42rnv6WNfsqANlUKYTwnRTHhBBCCCGEEGKQmIwGJiSrDqdthb6NKu50dkrpxSZv6IH8nmaOmU1GJurn28/Rys7nu7e0vtfj9CD+MyYnYTWbut0fHGDmmWtmMzE5nMrGNg45w/gnSOeYEMJHUhwTQgghhBBCiEGkh9z7kuOlaRq5Jaqw5G0nV2OrjdrmdgCSIjzrHIOO0cr+5o7tL+84394Ke602Ox9uKwbgnGnJPR4DEB5o4flr5zAqLgSAALORrNiQfp2fEOLYJcUxIYQQQgghhBhEenFsuw+dY2X1ra4CV35Vk1e5ZXoYf1igmbBAi8ePc22sLKzx/ESPYHdoHCjv3DnWc3FsRW45dS02EsKtzM2KcfucMaFWXrx+HvOyo7lhURZmk7y9FUL4xjzUJyCEEEIIIYQQx5LJR4Tym4wGjx+rd40BtNs1imqaSYsO9uixehh/sodh/LqpaXoxz/vz7XjtZlptDtc/7ynreazynS1qpPLsKckevU5iRCAv3zjf6/MRQojOpLQuhBBCCCGEEIOoP6H8nYtjAAcrPB+t9DaMX5cVG0pIgInmdjv7y31bIqCPUQZa1FvQfaUN3TZWaprGWme4/umTE316HSGE8IUUx4QQQgghhBBiEPUnlD/3iCD7Q17kjukbIpM8DOPXmYwGJjpHK7fk13j1WJ1eVFs4Og6T0UB9q43SutYuxxyubKK6qZ0Ak5FJztcTQojBIMUxIYQQQgghhBhkrhyvgjqvHrfHWRxLi1YFLp86x7wI49dNceWO+RbKr3eOTUgKIyNGjYHuPWK0crOz8DYxJbzHLZVCCDFQpDgmhBBCCCGEEINskg+h/HaH5iqOnTpBjR0e8qI4pgfyJ3vZOQYdOWm+bqzUi2Oj4kMZEx8KdA/l35RXDcC0tEifXkMIIXwlxTEhhBBCCCGEGGRTXKH8tR5vnMyvaqKl3UGA2cjinDhAjSJ6qtgZyJ/kZSA/wNTUSAB2Fte5tmV6Qx+rHBUXytiEMKD3zrHp6VFeP78QQvSHFMeEEEIIIYQQYpDpofyNXoTy63ljY+JDGRWnuq/yqpqw2R3uHgaosPtCHwP5ATJigslJCKPN5uCV7/K8emxlQyvVTaqgNioulNE9dI61tNvZWaxGTKdL55gQYpBJcUwIIYQQQgghBpkvofz6psqcxDASwwOxmo3YHB1FL3eqm9pptakiWqIPmWMGg4FrF2YC8J+1hz0qyOn2l6vRz5TIIIICTIyJ1zvHOjZW7iiqo92uERsaQGqU951tQgjRH1IcE0IIIYQQQogh4G0ov945lpMQhtFoIDMmBPAslF8P448Ntfocdn/utBRiQgIorGnm4x0lHj9OzxvTO8ay40IwGqC2uZ3yBrWxsiNvLAqDweDT+QkhhK+kOCaEEEIIIYQQQ8DbUH69c2xsouq80rc+ehLKX9SPkUpdoMXE5fMyAHh69UGPH6fnjenFsUCLifRode77nKOVHXljkT6fnxBC+EqKY0IIIYQQQggxBPTOsR1FtTj6COVvtdldHWLjnMWxrFjVOXbIg1D+4loVxp/sQxh/Z1fOyyDAZGRTXg0bnd1efXFtqnTmpAGM7jRaCbAprwaQvDEhxNCQ4pgQQgghhBBCDIFRcSEEWow0ttk50Ef314HyRuwOjbBAM4nhqvsr01Uc87xzLKkfnWMAcWFWzpmWDHjePXbkWCXAmAR1fU9pPWX1LRTWNGMwwBQpjgkhhoAUx4QQQgghhBBiCJhNRiYkqVD+vkYr9ZHKcYlhrkwuPXPMo7FKZ+dYSmT/w+6vW5gFwEfbiimodt+11txmdy0MGBUX4rp9rLM4tresgc3OrrGx8WGEWs39Pj8hhPCWFMeEEEIIIYQQYohMSY0E+t5YqYfxj00Ic92mj1XmVzfT3sf2yGK9c6yfY5UA45PCOW50DA4N/rP2kNtj9byxqGALMaFW1+36xsp9ZQ1skrwxIcQQk+KYEEIIIYQQQgyRSa6Nle6LY3ucnWM5iR3FsfgwK4EWI3aHRkF1s9vH+2usUqd3j738bb7b7rEjw/h1o+JCMRigqrGNL3aVATBNRiqFEENEimNCCCGEEEIIMUT0bqmNedXkV/VeZNqtF8c6dY4ZjQaPRisbW22usUr9+P46YWw8OQlh1LfaOPvR1azeW9Hjcft7COMHCAowkRqlutj0rrjp6VF+OTchhPCWFMeEEEIIIYQQYoiMigtl0ZhYbA6NR7/Y2+Mx9S3trtyuzmOV0Cl3zE0o/x5n8SkuzEp0SIA/Thuj0cAz18xmSmoE1U3tXPXMOv65Yj+a1nXr5r5eOsegY7QSINRq7vEYIYQYDFIcE0IIIYQQQoghdOcpYwF4Y2Nhjx1ge0pVgSk+zErUEcUt18ZKN51jenEs54jCWn+lRAbx6k3zuXBmKg4N/vTRbm55cSP1Le2uY/aXqfM6snMMYEynYtiU1AhMRoNfz08IITwlxTEhhBBCCCGEGEIz0qM4IScOu0Pj0S/2dbvfVdxK7F7cyooNBuBgZe8jmbklDb0+vr8CLSYevGAKfzhvEhaTgY+2l3D6I6v47lAVNruDg86iXU9dYZ1vkzB+IcRQkuKYEEIIIYQQQgyxO5ao7rG3NhW4Ckq63B7yxnQZHmSODVTnmM5gMHD53AxeuWk+adFBFFQ3c/GTX3P3m9toszuwmo2kRHbfkjmm0/lMS5O8MSHE0JHimBBCCCGEEEIMsWlpkZw0Lh6HBn//vCN77NMdJby9uRDorXNMFccKqptoszl6fG49zH/sAHSOdTYjPYoPb1vEBc4xy9c2FACQHReKsYeRydHxoQSYjZiMBukcE0IMKSmOCSGEEEIIIcQwcMeSMQC8s7mQbQW13P3mVm58YQM1Te1MSArnjMlJ3R4TH2YlOMCEQ4P86u6jlZUNrVQ0tAIwNmHgA+/DAi385cKpPHH5DCKDLQCM66UoF2o189SVM/nXVTOJDbUO+LkJIURvzEN9AkIIIYQQQgghYEpqJEvGx/PZrjKWPb4Gu0PDYIAbF2Vz19KxWM2mbo8xGAxkxISwq7iOw5WN3YLv9TD/9OhgggMG7+3f6ZOTmJERxesbCjhnanKvx52QEz9o5ySEEL2RzjEhhBBCCCGEGCb07DG7QyMpIpAXr5/L3WeM77EwpnOF8ld07xzLLakDYOwA5Y25kxAeyA9PHE1adPCgv7YQQnhDOseEEEIIIYQQYpiYlBLB786dSEFNM7csHk2EczTRHXeh/Lml+qbKgR+pFEKIo5UUx4QQQgghhBBiGLlyfqZXx2c5i2P6VsrOXJsqE8P7fV5CCDFSyVilEEIIIYQQQhzFZmdFA7DhcDW1ze2u2zVNY49zU2XOEIxVCiHE0UKKY0IIIYQQQghxFMuKDWFMfCg2h8ZXuWWu24tqW6hvtWE2GsiKDRnCMxRCiOFNimNCCCGEEEIIcZRbOjEBgE93lLpu07vGRsWFEmCWt35CCNEb+QkphBBCCCGEEEe5pRMSAfgqt4yWdjsAuc68sbGJMlIphBDuSHFMCCGEEEIIIY5yk1MiSAi30thm5+v9lQDkuvLGZFOlEEK4I8UxIYQQQgghhDjKGY0GTpngHK3cqUYr9eLYWAnjF0IIt6Q4JoQQQgghhBAjgD5auXxnKW02B/vKGwAYlxg+lKclhBDDnhTHhBBCCCGEEGIEmJcdQ5jVTEVDK29vLqTN5iDIYiI1KmioT00IIYY1KY4JIYQQQgghxAgQYDZywrh4AJ74aj8AYxNCMRoNQ3laQggx7ElxTAghhBBCCCFGiKXO3LGDFY0A5MimSiGE6JMUx4QQQgghhBBihDghJw6LqaNTTML4hRCibz4Vxx5//HGysrIIDAxk5syZrFq1qtdjr776agwGQ7c/EydOdB3z3HPP9XhMS0uLL6cnhBBCCCGEEMeksEAL80fFuv5ZOseEEKJvXhfHXnnlFe644w7uueceNm3axKJFizj99NPJy8vr8fhHHnmE4uJi15/8/Hyio6O58MILuxwXHh7e5bji4mICAwN9+6qEEEIIIYQQ4hilj1aCFMeEEMITXhfHHnroIa677jquv/56xo8fz8MPP0xaWhpPPPFEj8dHRESQmJjo+rN+/Xqqq6u55ppruhxnMBi6HJeYmOjbVySEEEIIIYQQx7ClExMIDzQzNiGUuFDrUJ+OEEIMe14Vx9ra2tiwYQNLly7tcvvSpUtZu3atR8/x9NNPs2TJEjIyMrrc3tDQQEZGBqmpqZx11lls2rTJ7fO0trZSV1fX5Y8QQgghhBBCHOviwwJZftdiXr1pPgaDbKoUQoi+eFUcq6iowG63k5CQ0OX2hIQESkpK+nx8cXExH330Eddff32X28eNG8dzzz3Hu+++y0svvURgYCDHHXcce/fu7fW57r//fiIiIlx/0tLSvPlShBBCCCGEEGLESggPJDI4YKhPQwghjgo+BfIf+emDpmkefSLx3HPPERkZybJly7rcPm/ePK644gqmTp3KokWLePXVVxk7diyPPvpor8919913U1tb6/qTn5/vy5cihBBCCCGEEEIIIY5hZm8Ojo2NxWQydesSKysr69ZNdiRN03jmmWe48sorCQhw/wmG0Whk9uzZbjvHrFYrVqvMzwshhBBCCCGEEEII33nVORYQEMDMmTNZvnx5l9uXL1/OggUL3D52xYoV7Nu3j+uuu67P19E0jc2bN5OUlOTN6QkhhBBCCCGEEEII4RWvOscA7rrrLq688kpmzZrF/Pnzeeqpp8jLy+Pmm28G1LhjYWEhzz//fJfHPf3008ydO5dJkyZ1e87f/va3zJs3jzFjxlBXV8ff//53Nm/ezD/+8Q8fvywhhBBCCCGEEEIIIfrmdXHs4osvprKykvvuu4/i4mImTZrEhx9+6No+WVxcTF5eXpfH1NbW8sYbb/DII4/0+Jw1NTXceOONlJSUEBERwfTp01m5ciVz5szx4UsSQgghhBBCCCGEEMIzBk3TtKE+CX+oq6sjIiKC2tpawsPDh/p0hBBCCCGEEEIIIcQQ8aZO5NO2SiGEEEIIIYQQQgghRgIpjgkhhBBCCCGEEEKIY5YUx4QQQgghhBBCCCHEMUuKY0IIIYQQQgghhBDimCXFMSGEEEIIIYQQQghxzJLimBBCCCGEEEIIIYQ4ZklxTAghhBBCCCGEEEIcs6Q4JoQQQgghhBBCCCGOWeahPgF/0TQNgLq6uiE+EyGEEEIIIYQQQggxlPT6kF4vcmfEFMfq6+sBSEtLG+IzEUIIIYQQQgghhBDDQX19PREREW6PMWielNCOAg6Hg6KiIsLCwjAYDEN9On5RV1dHWloa+fn5hIeHD/XpCHFUku8jIfxDvpeE6D/5PhLCP+R7SYj+Oxa+jzRNo76+nuTkZIxG96liI6ZzzGg0kpqaOtSnMSDCw8NH7F9WIQaLfB8J4R/yvSRE/8n3kRD+Id9LQvTfSP8+6qtjTCeB/EIIIYQQQgghhBDimCXFMSGEEEIIIYQQQghxzJLi2DBmtVr5zW9+g9VqHepTEeKoJd9HQviHfC8J0X/yfSSEf8j3khD9J99HXY2YQH4hhBBCCCGEEEIIIbwlnWNCCCGEEEIIIYQQ4pglxTEhhBBCCCGEEEIIccyS4pgQQgghhBBCCCGEOGZJcUwIIYQQQgghhBBCHLOO+eLY/fffz+zZswkLCyM+Pp5ly5aRm5vb5RhN07j33ntJTk4mKCiIE044gR07drjur6qq4tZbbyUnJ4fg4GDS09O57bbbqK2tdR1z6NAhrrvuOrKysggKCmLUqFH85je/oa2trc9z3LZtG4sXLyYoKIiUlBTuu+8+Ou9RKC4u5rLLLiMnJwej0cgdd9zh8df/+OOPk5WVRWBgIDNnzmTVqlVd7r/33nsZN24cISEhREVFsWTJEtatW+fx84tjx0j4Xlq9ejXHHXccMTExBAUFMW7cOP72t7/1+bwrV67k7LPPJjk5GYPBwNtvv93jcbt27eKcc84hIiKCsLAw5s2bR15eXp/PL44dI+H76KuvvsJgMHT7s3v37n5/7SDfR8IzI+F7CeDFF19k6tSpBAcHk5SUxDXXXENlZWWfz93X73elpaVcffXVJCcnExwczGmnncbevXv7fF5xbBnu30ctLS1cffXVTJ48GbPZzLJly3o8bsWKFcycOZPAwECys7P55z//2efX7snvdvJ9JDw1Er6X3nzzTU455RTi4uIIDw9n/vz5fPLJJ31+7U888QRTpkwhPDzc9biPPvrIq6990GjHuFNPPVV79tlnte3bt2ubN2/WzjzzTC09PV1raGhwHfOnP/1JCwsL09544w1t27Zt2sUXX6wlJSVpdXV1mqZp2rZt27Tzzz9fe/fdd7V9+/Zpn3/+uTZmzBjte9/7nus5PvroI+3qq6/WPvnkE23//v3aO++8o8XHx2s//vGP3Z5fbW2tlpCQoF1yySXatm3btDfeeEMLCwvT/vKXv7iOOXjwoHbbbbdp//nPf7Rp06Zpt99+u0df+8svv6xZLBbtX//6l7Zz507t9ttv10JCQrTDhw+7jnnxxRe15cuXa/v379e2b9+uXXfddVp4eLhWVlbm0WuIY8dI+F7auHGj9r///U/bvn27dvDgQe2FF17QgoODtSeffNLtc3/44YfaPffco73xxhsaoL311lvdjtm3b58WHR2t/fSnP9U2btyo7d+/X3v//fe10tJST/71imPESPg++vLLLzVAy83N1YqLi11/bDZbv792+T4SnhoJ30urVq3SjEaj9sgjj2gHDhzQVq1apU2cOFFbtmyZ2+fu6/c7h8OhzZs3T1u0aJH27bffart379ZuvPHGbv9+hBju30cNDQ3azTffrD311FPaqaeeqp177rndjjlw4IAWHBys3X777drOnTu1f/3rX5rFYtFef/11t8/d1+928n0kvDESvpduv/127YEHHtC+/fZbbc+ePdrdd9+tWSwWbePGjW6f+91339U++OADLTc3V8vNzdV++ctfahaLRdu+fbvHX/tgOeaLY0cqKyvTAG3FihWapqkffImJidqf/vQn1zEtLS1aRESE9s9//rPX53n11Ve1gIAArb29vddjHnzwQS0rK8vt+Tz++ONaRESE1tLS4rrt/vvv15KTkzWHw9Ht+MWLF3tcHJszZ4528803d7lt3Lhx2i9+8YteH1NbW6sB2meffebRa4hj19H+vaQ777zztCv+v737j4m6/uMA/kS4O9ATBEVA0VO7Qs/MEFIxFEgDzNLplk5z4dJSlzVp2SSd4miTUvMHqUURraHmFHCmruGmiA5dwo5EWWIEOX8gMzEQy0t4ff/wy305uTvO86ve3ef52PiDz33u9fm8bj7H+15+7j5z59qt3ZGt4disWbMeqA6RiHvmqH041tjY6EiLNt3fuwhzRM5zxyytW7dOhgwZYvG8LVu2SHh4uN3aXa3vzp8/LwAs3pjcvXtXgoKC5Ouvv7Zbm5TN1XLUUUpKitU39B999JEMHTrUYtvChQtl7NixDte2trZjjuhhuGOWrDEYDLJmzRqHa7cLDAyUb775RkSc7/1RUPzHKu/XflliUFAQAKC2thb19fVITEw076PRaBAXF4fS0lK7dfz9/eHj42N3n/bj2HLy5EnExcVBo9GYtyUlJeHKlSuoq6tzpCWrTCYTysvLLfoCgMTERJt9mUwmZGdnIyAgACNHjnT62KQMnpAlo9GI0tJSxMXF2a3dlba2Nhw8eBDPPPMMkpKS0LdvX4wZM8bmxy+J2rlzjiIjIxEWFoaJEyfi6NGjduvaOh/gf70zR/Qw3DFL48aNw6VLl3Do0CGICK5du4a9e/diypQpNus6sr67c+cOAMDX19f8uLe3N9RqNU6cOGH3vEnZXC1Hjjh58mSnPCQlJaGsrAz//vuv03WZI3oY7pil+7W1taG5ufmBare2tuKHH35AS0sLYmJiADjf+6PA4VgHIoIPPvgAsbGxePbZZwEA9fX1AICQkBCLfUNCQsyP3e/PP/9ERkYGFi5caPNYNTU1yMrKwqJFi+yeU319vdVjdzw3Z1y/fh2tra0O9XXgwAFotVr4+vpi48aNOHz4MPr06eP0scnzuXuWwsPDodFoEB0djXfffRcLFiywW7srDQ0NuHXrFjIzM5GcnIyioiJMnz4dM2bMwLFjxx6qNnkud81RWFgYsrOzkZ+fj4KCAkRERGDixIkoKSmxW7sja70zR+Qsd83SuHHjsGPHDsyaNQtqtRqhoaHo1asXsrKybNZ1ZH03dOhQ6HQ6pKWlobGxESaTCZmZmaivr8fVq1ftnjcplyvmyBG2snb37l1cv37d6brMETnLXbN0vw0bNqClpQUzZ87sct/KykpotVpoNBosWrQIhYWFMBgMAJzr/VHhcKyDJUuW4MyZM9i1a1enx7y8vCx+F5FO2wCgqakJU6ZMgcFgwOrVq60e58qVK0hOTsbrr79u8aZ7+PDh0Gq10Gq1mDx5st1jW9tuy/Hjx811tVotduzY8UB9JSQkoKKiAqWlpUhOTsbMmTPR0NDg0LFJmdw9S8ePH0dZWRm+/PJLbNq0ydyHvSzZ09bWBgCYNm0aUlNT8fzzz2P58uV49dVXHfpSWFImd81RREQE3n77bYwaNQoxMTHYtm0bpkyZgvXr1wNwLEfWemeOyFnumqWqqiq8//77WLVqFcrLy/HTTz+htrbW/CbH2fWdSqVCfn4+qqurERQUhO7du6O4uBiTJ0+Gt7e31d6IXDVHjrCXNWfXdswROcuds9Ru165dSE9Px+7du9G3b18A9v8mRUREoKKiAqdOncLixYuRkpKCqqoqp3p/lGxff6cw7733Hvbv34+SkhKEh4ebt4eGhgK4N9EMCwszb29oaOg03WxubkZycjK0Wi0KCwuhUqk6HefKlStISEhATEwMsrOzLR47dOiQ+fJePz8/8/Hvn5i2D6buP74t0dHRqKioMP8eEhICjUYDb29vq7Xvr9ujRw/o9Xro9XqMHTsWTz/9NHJycpCWlubQ8UlZPCFLgwcPBgCMGDEC165dQ3p6OmbPnm01S47o06cPfHx8zP9D0m7YsGG89J6s8oQcdTR27Fjk5eUBsP43yZHemSNyhjtnae3atXjxxRexbNkyAMBzzz2HHj16YPz48fjkk08ean0XFRWFiooK/PXXXzCZTAgODsaYMWMQHR1t66UkBXPVHDnCVtZ8fHzQu3dvBAQEOLW2A5gjenDunKV2u3fvxvz587Fnzx5MmjTJvN3e+k6tVkOv15v3O336NDZv3oyvvvrqgXp/1BR/5ZiIYMmSJSgoKMCRI0fMb4rbDR48GKGhoTh8+LB5m8lkwrFjxzBu3DjztqamJiQmJkKtVmP//v0Wnz9vd/nyZcTHx2PUqFHIzc1Ft26WL79OpzMPofr37w8AiImJQUlJicXtV4uKitCvXz8MGjTIoR79/PzMdfV6PXr27Am1Wo2oqCiLvgDg8OHDFn1ZIyLmz9kTtfPULHX8924tS45Qq9V44YUXOt2yubq6GjqdzqEapAyemiOj0Whe8NjKUVe9M0f0IDwhS7dv3+5Uq/2KFBH5v6zvAgICEBwcjAsXLqCsrAzTpk2z+ZqS8rh6jhwRExPTKQ9FRUWIjo6GSqVyem3XEXNEXfGELAH3rhibN28edu7c2en7Lx8kSx3fXzna+2PxyL/y38UtXrxYAgICpLi42OKW87dv3zbvk5mZKQEBAVJQUCCVlZUye/Zsi1uLNjU1yZgxY2TEiBHy22+/Wb11/eXLl0Wv18tLL70kly5dstjHnps3b0pISIjMnj1bKisrpaCgQPz9/S1u9S0iYjQaxWg0SlRUlMyZM0eMRqOcO3fObu32W33n5ORIVVWVLF26VHr06CF1dXUicu+WrmlpaXLy5Empq6uT8vJymT9/vmg0Gos7sxCJeEaWvvjiC9m/f79UV1dLdXW1fPvtt+Lv7y8rVqywW7u5udmcQQDy+eefi9FolD/++MO8T0FBgahUKsnOzpYLFy5IVlaWeHt7y/Hjxx/4tSbP5Qk52rhxoxQWFkp1dbWcPXtWli9fLgAkPz//oXtnjshRnpCl3Nxc8fHxkW3btklNTY2cOHFCoqOjZfTo0XZrd7W+E7l3h7OjR49KTU2N7Nu3T3Q6ncyYMeOBX2fybK6eIxGRc+fOidFolNdee03i4+PN67F2v//+u3Tv3l1SU1OlqqpKcnJyRKVSyd69e+3WdWRtxxyRozwhSzt37hQfHx/ZunWrRd2bN2/arZuWliYlJSVSW1srZ86ckY8//li6desmRUVFDvf+uCh+OAbA6k9ubq55n7a2Nlm9erWEhoaKRqORCRMmSGVlpfnx9tvWW/upra0VkXsLHFv7dOXMmTMyfvx40Wg0EhoaKunp6ebbfNvrQ6fTdVl769atotPpRK1Wy6hRo8y3kxUR+fvvv2X69OnSr18/UavVEhYWJlOnTpWff/65y7qkPJ6QpS1btsjw4cOle/fu4u/vL5GRkbJt2zZpbW21W9fWeaekpFjsl5OTI3q9Xnx9fWXkyJGyb9++rl9YUhRPyNGnn34qTz31lPj6+kpgYKDExsbKwYMH/y+9izBH5BhPyJLIvb9LBoNB/Pz8JCwsTN544w25dOlSl7Xtre9ERDZv3izh4eGiUqlk4MCBsnLlSrlz506XdUlZ3CFHOp2uy+cVFxdLZGSkqNVqGTRokGzfvr3Luo6s7ZgjcpQnZCkuLs6h9zv3e+utt8x/j4KDg2XixIkWgzFHen9cvET++42ERERERERERERECqP47xwjIiIiIiIiIiLl4nCMiIiIiIiIiIgUi8MxIiIiIiIiIiJSLA7HiIiIiIiIiIhIsTgcIyIiIiIiIiIixeJwjIiIiIiIiIiIFIvDMSIiIiIiIiIiUiwOx4iIiIjcSHx8PJYuXfqkT4OIiIjIY3A4RkREROShiouL4eXlhZs3bz7pUyEiIiJyWRyOERERERERERGRYnE4RkREROSiWlpa8Oabb0Kr1SIsLAwbNmyweDwvLw/R0dHo2bMnQkNDMWfOHDQ0NAAA6urqkJCQAAAIDAyEl5cX5s2bBwAQEXz22WcYMmQI/Pz8MHLkSOzdu/ex9kZERETkKjgcIyIiInJRy5Ytw9GjR1FYWIiioiIUFxejvLzc/LjJZEJGRgZ++eUX7Nu3D7W1teYB2IABA5Cfnw8AOH/+PK5evYrNmzcDAFauXInc3Fxs374d586dQ2pqKubOnYtjx4499h6JiIiInjQvEZEnfRJEREREZOnWrVvo3bs3vv/+e8yaNQsAcOPGDYSHh+Odd97Bpk2bOj3n9OnTGD16NJqbm6HValFcXIyEhAQ0NjaiV69eAO5djdanTx8cOXIEMTEx5ucuWLAAt2/fxs6dOx9He0REREQuw+dJnwARERERdVZTUwOTyWQxwAoKCkJERIT5d6PRiPT0dFRUVODGjRtoa2sDAFy8eBEGg8Fq3aqqKvzzzz94+eWXLbabTCZERkY+gk6IiIiIXBuHY0REREQuqKuL+1taWpCYmIjExETk5eUhODgYFy9eRFJSEkwmk83ntQ/QDh48iP79+1s8ptFoHv7EiYiIiNwMh2NERERELkiv10OlUuHUqVMYOHAgAKCxsRHV1dWIi4vDr7/+iuvXryMzMxMDBgwAAJSVlVnUUKvVAIDW1lbzNoPBAI1Gg4sXLyIuLu4xdUNERETkujgcIyIiInJBWq0W8+fPx7Jly9C7d2+EhIRgxYoV6Nbt3v2UBg4cCLVajaysLCxatAhnz55FRkaGRQ2dTgcvLy8cOHAAr7zyCvz8/NCzZ098+OGHSE1NRVtbG2JjY9HU1ITS0lJotVqkpKQ8iXaJiIiInhjerZKIiIjIRa1btw4TJkzA1KlTMWnSJMTGxiIqKgoAEBwcjO+++w579uyBwWBAZmYm1q9fb/H8/v37Y82aNVi+fDlCQkKwZMkSAEBGRgZWrVqFtWvXYtiwYUhKSsKPP/6IwYMHP/YeiYiIiJ403q2SiIiIiIiIiIgUi1eOERERERERERGRYnE4RkREREREREREisXhGBERERERERERKRaHY0REREREREREpFgcjhERERERERERkWJxOEZERERERERERIrF4RgRERERERERESkWh2NERERERERERKRYHI4REREREREREZFicThGRERERERERESKxeEYEREREREREREpFodjRERERERERESkWP8BmtzvI/AL0zoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value, \n",
    "#               baseline_ticker = '^DJI', \n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
