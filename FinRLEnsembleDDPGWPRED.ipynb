{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrds in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.26.4)\n",
      "Requirement already satisfied: packaging<23.3 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (23.2)\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.2.2)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.9.9)\n",
      "Requirement already satisfied: scipy<1.13,>=1.12 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.0.29)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (4.2.1)\n",
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n"
     ]
    }
   ],
   "source": [
    "# ## install finrl library\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 01:39:58.232653: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-28 01:39:58.267255: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 01:39:58.968201: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent, DRLEnsembleAgentDDPG\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_5_TICKER = [\n",
    "    \"AXP\",\n",
    "    \"AMGN\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "    \"CAT\",\n",
    "]\n",
    "INDEX_5_TICKER = [\n",
    "    \"^DJI\", \n",
    "    \"^IXIC\", \n",
    "    \"^NYA\", \n",
    "    \"^RUT\", \n",
    "    \"^GSPC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "\n",
    "# # TRAIN_START_DATE = '2009-04-01'\n",
    "# # TRAIN_END_DATE = '2021-01-01'\n",
    "# # TEST_START_DATE = '2021-01-01'\n",
    "# # TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "\n",
    "# TRAIN_START_DATE = '2009-06-01'\n",
    "# #TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "# dfexport = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = DOW_30_TICKER).fetch_data()\n",
    "\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfexport.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data export\n",
    "# import pickle\n",
    "# datasetName = \"dailydata\"\n",
    "# datasetDir = \"./datasets\"\n",
    "\n",
    "# os.makedirs(datasetDir, exist_ok=True)\n",
    "# datasetPath = os.path.join(datasetDir, datasetName) + \".pkl\"\n",
    "\n",
    "\n",
    "# with open(datasetPath, 'wb') as file:\n",
    "#     pickle.dump(dfexport, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Can't determine version for tzdata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32mtzconversion.pyx:83\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.tzconversion.Localizer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimezones.pyx:81\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timezones.is_utc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimezones.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timezones.is_utc_zoneinfo\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py:150\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    148\u001b[0m minimum_version \u001b[38;5;241m=\u001b[39m min_version \u001b[38;5;28;01mif\u001b[39;00m min_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m VERSIONS\u001b[38;5;241m.\u001b[39mget(parent)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m minimum_version:\n\u001b[0;32m--> 150\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_to_get\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mand\u001b[39;00m Version(version) \u001b[38;5;241m<\u001b[39m Version(minimum_version):\n\u001b[1;32m    152\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas requires version \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminimum_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or newer of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(version \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m currently installed).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py:78\u001b[0m, in \u001b[0;36mget_version\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m     75\u001b[0m version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt determine version for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsycopg2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# psycopg2 appends \" (dt dec pq3 ext lo64)\" to it's version\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: Can't determine version for tzdata"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.tslibs.conversion._localize_tso'\n",
      "Traceback (most recent call last):\n",
      "  File \"tzconversion.pyx\", line 83, in pandas._libs.tslibs.tzconversion.Localizer.__cinit__\n",
      "  File \"timezones.pyx\", line 81, in pandas._libs.tslibs.timezones.is_utc\n",
      "  File \"timezones.pyx\", line 70, in pandas._libs.tslibs.timezones.is_utc_zoneinfo\n",
      "  File \"/home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py\", line 150, in import_optional_dependency\n",
      "    version = get_version(module_to_get)\n",
      "  File \"/home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py\", line 78, in get_version\n",
      "    raise ImportError(f\"Can't determine version for {module.__name__}\")\n",
      "ImportError: Can't determine version for tzdata\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (16555, 8)\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "#TRAIN_START_DATE = '2000-01-01'\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2017-10-01'\n",
    "TEST_START_DATE = '2017-10-01'\n",
    "TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = INDEX_5_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Andrew Martin - UNCOMMENT BELOW TO ADD PREDICTION INDICATOR\n",
    "import pickle\n",
    "with open(\"./index_5_predictor_4.pkl\", 'rb') as file:\n",
    "  df_prob = pickle.load(file)\n",
    "df6 = df_prob.copy()\n",
    "df6 = df6.loc[:, ~df6.columns.duplicated(keep='first')]\n",
    "df6[\"date\"] = df6[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "df2 = processed.merge(df6[['tic', 'date', 'Probability']], on=['tic', 'date'], how='left')\n",
    "processed = df2.copy()\n",
    "INDICATORS.append(\"Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Andrew Martin - UNCOMMENT BELOW TO ADD PREDICTION INDICATOR\n",
    "# import pickle\n",
    "# with open(\"./datasets/index_5_predictor_2.pkl\", 'rb') as file:\n",
    "#   df_prob = pickle.load(file)\n",
    "# df6 = df_prob.copy()\n",
    "# df6 = df6.loc[:, ~df6.columns.duplicated(keep='first')]\n",
    "# df6[\"date\"] = df6[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "# df2 = processed.merge(df6[['tic', 'date', 'Predicted_Target']], on=['tic', 'date'], how='left')\n",
    "# processed = df2.copy()\n",
    "# INDICATORS.append(\"Predicted_Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 5, State Space: 56\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgentDDPG(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2017-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 164        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.133     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | 0.16840105 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 3.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 163        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.651     |\n",
      "|    reward             | 0.63620317 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 1.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 161        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.67      |\n",
      "|    reward             | -2.1296368 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 159        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -0.00493   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 8.33       |\n",
      "|    reward             | 0.46446767 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 1.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 159        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -2.81      |\n",
      "|    reward             | 0.21928309 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.159      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 159        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 7.95       |\n",
      "|    reward             | 0.21562569 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 1.16       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 159       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 3.5       |\n",
      "|    reward             | 0.7190259 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.698     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 159       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -37.1     |\n",
      "|    reward             | 2.7844517 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 29.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 161       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -13.3     |\n",
      "|    reward             | 1.1997771 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 4.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 163       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 11.3      |\n",
      "|    reward             | 0.4024002 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 2.89      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 164         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -16.4       |\n",
      "|    reward             | -0.27844813 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 5.23        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 165        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 36         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 8.69       |\n",
      "|    reward             | 0.42975974 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 2.06       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 166         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -10.8       |\n",
      "|    reward             | -0.32019717 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 3.7         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 167         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -1.26       |\n",
      "|    reward             | -0.29441005 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 0.335       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 168         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 9.56        |\n",
      "|    reward             | -0.73336625 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 1.84        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 169        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 1.06       |\n",
      "|    reward             | -1.5561739 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 0.0491     |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 170         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 49          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 3.97        |\n",
      "|    reward             | -0.17497241 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 0.382       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 171        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -14.7      |\n",
      "|    reward             | -3.1577497 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 5.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 172        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -3.99      |\n",
      "|    reward             | -0.8038847 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "day: 1949, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2112915.68\n",
      "total_reward: 1112915.68\n",
      "total_cost: 2081.44\n",
      "total_trades: 5968\n",
      "Sharpe: 0.767\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 173         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 3.22        |\n",
      "|    reward             | -0.14779626 |\n",
      "|    std                | 0.986       |\n",
      "|    value_loss         | 0.144       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2017-10-02 to  2018-01-02\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_23\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 261         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.15781134 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005705503 |\n",
      "|    clip_fraction        | 0.056       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.0251     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00417    |\n",
      "|    reward               | 0.6261728   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005138954 |\n",
      "|    clip_fraction        | 0.0335      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | -0.00189    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.76        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00144    |\n",
      "|    reward               | -0.65177435 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.5         |\n",
      "-----------------------------------------\n",
      "day: 1949, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 636805.18\n",
      "total_reward: -363194.82\n",
      "total_cost: 850688.91\n",
      "total_trades: 7510\n",
      "Sharpe: -0.344\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006973056 |\n",
      "|    clip_fraction        | 0.0572      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.17       |\n",
      "|    explained_variance   | -0.00957    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.66        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00709    |\n",
      "|    reward               | -0.79517484 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 3.09        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 226        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 45         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00698487 |\n",
      "|    clip_fraction        | 0.0678     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.19      |\n",
      "|    explained_variance   | 0.00122    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.47       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.006     |\n",
      "|    reward               | -1.5896353 |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 3.52       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2017-10-02 to  2018-01-02\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_21\n",
      "day: 1949, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1706131.40\n",
      "total_reward: 706131.40\n",
      "total_cost: 1526.56\n",
      "total_trades: 3899\n",
      "Sharpe: 0.515\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 105        |\n",
      "|    time_elapsed    | 73         |\n",
      "|    total_timesteps | 7800       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -1.2e+03   |\n",
      "|    critic_loss     | 132        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7699       |\n",
      "|    reward          | 0.40062988 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2017-10-02 to  2018-01-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-01-02\n",
      "======Trading from:  2018-01-02 to  2018-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-01-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_21\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 155          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 3            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.07        |\n",
      "|    explained_variance | 0.698        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -10.7        |\n",
      "|    reward             | -0.046126124 |\n",
      "|    std                | 0.995        |\n",
      "|    value_loss         | 2.06         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 157       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.275     |\n",
      "|    reward             | 1.2247003 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.41      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 160        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.0143    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.22      |\n",
      "|    reward             | -2.1155918 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 3.52       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 162        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -22.3      |\n",
      "|    reward             | -0.3546932 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 9.52       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 163         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 34.7        |\n",
      "|    reward             | 0.076374605 |\n",
      "|    std                | 0.986       |\n",
      "|    value_loss         | 26.9        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 163          |\n",
      "|    iterations         | 600          |\n",
      "|    time_elapsed       | 18           |\n",
      "|    total_timesteps    | 3000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.02        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 599          |\n",
      "|    policy_loss        | 8.48         |\n",
      "|    reward             | -0.080061086 |\n",
      "|    std                | 0.985        |\n",
      "|    value_loss         | 2.48         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 163         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 25.7        |\n",
      "|    reward             | -0.72761375 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 15.5        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 163       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -15.3     |\n",
      "|    reward             | 0.1880597 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 5.56      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 163        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -15.9      |\n",
      "|    reward             | -1.7150571 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 6.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 165        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -23        |\n",
      "|    reward             | 0.27785778 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 10.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 166         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 15.2        |\n",
      "|    reward             | -0.49773327 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 7.76        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 166       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 25.9      |\n",
      "|    reward             | 2.0442004 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 19.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 167        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 18         |\n",
      "|    reward             | -2.5867095 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 9.68       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 168          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 41           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.03        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | 3.02         |\n",
      "|    reward             | -0.020293713 |\n",
      "|    std                | 0.986        |\n",
      "|    value_loss         | 0.719        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 169       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 5.04      |\n",
      "|    reward             | 3.0440676 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 169         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 47          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 1.34        |\n",
      "|    reward             | -0.17175813 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 0.155       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 170       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 6.62      |\n",
      "|    reward             | 1.2807946 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 6.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 170        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 2.79       |\n",
      "|    reward             | -0.5302416 |\n",
      "|    std                | 0.978      |\n",
      "|    value_loss         | 0.534      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 171       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 55        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -51.3     |\n",
      "|    reward             | 3.6052272 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 55        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 172        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 57         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 0.343      |\n",
      "|    reward             | 0.37882748 |\n",
      "|    std                | 0.973      |\n",
      "|    value_loss         | 0.423      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-01-02 to  2018-04-04\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_21\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 265        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 7          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.17290816 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005983077 |\n",
      "|    clip_fraction        | 0.0521      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.025      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.17        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    reward               | 0.031977076 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.33        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038585514 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.00195     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.58         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    reward               | -1.8429974   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.39         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 235         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008873581 |\n",
      "|    clip_fraction        | 0.0744      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | -0.00179    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.05        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00479    |\n",
      "|    reward               | 0.54349375  |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 4.65        |\n",
      "-----------------------------------------\n",
      "day: 2012, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 855343.48\n",
      "total_reward: -144656.52\n",
      "total_cost: 842193.31\n",
      "total_trades: 7870\n",
      "Sharpe: -0.065\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 229         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007866662 |\n",
      "|    clip_fraction        | 0.0678      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00226    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    reward               | 0.004125811 |\n",
      "|    std                  | 0.998       |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-01-02 to  2018-04-04\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_21\n",
      "day: 2012, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2488142.89\n",
      "total_reward: 1488142.89\n",
      "total_cost: 998.50\n",
      "total_trades: 6036\n",
      "Sharpe: 0.862\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 103        |\n",
      "|    time_elapsed    | 77         |\n",
      "|    total_timesteps | 8052       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 342        |\n",
      "|    critic_loss     | 2.84       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7951       |\n",
      "|    reward          | -1.3933445 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-01-02 to  2018-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-04-04\n",
      "======Trading from:  2018-04-04 to  2018-07-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_20\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 152         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.00395    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -13.4       |\n",
      "|    reward             | -0.07614722 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.41        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 156        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.44      |\n",
      "|    reward             | 0.79914045 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 159        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.65      |\n",
      "|    reward             | -1.8500414 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.72       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 161          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 12           |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -8.65        |\n",
      "|    reward             | -0.020760063 |\n",
      "|    std                | 0.997        |\n",
      "|    value_loss         | 1.54         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 162        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 19.2       |\n",
      "|    reward             | -1.3322678 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 9.65       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 162       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -11.2     |\n",
      "|    reward             | 0.2735222 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 2.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 163        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -75.1      |\n",
      "|    reward             | -0.8836793 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 128        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 163         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -10.3       |\n",
      "|    reward             | -0.16075356 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 2.94        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 163         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.72       |\n",
      "|    reward             | -0.96076095 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 2.73        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 164       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 1.79e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 7.63      |\n",
      "|    reward             | 1.3832847 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 1.44      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 164          |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 33           |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.04        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | -10          |\n",
      "|    reward             | -0.035537757 |\n",
      "|    std                | 0.989        |\n",
      "|    value_loss         | 3.6          |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 165         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 36          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | -4.05       |\n",
      "|    reward             | -0.35065287 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 0.684       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 166       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.577    |\n",
      "|    reward             | 2.0730288 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 0.373     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 167       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 6.28      |\n",
      "|    reward             | 0.7324113 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 168        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -15.5      |\n",
      "|    reward             | -0.8395235 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 7.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 168        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -2.82      |\n",
      "|    reward             | 0.34677005 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.873      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 169       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 10.3      |\n",
      "|    reward             | 1.2706412 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 169         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 53          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -7.74       |\n",
      "|    reward             | -0.16694942 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 2.75        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 170      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -10.1    |\n",
      "|    reward             | 1.169562 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 4.89     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 171       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 58        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    reward             | 1.3005764 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 7.2       |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2018-04-04 to  2018-07-03\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_20\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 265       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.0665371 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060580787 |\n",
      "|    clip_fraction        | 0.0598       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.00889     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00547     |\n",
      "|    reward               | 0.33754903   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.76         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 240          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065262057 |\n",
      "|    clip_fraction        | 0.0772       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0105      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.9          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00639     |\n",
      "|    reward               | 0.5286221    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.97         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005343406 |\n",
      "|    clip_fraction        | 0.048       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.00252     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.84        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00385    |\n",
      "|    reward               | 0.34891865  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.83        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 233          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059879413 |\n",
      "|    clip_fraction        | 0.062        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.16        |\n",
      "|    explained_variance   | 0.00203      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.33         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00688     |\n",
      "|    reward               | 0.22320288   |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 3.97         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-04-04 to  2018-07-03\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_20\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 105       |\n",
      "|    time_elapsed    | 78        |\n",
      "|    total_timesteps | 8304      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 754       |\n",
      "|    critic_loss     | 952       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8203      |\n",
      "|    reward          | 3.0812137 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2018-04-04 to  2018-07-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-07-03\n",
      "======Trading from:  2018-07-03 to  2018-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-07-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_20\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 177          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.07        |\n",
      "|    explained_variance | 0.308        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -19.9        |\n",
      "|    reward             | -0.018817773 |\n",
      "|    std                | 0.996        |\n",
      "|    value_loss         | 5.1          |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | -0.755    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 5.5       |\n",
      "|    reward             | 1.0849805 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 2.68      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 184        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0.0165     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.78      |\n",
      "|    reward             | -1.2718247 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 3.53       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 185         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -14.9       |\n",
      "|    reward             | -0.33256537 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 4.17        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -9.88     |\n",
      "|    reward             | 0.6831054 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 5.23      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 7.53      |\n",
      "|    reward             | 1.6449331 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 1.4       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -20.2      |\n",
      "|    reward             | 0.31606427 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 7.35       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -0.113    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -21.4     |\n",
      "|    reward             | 0.2490029 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 9.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -3.22     |\n",
      "|    reward             | 1.9356681 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 0.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -20.3      |\n",
      "|    reward             | -2.2334301 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 7.13       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 21.9      |\n",
      "|    reward             | 0.8715655 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 13.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 189        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -19.7      |\n",
      "|    reward             | -1.6741682 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 5.05       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0.0165    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -14.8     |\n",
      "|    reward             | -3.110146 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 4.61      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -6.94     |\n",
      "|    reward             | 0.8958556 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 2.18      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 2.32      |\n",
      "|    reward             | 0.9103462 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 9.06      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 191         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 1.03        |\n",
      "|    reward             | -0.83288455 |\n",
      "|    std                | 0.979       |\n",
      "|    value_loss         | 1.99        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -23.1      |\n",
      "|    reward             | -1.4162283 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 25.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | -0.0117   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -7.13     |\n",
      "|    reward             | 1.8014612 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 2.3       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 1.36       |\n",
      "|    reward             | -1.4656087 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 0.304      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 194         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -27.7       |\n",
      "|    reward             | -0.94608885 |\n",
      "|    std                | 0.982       |\n",
      "|    value_loss         | 16.8        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2018-07-03 to  2018-10-02\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 295        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 6          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.28613734 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 280         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006244654 |\n",
      "|    clip_fraction        | 0.0512      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.0403     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.02        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00472    |\n",
      "|    reward               | 0.1491824   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 275          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045858677 |\n",
      "|    clip_fraction        | 0.0374       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.00064     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.41         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    reward               | 0.19680595   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 274         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007688133 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.000398    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00716    |\n",
      "|    reward               | -0.15493369 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.76        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005502414 |\n",
      "|    clip_fraction        | 0.0463      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | -0.00286    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.16        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0047     |\n",
      "|    reward               | 0.973624    |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 2.91        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2018-07-03 to  2018-10-02\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 121        |\n",
      "|    time_elapsed    | 70         |\n",
      "|    total_timesteps | 8556       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 1.22e+03   |\n",
      "|    critic_loss     | 705        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8455       |\n",
      "|    reward          | 0.27667612 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-07-03 to  2018-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-10-02\n",
      "======Trading from:  2018-10-02 to  2019-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_378_20\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 177        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0.202      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -12.5      |\n",
      "|    reward             | 0.13811709 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 3.36       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 1.86      |\n",
      "|    reward             | 1.1152552 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 1.85      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 185        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | -1.6411772 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 4.46       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 186         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -17.2       |\n",
      "|    reward             | -0.39431688 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 5.93        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 15.2       |\n",
      "|    reward             | -3.2926953 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 5.91       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 4.34       |\n",
      "|    reward             | 0.70532775 |\n",
      "|    std                | 0.972      |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 6.27       |\n",
      "|    reward             | -1.1176404 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 12.6       |\n",
      "|    reward             | 0.36933157 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 4.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -8.13     |\n",
      "|    reward             | 1.0847253 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 6.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -21.2     |\n",
      "|    reward             | -0.373722 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 12.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -13        |\n",
      "|    reward             | -0.7252936 |\n",
      "|    std                | 0.972      |\n",
      "|    value_loss         | 4.1        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -23.4     |\n",
      "|    reward             | 1.1544111 |\n",
      "|    std                | 0.967     |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.93      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -21.8      |\n",
      "|    reward             | 0.62987953 |\n",
      "|    std                | 0.968      |\n",
      "|    value_loss         | 17.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 189       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.9      |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 3.62      |\n",
      "|    reward             | -3.627869 |\n",
      "|    std                | 0.963     |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 39         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.92      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 5.85       |\n",
      "|    reward             | -1.2554992 |\n",
      "|    std                | 0.966      |\n",
      "|    value_loss         | 1.8        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 191         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 41          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.94       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 19.2        |\n",
      "|    reward             | -0.47398323 |\n",
      "|    std                | 0.97        |\n",
      "|    value_loss         | 11.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.94      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -21.3      |\n",
      "|    reward             | 0.19069451 |\n",
      "|    std                | 0.97       |\n",
      "|    value_loss         | 10.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 192         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.93       |\n",
      "|    explained_variance | 0.0032      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 4.03        |\n",
      "|    reward             | -0.10352377 |\n",
      "|    std                | 0.968       |\n",
      "|    value_loss         | 2.24        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.94      |\n",
      "|    explained_variance | 0.12       |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -6.65      |\n",
      "|    reward             | 0.07995703 |\n",
      "|    std                | 0.97       |\n",
      "|    value_loss         | 0.91       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 194         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -15.9       |\n",
      "|    reward             | -0.21749003 |\n",
      "|    std                | 0.97        |\n",
      "|    value_loss         | 5.91        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2018-10-02 to  2019-01-03\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_378_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 283        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 7          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.06442951 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006791838 |\n",
      "|    clip_fraction        | 0.0546      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.145      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.62        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00393    |\n",
      "|    reward               | 0.045203507 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066147065 |\n",
      "|    clip_fraction        | 0.0797       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.0348       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.91         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00534     |\n",
      "|    reward               | -0.41377547  |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 4.18         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 268          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072443984 |\n",
      "|    clip_fraction        | 0.0783       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | 0.0379       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.37         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00672     |\n",
      "|    reward               | 0.08837293   |\n",
      "|    std                  | 0.993        |\n",
      "|    value_loss           | 3.82         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061307494 |\n",
      "|    clip_fraction        | 0.0704       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.06        |\n",
      "|    explained_variance   | -0.00126     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00546     |\n",
      "|    reward               | 0.09273759   |\n",
      "|    std                  | 0.992        |\n",
      "|    value_loss           | 3.68         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-10-02 to  2019-01-03\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 121        |\n",
      "|    time_elapsed    | 72         |\n",
      "|    total_timesteps | 8808       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 1.04e+03   |\n",
      "|    critic_loss     | 68.8       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8707       |\n",
      "|    reward          | 0.92797613 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-10-02 to  2019-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-01-03\n",
      "======Trading from:  2019-01-03 to  2019-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_441_20\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 178          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.14        |\n",
      "|    explained_variance | -0.444       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -8.73        |\n",
      "|    reward             | -0.022668017 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 2.26         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.177     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.437     |\n",
      "|    reward             | 0.05080854 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 183        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.151     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.62      |\n",
      "|    reward             | -1.6713752 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 185      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.1     |\n",
      "|    explained_variance | -1.02    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -1.81    |\n",
      "|    reward             | 0.641366 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.309    |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 186        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 8.72       |\n",
      "|    reward             | 0.13901626 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 1.94       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 187         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -1.07       |\n",
      "|    reward             | -0.09038242 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 0.503       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 187      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 2.79     |\n",
      "|    reward             | -1.21175 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 0.242    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 11.5      |\n",
      "|    reward             | 0.4305188 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 2.49      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.00548   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 9.62       |\n",
      "|    reward             | 0.82308537 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 7.91       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0.0358    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 0.678     |\n",
      "|    reward             | 2.4423018 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 3.4       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.000561  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -0.406    |\n",
      "|    reward             | 1.5713152 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 0.217     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 188         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 16.3        |\n",
      "|    reward             | -0.46984288 |\n",
      "|    std                | 0.983       |\n",
      "|    value_loss         | 7.66        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 8.4        |\n",
      "|    reward             | -0.7927216 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 2.61       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 188         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 37          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0.0671      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -8.69       |\n",
      "|    reward             | 0.050279457 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 1.44        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 189       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.00327   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -12.4     |\n",
      "|    reward             | -0.074954 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 5.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -0.00203   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -18.3      |\n",
      "|    reward             | 0.15255211 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 7.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -6.52      |\n",
      "|    reward             | -1.8816189 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 1.53       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -0.0207    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -5.99      |\n",
      "|    reward             | -1.6878643 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 2.43       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -0.0212    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -29.8      |\n",
      "|    reward             | -2.2616456 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 29.3       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 193         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 51          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0.00512     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -6.05       |\n",
      "|    reward             | -0.66067934 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 2           |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2019-01-03 to  2019-04-04\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_441_20\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 286          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 7            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.025739908 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 267          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074353237 |\n",
      "|    clip_fraction        | 0.0549       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.155       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.52         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00684     |\n",
      "|    reward               | -0.27652     |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.62         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008485351 |\n",
      "|    clip_fraction        | 0.085       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0185     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.59        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00803    |\n",
      "|    reward               | 0.21579462  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.56        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005503406 |\n",
      "|    clip_fraction        | 0.0383      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00632    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.04        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00379    |\n",
      "|    reward               | -0.5739096  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.99        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 266          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 38           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049333777 |\n",
      "|    clip_fraction        | 0.0342       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.00856     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2            |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00346     |\n",
      "|    reward               | 0.30464834   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.56         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-01-03 to  2019-04-04\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_20\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 121      |\n",
      "|    time_elapsed    | 74       |\n",
      "|    total_timesteps | 9060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 824      |\n",
      "|    critic_loss     | 3.06e+03 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8959     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2019-01-03 to  2019-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-04-04\n",
      "======Trading from:  2019-04-04 to  2019-07-05\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_504_20\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 176           |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 2             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -7.03         |\n",
      "|    explained_variance | 0.187         |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -9.67         |\n",
      "|    reward             | -0.0056212586 |\n",
      "|    std                | 0.986         |\n",
      "|    value_loss         | 2.01          |\n",
      "-----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 179        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -3.27      |\n",
      "|    reward             | 0.73492336 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 1.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.47      |\n",
      "|    reward             | -2.3447402 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 3.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 183        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0.2        |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.98      |\n",
      "|    reward             | 0.21091846 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 0.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 184        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 15.8       |\n",
      "|    reward             | 0.44965118 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.88       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 184       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -2.8      |\n",
      "|    reward             | 0.6134379 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 0.484     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 185       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 4.08      |\n",
      "|    reward             | 0.5708434 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 186        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -1.92      |\n",
      "|    reward             | -0.9834309 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.258      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -19.3     |\n",
      "|    reward             | 0.5916742 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 7.69      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 186         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -3          |\n",
      "|    reward             | -0.72459424 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 1.1         |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 186          |\n",
      "|    iterations         | 1100         |\n",
      "|    time_elapsed       | 29           |\n",
      "|    total_timesteps    | 5500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.07        |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1099         |\n",
      "|    policy_loss        | 9.57         |\n",
      "|    reward             | 0.0069448976 |\n",
      "|    std                | 0.995        |\n",
      "|    value_loss         | 2.91         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -23.2     |\n",
      "|    reward             | 0.7369314 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 8.24      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 24.6      |\n",
      "|    reward             | 0.2140397 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -5.77      |\n",
      "|    reward             | 0.63993466 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.982      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 187          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 40           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.09        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | 7.55         |\n",
      "|    reward             | -0.034323744 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 1.56         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 1.95       |\n",
      "|    reward             | 0.12961224 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.509      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -42.6     |\n",
      "|    reward             | 2.0418048 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 44.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 189       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -2.86     |\n",
      "|    reward             | 2.4586885 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.554     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 6.87       |\n",
      "|    reward             | 0.43931425 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 1.9        |\n",
      "|    reward             | -1.3617043 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-04-04 to  2019-07-05\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_504_20\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 282         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.10110947 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006116178 |\n",
      "|    clip_fraction        | 0.0579      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.209      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.4         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00678    |\n",
      "|    reward               | 0.06959491  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.8         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 260          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053397217 |\n",
      "|    clip_fraction        | 0.0401       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.00445     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.01         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0045      |\n",
      "|    reward               | 0.8306992    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.83         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 261         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004777708 |\n",
      "|    clip_fraction        | 0.0376      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00251    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00434    |\n",
      "|    reward               | 1.6939876   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.85        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 262          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057541355 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.15        |\n",
      "|    explained_variance   | -0.00454     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.11         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    reward               | -0.025543556 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 5.11         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-04-04 to  2019-07-05\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_20\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 121       |\n",
      "|    time_elapsed    | 76        |\n",
      "|    total_timesteps | 9312      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 337       |\n",
      "|    critic_loss     | 31.5      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9211      |\n",
      "|    reward          | 0.6296957 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-04-04 to  2019-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-07-05\n",
      "======Trading from:  2019-07-05 to  2019-10-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_567_20\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 179          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.09        |\n",
      "|    explained_variance | -0.00208     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -8.74        |\n",
      "|    reward             | -0.021509662 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 2.19         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 177       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.586    |\n",
      "|    reward             | 1.3954455 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 180        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -2.66      |\n",
      "|    reward             | -1.9252071 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -14         |\n",
      "|    reward             | 0.004040964 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 5.92        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 184       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0.015     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 2.48      |\n",
      "|    reward             | 2.5010402 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.14      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 185         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -0.15       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -2.88       |\n",
      "|    reward             | -0.14522448 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 0.657       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 185        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -0.135     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 6.12       |\n",
      "|    reward             | 0.09389397 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.708      |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 186      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | -0.0391  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -3.82    |\n",
      "|    reward             | 0.764725 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 0.773    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 1.362038  |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 3.85      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 187         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 13          |\n",
      "|    reward             | -0.85487014 |\n",
      "|    std                | 0.981       |\n",
      "|    value_loss         | 4.11        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.0629    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 6.68       |\n",
      "|    reward             | -1.1617385 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 1.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 29.2       |\n",
      "|    reward             | -0.5573557 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 17.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 188         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 3.01        |\n",
      "|    reward             | -0.74473983 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 0.496       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 38.4       |\n",
      "|    reward             | -10.310818 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 36.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -6.97     |\n",
      "|    reward             | 2.3486204 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 2.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 42         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 15.8       |\n",
      "|    reward             | -1.6281672 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 6.76       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 189         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 44          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -1.47       |\n",
      "|    reward             | -0.46360636 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 1.27        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 16         |\n",
      "|    reward             | 0.60919607 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 6.17       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -17.6     |\n",
      "|    reward             | 1.9728003 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 19.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 52        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 11.4      |\n",
      "|    reward             | 1.4521623 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 6.19      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2019-07-05 to  2019-10-03\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_567_20\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 278         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.09312055 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 261          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0095989015 |\n",
      "|    clip_fraction        | 0.0849       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0351      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00759     |\n",
      "|    reward               | -0.8564273   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.35         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 256          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046677003 |\n",
      "|    clip_fraction        | 0.0351       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.001       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.08         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00328     |\n",
      "|    reward               | 0.30569178   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.96         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 31          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00777111  |\n",
      "|    clip_fraction        | 0.0472      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.00296     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    reward               | 0.015255962 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.01        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004967271 |\n",
      "|    clip_fraction        | 0.03        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.000239    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.82        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00373    |\n",
      "|    reward               | 0.34475142  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.21        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-07-05 to  2019-10-03\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_20\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 122       |\n",
      "|    time_elapsed    | 78        |\n",
      "|    total_timesteps | 9564      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -1.69e+03 |\n",
      "|    critic_loss     | 94.9      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9463      |\n",
      "|    reward          | 1.4863836 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-07-05 to  2019-10-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-10-03\n",
      "======Trading from:  2019-10-03 to  2020-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-10-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_630_20\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -1.91e-06  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | 0.40739152 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 3.78       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -3.67      |\n",
      "|    reward             | 0.80358064 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 1.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.66      |\n",
      "|    reward             | -2.5894203 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 3.77       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.13      |\n",
      "|    reward             | 0.34196973 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 0.99       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 4.54      |\n",
      "|    reward             | 0.3592033 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 0.853     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 190         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.452      |\n",
      "|    reward             | -0.22804008 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 0.0177      |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 190         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | -0.0219     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 3.2         |\n",
      "|    reward             | -0.87479013 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 0.325       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | -0.017    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 24.4      |\n",
      "|    reward             | 1.3021446 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 7.27      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 190         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0.00361     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -10.8       |\n",
      "|    reward             | -0.09597891 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 8.13        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 189       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -0.0212   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -4.55     |\n",
      "|    reward             | 0.2853671 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 12.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 189        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -5.58      |\n",
      "|    reward             | -0.5210189 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 0.715      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 189         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 1.75        |\n",
      "|    reward             | -0.51313144 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 1.46        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 34        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -13.7     |\n",
      "|    reward             | 1.6469692 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 4.36      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 188      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 7.94     |\n",
      "|    reward             | 4.482658 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 10.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 39        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 10.2      |\n",
      "|    reward             | 2.5339093 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 2.91      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 188         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 42          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -6.94       |\n",
      "|    reward             | -0.49925026 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 2.22        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 2.43      |\n",
      "|    reward             | 1.0794555 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.73      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 47         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 7.91       |\n",
      "|    reward             | 0.76424074 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 25.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 189      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 50       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -61.9    |\n",
      "|    reward             | 2.484993 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 80.2     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 12.2       |\n",
      "|    reward             | -0.4941439 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 4.23       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-10-03 to  2020-01-03\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_630_20\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 283          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 7            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | 0.0110432245 |\n",
      "-------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 261          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072394665 |\n",
      "|    clip_fraction        | 0.0607       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0558      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.94         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    reward               | -0.22106707  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.97         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065498734 |\n",
      "|    clip_fraction        | 0.0451       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.00427     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.29         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    reward               | -0.79222345  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.31         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007598513 |\n",
      "|    clip_fraction        | 0.0745      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.00389     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.7         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00649    |\n",
      "|    reward               | 1.1009452   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 40           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048472774 |\n",
      "|    clip_fraction        | 0.0469       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.000688    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.78         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00338     |\n",
      "|    reward               | -0.9311503   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.88         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-10-03 to  2020-01-03\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_20\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 121       |\n",
      "|    time_elapsed    | 81        |\n",
      "|    total_timesteps | 9816      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 556       |\n",
      "|    critic_loss     | 9.22      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9715      |\n",
      "|    reward          | -4.820445 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-10-03 to  2020-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-01-03\n",
      "======Trading from:  2020-01-03 to  2020-04-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_693_20\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 185       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -0.042    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -9.08     |\n",
      "|    reward             | 0.1313252 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.04      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 190       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.842    |\n",
      "|    reward             | 0.3887014 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.739     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0.0104     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -7.55      |\n",
      "|    reward             | -1.7644812 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.44      |\n",
      "|    reward             | 0.20004985 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.607      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | -0.5899805 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 2.79       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 196         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -32.3       |\n",
      "|    reward             | -0.20896442 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 21.9        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -6.93      |\n",
      "|    reward             | 0.39379027 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 194         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -5.63       |\n",
      "|    reward             | -0.25614238 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 5.61        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -5.69     |\n",
      "|    reward             | 2.4863057 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.931     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 7.86       |\n",
      "|    reward             | -0.1298171 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 4.44       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -6.97      |\n",
      "|    reward             | 0.39650396 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 3.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.72      |\n",
      "|    reward             | 1.7931399 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.132     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -0.301     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -1.75      |\n",
      "|    reward             | -1.5530804 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 1.34      |\n",
      "|    reward             | 0.3008001 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 0.0955    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 191      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7       |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 17.9     |\n",
      "|    reward             | 2.218196 |\n",
      "|    std                | 0.981    |\n",
      "|    value_loss         | 13.6     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | -0.0115    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 26.5       |\n",
      "|    reward             | 0.43828768 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 13.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 44        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -8.56     |\n",
      "|    reward             | 3.2473676 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 1.64      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -46        |\n",
      "|    reward             | -1.0686356 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 45.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -7.86     |\n",
      "|    reward             | 1.5917252 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 1.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 191        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -2.04      |\n",
      "|    reward             | -1.8017821 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 0.411      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-01-03 to  2020-04-03\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_693_20\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 282          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 7            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.041539796 |\n",
      "-------------------------------------\n",
      "day: 2516, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 569370.48\n",
      "total_reward: -430629.52\n",
      "total_cost: 1154056.29\n",
      "total_trades: 9658\n",
      "Sharpe: -0.314\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 260         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007953495 |\n",
      "|    clip_fraction        | 0.0929      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.112      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.9         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0066     |\n",
      "|    reward               | 1.1575936   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.19        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 252          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066770134 |\n",
      "|    clip_fraction        | 0.038        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | 0.0121       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00319     |\n",
      "|    reward               | -0.012559762 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.62         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007429175 |\n",
      "|    clip_fraction        | 0.0619      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00522    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.8         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0063     |\n",
      "|    reward               | 0.35213184  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.55        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 251        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 40         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00704519 |\n",
      "|    clip_fraction        | 0.0718     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.11      |\n",
      "|    explained_variance   | 0.0141     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.59       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00498   |\n",
      "|    reward               | 0.02471468 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 2.86       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2020-01-03 to  2020-04-03\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_20\n",
      "day: 2516, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3072114.51\n",
      "total_reward: 2072114.51\n",
      "total_cost: 47441.24\n",
      "total_trades: 10092\n",
      "Sharpe: 0.790\n",
      "=================================\n",
      "======DDPG Validation from:  2020-01-03 to  2020-04-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-04-03\n",
      "======Trading from:  2020-04-03 to  2020-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-04-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_756_20\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0.113      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | 0.03674631 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 1.98       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 2.58      |\n",
      "|    reward             | 1.0822487 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 1.75      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -12        |\n",
      "|    reward             | -1.6418993 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 4.66       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 197         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0.0357      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -11.5       |\n",
      "|    reward             | -0.09128781 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 2.78        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0.181      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -8.1       |\n",
      "|    reward             | -0.3890813 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 2.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 7.97      |\n",
      "|    reward             | -2.649511 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 6.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 10.6      |\n",
      "|    reward             | 1.4212991 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 3.02      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | -9.4      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 8.28      |\n",
      "|    reward             | -1.408999 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 4.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -25.8     |\n",
      "|    reward             | -0.170929 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 22.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 196         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 17.4        |\n",
      "|    reward             | -0.86171526 |\n",
      "|    std                | 0.983       |\n",
      "|    value_loss         | 17.7        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -14.6     |\n",
      "|    reward             | 1.9428998 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 6.3       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 7.55      |\n",
      "|    reward             | 1.1885669 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 2.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 2.41      |\n",
      "|    reward             | 1.1794411 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 0.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 4.9       |\n",
      "|    reward             | 1.2316394 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 2.79      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 193      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7       |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -1.3     |\n",
      "|    reward             | 3.697369 |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 1.55     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 193          |\n",
      "|    iterations         | 1600         |\n",
      "|    time_elapsed       | 41           |\n",
      "|    total_timesteps    | 8000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.98        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1599         |\n",
      "|    policy_loss        | -1.97        |\n",
      "|    reward             | -0.078308694 |\n",
      "|    std                | 0.978        |\n",
      "|    value_loss         | 0.394        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 20.5       |\n",
      "|    reward             | 0.41448575 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 13.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | -2.8439772 |\n",
      "|    std                | 0.974      |\n",
      "|    value_loss         | 3.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 3.61       |\n",
      "|    reward             | 0.33841407 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 1.8        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 192      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.99    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -38.7    |\n",
      "|    reward             | 9.056162 |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 63.3     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-04-03 to  2020-07-06\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_756_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 274        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 7          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.18060346 |\n",
      "-----------------------------------\n",
      "day: 2579, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 297480.80\n",
      "total_reward: -702519.20\n",
      "total_cost: 991234.12\n",
      "total_trades: 9735\n",
      "Sharpe: -0.639\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 258         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005680442 |\n",
      "|    clip_fraction        | 0.0701      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0249     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00502    |\n",
      "|    reward               | 0.5324478   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.99        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 253          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054502394 |\n",
      "|    clip_fraction        | 0.0398       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0199      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00414     |\n",
      "|    reward               | -0.014574211 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.98         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 250          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 32           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065572257 |\n",
      "|    clip_fraction        | 0.0699       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.00927     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.8          |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0074      |\n",
      "|    reward               | 1.4939734    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.69         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 248          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072969333 |\n",
      "|    clip_fraction        | 0.0787       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | 0.00208      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.77         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00564     |\n",
      "|    reward               | -0.2511924   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.21         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2020-04-03 to  2020-07-06\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_20\n",
      "day: 2579, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1768594.55\n",
      "total_reward: 768594.55\n",
      "total_cost: 1001.06\n",
      "total_trades: 7739\n",
      "Sharpe: 0.404\n",
      "=================================\n",
      "======DDPG Validation from:  2020-04-03 to  2020-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-07-06\n",
      "======Trading from:  2020-07-06 to  2020-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_819_20\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 189          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -10.2        |\n",
      "|    reward             | -0.031895336 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 2.95         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 187       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0.00447   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -7.54     |\n",
      "|    reward             | 0.7392962 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 192        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.0753    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.68      |\n",
      "|    reward             | -2.1720543 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -0.0345   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2         |\n",
      "|    reward             | 0.7996798 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.65      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -16.6       |\n",
      "|    reward             | -0.73435503 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 5.19        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.226    |\n",
      "|    reward             | -0.723811 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.03      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -9.66      |\n",
      "|    reward             | -2.0357146 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 2.23      |\n",
      "|    reward             | 1.024164  |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 0.455     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -4.89     |\n",
      "|    reward             | 1.1009288 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 0.667     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -15.6     |\n",
      "|    reward             | 1.3935735 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 9.82      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 196         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -2.23       |\n",
      "|    reward             | -0.33380407 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.212       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -18.1      |\n",
      "|    reward             | -1.1442854 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 9.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 8.5        |\n",
      "|    reward             | -0.3475959 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 4.23       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -9.84     |\n",
      "|    reward             | 0.2587471 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 2.06      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -61.3      |\n",
      "|    reward             | -2.0778599 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 56.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.0071    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 3.85       |\n",
      "|    reward             | -1.8230405 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.435      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.00455   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 5.16       |\n",
      "|    reward             | -1.9590021 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.694      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.00128   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 12.3       |\n",
      "|    reward             | -1.4498965 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.4        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.00397   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -10.8      |\n",
      "|    reward             | 0.16313899 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.7        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -11.5      |\n",
      "|    reward             | -1.6647815 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 2.6        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-07-06 to  2020-10-02\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_819_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 265        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 7          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.10113501 |\n",
      "-----------------------------------\n",
      "day: 2642, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 555783.87\n",
      "total_reward: -444216.13\n",
      "total_cost: 1202359.52\n",
      "total_trades: 10149\n",
      "Sharpe: -0.243\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 254          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047372947 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0292      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.43         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    reward               | 0.07896659   |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.89         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 250         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007124788 |\n",
      "|    clip_fraction        | 0.0787      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.07       |\n",
      "|    explained_variance   | 0.0165      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00712    |\n",
      "|    reward               | 0.5110755   |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 247         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006022102 |\n",
      "|    clip_fraction        | 0.0449      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.05       |\n",
      "|    explained_variance   | 0.0021      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.78        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    reward               | -1.4903663  |\n",
      "|    std                  | 0.993       |\n",
      "|    value_loss           | 4.35        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005418136 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.06       |\n",
      "|    explained_variance   | 0.0246      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 0.924       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00508    |\n",
      "|    reward               | -0.01726897 |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 3.25        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-06 to  2020-10-02\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_20\n",
      "day: 2642, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2233883.57\n",
      "total_reward: 1233883.57\n",
      "total_cost: 998.65\n",
      "total_trades: 13210\n",
      "Sharpe: 0.515\n",
      "=================================\n",
      "======DDPG Validation from:  2020-07-06 to  2020-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-10-02\n",
      "======Trading from:  2020-10-02 to  2021-01-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_882_20\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 190         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.3       |\n",
      "|    reward             | 0.015532292 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 3.44        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 191       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0.449     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.422    |\n",
      "|    reward             | 1.3656377 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.904     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0.311      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.92      |\n",
      "|    reward             | -2.4219577 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 3.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -20.8      |\n",
      "|    reward             | -0.4132862 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 11.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -14.9      |\n",
      "|    reward             | -1.1356914 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 9.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.347     |\n",
      "|    reward             | -0.6060918 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.77       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -6.34      |\n",
      "|    reward             | 0.39941338 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.74       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 198         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 14.3        |\n",
      "|    reward             | -0.04288289 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 7.88        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 14.3      |\n",
      "|    reward             | 0.9894049 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.98      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 199         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -15.4       |\n",
      "|    reward             | 0.046948366 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 9.1         |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 198         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -23.5       |\n",
      "|    reward             | -0.08985835 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 7.73        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 11.9       |\n",
      "|    reward             | -0.9451299 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.93       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 196          |\n",
      "|    iterations         | 1300         |\n",
      "|    time_elapsed       | 33           |\n",
      "|    total_timesteps    | 6500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.13        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1299         |\n",
      "|    policy_loss        | 2.49         |\n",
      "|    reward             | -0.023723986 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.233        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 9.93       |\n",
      "|    reward             | 0.34368128 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.39       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 8.14       |\n",
      "|    reward             | -2.0208201 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.36       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 41        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0.000288  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 119       |\n",
      "|    reward             | 1.4024702 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 295       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 26.6       |\n",
      "|    reward             | -2.1193092 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 13.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 2.86      |\n",
      "|    reward             | 0.7464423 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 1.46      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -7.28     |\n",
      "|    reward             | 1.2463849 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.54      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 193       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 8.66      |\n",
      "|    reward             | 0.0670687 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 1.56      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2020-10-02 to  2021-01-04\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_882_20\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 257          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 7            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.052111264 |\n",
      "-------------------------------------\n",
      "day: 2705, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 465345.78\n",
      "total_reward: -534654.22\n",
      "total_cost: 1178687.16\n",
      "total_trades: 10317\n",
      "Sharpe: -0.336\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 247          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068322914 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.000328    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00596     |\n",
      "|    reward               | 0.17009027   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.23         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 246         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005400379 |\n",
      "|    clip_fraction        | 0.0447      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.14       |\n",
      "|    explained_variance   | 0.000398    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0039     |\n",
      "|    reward               | 0.3138377   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.31        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 244         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009568314 |\n",
      "|    clip_fraction        | 0.079       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.16       |\n",
      "|    explained_variance   | 0.028       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00764    |\n",
      "|    reward               | 0.041422438 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.17        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064025903 |\n",
      "|    clip_fraction        | 0.0581       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.17        |\n",
      "|    explained_variance   | 0.0625       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00597     |\n",
      "|    reward               | 0.17185554   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.08         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2020-10-02 to  2021-01-04\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_20\n",
      "day: 2705, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2480344.87\n",
      "total_reward: 1480344.87\n",
      "total_cost: 997.55\n",
      "total_trades: 5410\n",
      "Sharpe: 0.564\n",
      "=================================\n",
      "======DDPG Validation from:  2020-10-02 to  2021-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-01-04\n",
      "======Trading from:  2021-01-04 to  2021-04-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_945_20\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 191         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -11.3       |\n",
      "|    reward             | 0.046977364 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 3.05        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 186       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.37     |\n",
      "|    reward             | 0.6435414 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 0.929     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -0.308     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.29      |\n",
      "|    reward             | -1.4563758 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 2.87       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 192       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0.418     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -6.01     |\n",
      "|    reward             | 0.2487126 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 0.96      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 193         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0.0122      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -6.48       |\n",
      "|    reward             | -0.48500538 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 1.47        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 11.7      |\n",
      "|    reward             | 1.2865671 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 2.42      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 12.4      |\n",
      "|    reward             | 1.0330553 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 5.08      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 5.53      |\n",
      "|    reward             | 0.4761942 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 35.9      |\n",
      "|    reward             | 0.8114863 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 36.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 15.2      |\n",
      "|    reward             | -8.797472 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 40.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0.00397    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -129       |\n",
      "|    reward             | 0.18235312 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 336        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -1.89      |\n",
      "|    reward             | -3.2916229 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 2.43       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 15.1      |\n",
      "|    reward             | 0.5763128 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 6.27      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 13.2      |\n",
      "|    reward             | 3.4415312 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 4         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 3.34       |\n",
      "|    reward             | 0.12326919 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 0.575      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0.0291    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -17.8     |\n",
      "|    reward             | 2.0421734 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 9.96      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 194       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0.0707    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 3.09      |\n",
      "|    reward             | 1.3697606 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 0.367     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 46         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.372     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -17.3      |\n",
      "|    reward             | 0.17327099 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 4.8        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 193         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | 0.00196     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -11.3       |\n",
      "|    reward             | -0.59219205 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 5.03        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0.00747    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -7         |\n",
      "|    reward             | -0.9646699 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 1.49       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_945_20\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 256         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.15820597 |\n",
      "------------------------------------\n",
      "day: 2768, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 609478.33\n",
      "total_reward: -390521.67\n",
      "total_cost: 1292538.84\n",
      "total_trades: 10686\n",
      "Sharpe: -0.171\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 244          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 16           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044057327 |\n",
      "|    clip_fraction        | 0.0485       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.159       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.05         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00533     |\n",
      "|    reward               | 0.22475962   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.75         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069342116 |\n",
      "|    clip_fraction        | 0.0779       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.00959     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.34         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0067      |\n",
      "|    reward               | -1.3310738   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.44         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 242         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 33          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007815616 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | -0.0141     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.01        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00876    |\n",
      "|    reward               | 0.26841202  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.12        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006750419  |\n",
      "|    clip_fraction        | 0.07         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | 0.00792      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.18         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00492     |\n",
      "|    reward               | -0.018374864 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.08         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_20\n",
      "day: 2768, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4881251.83\n",
      "total_reward: 3881251.83\n",
      "total_cost: 998.37\n",
      "total_trades: 5536\n",
      "Sharpe: 0.863\n",
      "=================================\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1008_20\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 188        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -0.0423    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -12.3      |\n",
      "|    reward             | 0.12343416 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 3.56       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 188       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0.0489    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.0391   |\n",
      "|    reward             | 0.7820228 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 0.143     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.99      |\n",
      "|    reward             | -1.3030214 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 189        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -1.15      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -21        |\n",
      "|    reward             | -0.1380408 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 7.59       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 192         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0.00112     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -8.73       |\n",
      "|    reward             | -0.56823236 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 2.43        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -11.6     |\n",
      "|    reward             | 0.9824047 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 4.33      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 8.53       |\n",
      "|    reward             | 0.14347588 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 2.16       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 23.4       |\n",
      "|    reward             | 0.79149234 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 4.23      |\n",
      "|    reward             | 0.4837183 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 1.09      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 30.8      |\n",
      "|    reward             | 2.1546855 |\n",
      "|    std                | 0.978     |\n",
      "|    value_loss         | 26.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 83.2       |\n",
      "|    reward             | -1.7943069 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 208        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 15.7      |\n",
      "|    reward             | -0.585149 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 7.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 19.1       |\n",
      "|    reward             | -1.2931828 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 5.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 0.436      |\n",
      "|    reward             | -1.3877914 |\n",
      "|    std                | 0.973      |\n",
      "|    value_loss         | 1.8        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 198      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.99    |\n",
      "|    explained_variance | -0.00465 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -12.2    |\n",
      "|    reward             | 2.3435   |\n",
      "|    std                | 0.98     |\n",
      "|    value_loss         | 4.05     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.98     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -1.69     |\n",
      "|    reward             | 1.0560014 |\n",
      "|    std                | 0.977     |\n",
      "|    value_loss         | 2.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 43         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 138        |\n",
      "|    reward             | 0.34730592 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 506        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 196      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 45       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 16.6     |\n",
      "|    reward             | 1.748868 |\n",
      "|    std                | 0.973    |\n",
      "|    value_loss         | 5.71     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.93       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 18.4        |\n",
      "|    reward             | -0.10036524 |\n",
      "|    std                | 0.969       |\n",
      "|    value_loss         | 6.43        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 51        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.92     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 18.5      |\n",
      "|    reward             | 1.8824531 |\n",
      "|    std                | 0.966     |\n",
      "|    value_loss         | 12        |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1008_20\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 251          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 8            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.039245322 |\n",
      "-------------------------------------\n",
      "day: 2831, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 775999.32\n",
      "total_reward: -224000.68\n",
      "total_cost: 1380912.90\n",
      "total_trades: 10787\n",
      "Sharpe: -0.045\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 239         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007849584 |\n",
      "|    clip_fraction        | 0.0835      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0623     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.69        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00733    |\n",
      "|    reward               | -0.5996013  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 238         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009213699 |\n",
      "|    clip_fraction        | 0.0909      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.69        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00681    |\n",
      "|    reward               | 1.0469644   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.35        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0071254843 |\n",
      "|    clip_fraction        | 0.0729       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.0286       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00741     |\n",
      "|    reward               | -0.30920267  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.77         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 240        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 42         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00838118 |\n",
      "|    clip_fraction        | 0.0841     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.14      |\n",
      "|    explained_variance   | -0.0172    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 0.765      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00768   |\n",
      "|    reward               | 0.38638902 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 1.91       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1008_20\n",
      "day: 2831, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4939420.38\n",
      "total_reward: 3939420.38\n",
      "total_cost: 998.81\n",
      "total_trades: 8493\n",
      "Sharpe: 0.835\n",
      "=================================\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1071_20\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 191         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -0.00578    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -7.69       |\n",
      "|    reward             | -0.10060598 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 1.76        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0.0556     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.25      |\n",
      "|    reward             | 0.42719817 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 0.496      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 187        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0.0349     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -5.84      |\n",
      "|    reward             | -1.3291776 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 1.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 190        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -1.48      |\n",
      "|    reward             | 0.24954464 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 0.187      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 191         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -6.01       |\n",
      "|    reward             | -0.27439174 |\n",
      "|    std                | 0.99        |\n",
      "|    value_loss         | 0.838       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 193        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 18.5       |\n",
      "|    reward             | -3.5376337 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 7.89       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 195      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -5.28    |\n",
      "|    reward             | 1.095896 |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 2.4      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -3.74      |\n",
      "|    reward             | 0.52407295 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 0.998      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -1.51     |\n",
      "|    reward             | -0.636794 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 0.427     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 2.19       |\n",
      "|    reward             | -1.1331686 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 0.521      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 197         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -29.2       |\n",
      "|    reward             | -0.95212877 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 19.4        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.488     |\n",
      "|    reward             | 0.9661916 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 0.248     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 9.36       |\n",
      "|    reward             | 0.41590142 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 2.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 35        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -12.8     |\n",
      "|    reward             | 1.5564009 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 5.6       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 37        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -11.7     |\n",
      "|    reward             | 1.0268043 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 3.21      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -10.5     |\n",
      "|    reward             | 1.2314882 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 2.89      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 197      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 43       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 11.6     |\n",
      "|    reward             | 2.031025 |\n",
      "|    std                | 0.996    |\n",
      "|    value_loss         | 6.11     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 196        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 45         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 0.325      |\n",
      "|    reward             | 0.18902549 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 0.216      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 48          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -4.28       |\n",
      "|    reward             | 0.053522144 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 0.449       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 2.81       |\n",
      "|    reward             | -0.4266301 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 1.31       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1071_20\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 257         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 7           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.056684624 |\n",
      "------------------------------------\n",
      "day: 2894, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 506381.52\n",
      "total_reward: -493618.48\n",
      "total_cost: 1176403.34\n",
      "total_trades: 11038\n",
      "Sharpe: -0.271\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 239          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064539867 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.111       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0054      |\n",
      "|    reward               | -1.3211141   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.16         |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 234       |\n",
      "|    iterations           | 3         |\n",
      "|    time_elapsed         | 26        |\n",
      "|    total_timesteps      | 6144      |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0093547 |\n",
      "|    clip_fraction        | 0.0802    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -7.12     |\n",
      "|    explained_variance   | -0.00246  |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 1.98      |\n",
      "|    n_updates            | 20        |\n",
      "|    policy_gradient_loss | -0.0103   |\n",
      "|    reward               | 1.2296642 |\n",
      "|    std                  | 1.01      |\n",
      "|    value_loss           | 3.92      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008258808 |\n",
      "|    clip_fraction        | 0.0697      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.0147      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.55        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00679    |\n",
      "|    reward               | 0.30842158  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.71        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 236         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008081699 |\n",
      "|    clip_fraction        | 0.0695      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0322      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.49        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00592    |\n",
      "|    reward               | 0.28979853  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.13        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1071_20\n",
      "day: 2894, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2449548.32\n",
      "total_reward: 1449548.32\n",
      "total_cost: 1402.57\n",
      "total_trades: 5792\n",
      "Sharpe: 0.522\n",
      "=================================\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1134_20\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 207        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.6      |\n",
      "|    reward             | 0.25649828 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 4.39       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.26      |\n",
      "|    reward             | 1.5723149 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 1.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 194        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.62      |\n",
      "|    reward             | -2.4688828 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 4.77       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 193         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | -0.00159    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -19.3       |\n",
      "|    reward             | -0.44533285 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 11.2        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 195        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | -0.00484   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -19.3      |\n",
      "|    reward             | -1.2026167 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 195       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -1.05     |\n",
      "|    reward             | 1.4773377 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 0.313     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.295    |\n",
      "|    reward             | 0.8940442 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 0.372     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 197         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 19.3        |\n",
      "|    reward             | -0.23202462 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 8.22        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -0.000805  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -25        |\n",
      "|    reward             | 0.72120386 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 27.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 197       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0.000197  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -54.5     |\n",
      "|    reward             | 1.3282468 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 107       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 4.3691354 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 8.29      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.0427    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -28.7      |\n",
      "|    reward             | -0.6496847 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 15.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.474     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -1.04      |\n",
      "|    reward             | 0.50489134 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 0.626      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.00125   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 11.6       |\n",
      "|    reward             | -2.7292418 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 3.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 198        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -0.0751    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 18.8       |\n",
      "|    reward             | 0.25121045 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 7.85       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 199       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 40        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -5.43     |\n",
      "|    reward             | 1.4863882 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 4.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 198       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | 0.0001    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -60.8     |\n",
      "|    reward             | 2.6649072 |\n",
      "|    std                | 0.971     |\n",
      "|    value_loss         | 48.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 198         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.96       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -10.4       |\n",
      "|    reward             | 0.089445315 |\n",
      "|    std                | 0.973       |\n",
      "|    value_loss         | 4.51        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 197        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.94      |\n",
      "|    explained_variance | 0.000641   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 10.9       |\n",
      "|    reward             | 0.55703163 |\n",
      "|    std                | 0.969      |\n",
      "|    value_loss         | 4.59       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 196       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 8.54      |\n",
      "|    reward             | 0.9379386 |\n",
      "|    std                | 0.972     |\n",
      "|    value_loss         | 1.55      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1134_20\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 256       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 7         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.1342167 |\n",
      "----------------------------------\n",
      "day: 2957, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 600961.70\n",
      "total_reward: -399038.30\n",
      "total_cost: 1323669.05\n",
      "total_trades: 11292\n",
      "Sharpe: -0.158\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 237         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006991473 |\n",
      "|    clip_fraction        | 0.0622      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0101     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.27        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00609    |\n",
      "|    reward               | 0.29437813  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 232         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006848396 |\n",
      "|    clip_fraction        | 0.0683      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.000352   |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.58        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00624    |\n",
      "|    reward               | -0.2224417  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.1         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 231         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005630131 |\n",
      "|    clip_fraction        | 0.0574      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.0175      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.56        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00522    |\n",
      "|    reward               | 0.6701883   |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 4.24        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 233          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070917136 |\n",
      "|    clip_fraction        | 0.0913       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0128      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.52         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00729     |\n",
      "|    reward               | -0.10104456  |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 2.94         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1134_20\n",
      "day: 2957, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1000000.00\n",
      "total_reward: 0.00\n",
      "total_cost: 0.00\n",
      "total_trades: 0\n",
      "=================================\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
      "======Trading from:  2022-01-03 to  2022-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1197_20\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 195         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.371      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -13.1       |\n",
      "|    reward             | -0.25082737 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 4.54        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 201        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.69      |\n",
      "|    reward             | 0.47270325 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.7        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 203        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0.275      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.9       |\n",
      "|    reward             | -1.3623469 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 201       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | -0.016    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 1.24      |\n",
      "|    reward             | 0.5590861 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.764     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 199        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -0.00506   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -7.73      |\n",
      "|    reward             | -0.3086395 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | -0.287    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 0.421     |\n",
      "|    reward             | 2.5802572 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.457     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 200         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.16       |\n",
      "|    explained_variance | 0.012       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -16.6       |\n",
      "|    reward             | -0.23557664 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.71        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | -0.0492   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 5.75      |\n",
      "|    reward             | 0.9500994 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.3       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 200         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.15       |\n",
      "|    explained_variance | -0.000529   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.63       |\n",
      "|    reward             | 0.054054994 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.78        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 201         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.15       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | 4.74        |\n",
      "|    reward             | -0.33769238 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.731       |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 201      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 27       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.12    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0482  |\n",
      "|    reward             | 1.53265  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 0.989    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -0.0368   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 5.99      |\n",
      "|    reward             | 1.1673582 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 32        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -2.21     |\n",
      "|    reward             | 1.4888213 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.829     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 200         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 12.8        |\n",
      "|    reward             | -0.49440372 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 5.88        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 200        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 10.2       |\n",
      "|    reward             | 0.13257207 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.58       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 200         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 39          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.16       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 13.2        |\n",
      "|    reward             | -0.29011583 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.48        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 42        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -50.2     |\n",
      "|    reward             | 1.6559334 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 51.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 200        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 29.1       |\n",
      "|    reward             | -2.9442055 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 29.8       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 200       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 47        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 2.68      |\n",
      "|    reward             | 1.3826162 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.62      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 199         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 50          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 8.79        |\n",
      "|    reward             | -0.62915236 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.64        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2022-01-03 to  2022-04-04\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1197_20\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 253         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 8           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.05163526 |\n",
      "------------------------------------\n",
      "day: 3020, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 481383.46\n",
      "total_reward: -518616.54\n",
      "total_cost: 1198167.81\n",
      "total_trades: 11448\n",
      "Sharpe: -0.264\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 237          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059706005 |\n",
      "|    clip_fraction        | 0.0499       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.00722     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.55         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00418     |\n",
      "|    reward               | -0.9847708   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.34         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 233         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007085732 |\n",
      "|    clip_fraction        | 0.0631      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.0201     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    reward               | -1.5060555  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.32        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 230         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 35          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005790704 |\n",
      "|    clip_fraction        | 0.05        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0109      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00629    |\n",
      "|    reward               | -0.28735006 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.63        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 236          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062469435 |\n",
      "|    clip_fraction        | 0.043        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.00566     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.997        |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00333     |\n",
      "|    reward               | -0.34554008  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.32         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-01-03 to  2022-04-04\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1197_20\n",
      "day: 3020, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2630232.37\n",
      "total_reward: 1630232.37\n",
      "total_cost: 998.41\n",
      "total_trades: 9060\n",
      "Sharpe: 0.557\n",
      "=================================\n",
      "======DDPG Validation from:  2022-01-03 to  2022-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
      "======Trading from:  2022-04-04 to  2022-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1260_20\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 281         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -0.137      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -8.91       |\n",
      "|    reward             | 0.008167969 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 1.97        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 276        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -2.66      |\n",
      "|    reward             | 0.78241456 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 1.6        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 279        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | -2.5162303 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 3.54       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -0.0117    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -5.76      |\n",
      "|    reward             | 0.33047342 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 0.9        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 283        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -15.4      |\n",
      "|    reward             | -0.7989811 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 4.97       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 287      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 10       |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.03    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 2.14     |\n",
      "|    reward             | 5.144023 |\n",
      "|    std                | 0.987    |\n",
      "|    value_loss         | 0.59     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -41.9     |\n",
      "|    reward             | 1.5479016 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 32.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 293        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -19.4      |\n",
      "|    reward             | 0.57748544 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 8.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -5.32      |\n",
      "|    reward             | -1.5284566 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.28       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 1.73      |\n",
      "|    reward             | 2.1651177 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 0.121     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -43.4      |\n",
      "|    reward             | -1.7165684 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 43.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 3.38       |\n",
      "|    reward             | -1.8092446 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 4.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 297        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 7.74       |\n",
      "|    reward             | 0.41821334 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 1.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 298       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -21.6     |\n",
      "|    reward             | 0.5522786 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 11.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 299         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 2.28        |\n",
      "|    reward             | -0.26962516 |\n",
      "|    std                | 0.981       |\n",
      "|    value_loss         | 0.461       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 300        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -6.74      |\n",
      "|    reward             | -1.3854643 |\n",
      "|    std                | 0.973      |\n",
      "|    value_loss         | 0.728      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 301       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 18.9      |\n",
      "|    reward             | 0.8678678 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 11.3      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 302         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.93       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | 24.3        |\n",
      "|    reward             | -0.34419647 |\n",
      "|    std                | 0.968       |\n",
      "|    value_loss         | 19          |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 302        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.91      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 1.29       |\n",
      "|    reward             | 0.08448445 |\n",
      "|    std                | 0.964      |\n",
      "|    value_loss         | 0.0858     |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 302        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.91      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -7.03      |\n",
      "|    reward             | -1.1793268 |\n",
      "|    std                | 0.965      |\n",
      "|    value_loss         | 1.45       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-04-04 to  2022-07-06\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1260_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 431        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.13576671 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 369250.17\n",
      "total_reward: -630749.83\n",
      "total_cost: 1188970.90\n",
      "total_trades: 11743\n",
      "Sharpe: -0.361\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 412          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054448014 |\n",
      "|    clip_fraction        | 0.0778       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0105      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.49         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    reward               | 0.016719606  |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 3.16         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007265498 |\n",
      "|    clip_fraction        | 0.0593      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.0219     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.44        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    reward               | -0.961211   |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 3.49        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 405          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057344274 |\n",
      "|    clip_fraction        | 0.0545       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.0221       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.905        |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00503     |\n",
      "|    reward               | -0.14122659  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 4.15         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 404         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009855295 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | 0.00593     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.98        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00568    |\n",
      "|    reward               | -0.3085221  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.41        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-04-04 to  2022-07-06\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1260_20\n",
      "day: 3083, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3133026.40\n",
      "total_reward: 2133026.40\n",
      "total_cost: 998.79\n",
      "total_trades: 9249\n",
      "Sharpe: 0.612\n",
      "=================================\n",
      "======DDPG Validation from:  2022-04-04 to  2022-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-06\n",
      "======Trading from:  2022-07-06 to  2022-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1323_20\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.1      |\n",
      "|    reward             | 0.04315469 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 3.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 282       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.313     |\n",
      "|    reward             | 1.3328929 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 282        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -12.3      |\n",
      "|    reward             | -1.9533604 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3          |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 283        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | -0.0196    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -5.19      |\n",
      "|    reward             | 0.16809471 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.882      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 284        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -7.52      |\n",
      "|    reward             | -0.4612679 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.73       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 289       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -2.49     |\n",
      "|    reward             | 3.7549639 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.36      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -0.0765   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -5.61     |\n",
      "|    reward             | 0.6153198 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 0.861     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 293        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 6.08       |\n",
      "|    reward             | -0.3200203 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.18       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 296          |\n",
      "|    iterations         | 900          |\n",
      "|    time_elapsed       | 15           |\n",
      "|    total_timesteps    | 4500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.07        |\n",
      "|    explained_variance | -0.0207      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 899          |\n",
      "|    policy_loss        | -0.723       |\n",
      "|    reward             | -0.038040616 |\n",
      "|    std                | 0.995        |\n",
      "|    value_loss         | 0.595        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -0.0116   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 1.78      |\n",
      "|    reward             | 0.8571013 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.214     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 299       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 2.77      |\n",
      "|    reward             | 0.7215265 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 0.601     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 300        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 10.4       |\n",
      "|    reward             | 0.75108594 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 3.55       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 300         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | -2.17       |\n",
      "|    reward             | -0.67904377 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 0.742       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 301        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -9.14      |\n",
      "|    reward             | -1.6252009 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 2.51       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 302       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -27.6     |\n",
      "|    reward             | 1.7107952 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 16.9      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 303        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 2.38e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -2.91      |\n",
      "|    reward             | 0.16515338 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 0.596      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 303         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0.0625      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -2.03       |\n",
      "|    reward             | 0.071903974 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 0.408       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 304         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -12.5       |\n",
      "|    reward             | -0.35043776 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 10.5        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 304         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 31          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | 2.78        |\n",
      "|    reward             | -0.05684375 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 0.38        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 304        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 8.11       |\n",
      "|    reward             | -1.0614872 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.92       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-07-06 to  2022-10-04\n",
      "A2C Sharpe Ratio:  -1\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1323_20\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 441        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.20889612 |\n",
      "-----------------------------------\n",
      "day: 3146, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 650301.43\n",
      "total_reward: -349698.57\n",
      "total_cost: 1556532.78\n",
      "total_trades: 12185\n",
      "Sharpe: -0.108\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 407          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054740654 |\n",
      "|    clip_fraction        | 0.0762       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0343      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00596     |\n",
      "|    reward               | 1.5291109    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.45         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 406          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0064450772 |\n",
      "|    clip_fraction        | 0.0635       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.000914     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.97         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00574     |\n",
      "|    reward               | -0.041960813 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 6.45         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 403        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 20         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.0067374  |\n",
      "|    clip_fraction        | 0.0542     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.14      |\n",
      "|    explained_variance   | 0.0266     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 0.586      |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.00431   |\n",
      "|    reward               | 0.34290332 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 1.75       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007631976 |\n",
      "|    clip_fraction        | 0.0722      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.14       |\n",
      "|    explained_variance   | -0.0238     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00747    |\n",
      "|    reward               | 0.45980084  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.2         |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-06 to  2022-10-04\n",
      "PPO Sharpe Ratio:  -1\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1323_20\n",
      "day: 3146, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4431776.17\n",
      "total_reward: 3431776.17\n",
      "total_cost: 998.37\n",
      "total_trades: 6292\n",
      "Sharpe: 0.714\n",
      "=================================\n",
      "======DDPG Validation from:  2022-07-06 to  2022-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-04\n",
      "======Trading from:  2022-10-04 to  2023-01-04\n",
      "Ensemble Strategy took:  58.18696219126384  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.420461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.00767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.265496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.358259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.304516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>441</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>504</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.11588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>567</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.170601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>630</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.718498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>693</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.165558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>756</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.330678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>819</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.197196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.412785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>945</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.175407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1008</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.202291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1071</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.046011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1134</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1197</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.156932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1260</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.300059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1323</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.076032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0    126  2017-10-02  2018-01-02       DDPG         -1         -1    0.420461\n",
       "1    189  2018-01-02  2018-04-04       DDPG         -1         -1    -0.00767\n",
       "2    252  2018-04-04  2018-07-03       DDPG         -1         -1    0.265496\n",
       "3    315  2018-07-03  2018-10-02       DDPG         -1         -1    0.358259\n",
       "4    378  2018-10-02  2019-01-03       DDPG         -1         -1   -0.304516\n",
       "5    441  2019-01-03  2019-04-04       DDPG         -1         -1         0.0\n",
       "6    504  2019-04-04  2019-07-05       DDPG         -1         -1     0.11588\n",
       "7    567  2019-07-05  2019-10-03       DDPG         -1         -1   -0.170601\n",
       "8    630  2019-10-03  2020-01-03       DDPG         -1         -1    0.718498\n",
       "9    693  2020-01-03  2020-04-03       DDPG         -1         -1   -0.165558\n",
       "10   756  2020-04-03  2020-07-06       DDPG         -1         -1    0.330678\n",
       "11   819  2020-07-06  2020-10-02       DDPG         -1         -1    0.197196\n",
       "12   882  2020-10-02  2021-01-04       DDPG         -1         -1    0.412785\n",
       "13   945  2021-01-04  2021-04-06       DDPG         -1         -1    0.175407\n",
       "14  1008  2021-04-06  2021-07-06       DDPG         -1         -1    0.202291\n",
       "15  1071  2021-07-06  2021-10-04       DDPG         -1         -1   -0.046011\n",
       "16  1134  2021-10-04  2022-01-03       DDPG         -1         -1         0.0\n",
       "17  1197  2022-01-03  2022-04-04       DDPG         -1         -1   -0.156932\n",
       "18  1260  2022-04-04  2022-07-06       DDPG         -1         -1   -0.300059\n",
       "19  1323  2022-07-06  2022-10-04       DDPG         -1         -1   -0.076032"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  -0.05237945488931736\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp],ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.005723e+06</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.005723</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.009503e+06</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.003759</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.015255e+06</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.016111e+06</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>2018-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0   1.000000e+06  2018-01-02           NaN  2018-01-02\n",
       "1   1.005723e+06  2018-01-03      0.005723  2018-01-03\n",
       "2   1.009503e+06  2018-01-04      0.003759  2018-01-04\n",
       "3   1.015255e+06  2018-01-05      0.005698  2018-01-05\n",
       "4   1.016111e+06  2018-01-08      0.000843  2018-01-08"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return         -0.085406\n",
      "Cumulative returns    -0.360056\n",
      "Annual volatility      0.319354\n",
      "Sharpe ratio          -0.052379\n",
      "Calmar ratio          -0.120530\n",
      "Stability              0.264341\n",
      "Max drawdown          -0.708583\n",
      "Omega ratio            0.985758\n",
      "Sortino ratio         -0.058051\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.989586\n",
      "Daily value at risk   -0.040301\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (1259, 8)\n",
      "Annual return          0.059583\n",
      "Cumulative returns     0.335290\n",
      "Annual volatility      0.217954\n",
      "Sharpe ratio           0.375375\n",
      "Calmar ratio           0.160661\n",
      "Stability              0.685042\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.077914\n",
      "Sortino ratio          0.516270\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.893140\n",
      "Daily value at risk   -0.027135\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_dji_ = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:              date           dji\n",
      "0     2018-01-02  1.000000e+06\n",
      "1     2018-01-03  1.003975e+06\n",
      "2     2018-01-04  1.010116e+06\n",
      "3     2018-01-05  1.019008e+06\n",
      "4     2018-01-08  1.018490e+06\n",
      "...          ...           ...\n",
      "1255  2022-12-27  1.339089e+06\n",
      "1256  2022-12-28  1.324351e+06\n",
      "1257  2022-12-29  1.338253e+06\n",
      "1258  2022-12-30  1.335290e+06\n",
      "1259  2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.003975e+06\n",
      "2018-01-04  1.010116e+06\n",
      "2018-01-05  1.019008e+06\n",
      "2018-01-08  1.018490e+06\n",
      "...                  ...\n",
      "2022-12-27  1.339089e+06\n",
      "2022-12-28  1.324351e+06\n",
      "2022-12-29  1.338253e+06\n",
      "2022-12-30  1.335290e+06\n",
      "2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:          datadate\n",
      "0     2017-10-02\n",
      "1     2017-10-03\n",
      "2     2017-10-04\n",
      "3     2017-10-05\n",
      "4     2017-10-06\n",
      "...          ...\n",
      "1356  2023-02-22\n",
      "1357  2023-02-23\n",
      "1358  2023-02-24\n",
      "1359  2023-02-27\n",
      "1360  2023-02-28\n",
      "\n",
      "[1361 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.005723e+06\n",
      "2018-01-04  1.009503e+06\n",
      "2018-01-05  1.015255e+06\n",
      "2018-01-08  1.016111e+06\n",
      "...                  ...\n",
      "2022-12-27  6.271834e+05\n",
      "2022-12-28  6.265568e+05\n",
      "2022-12-29  6.421291e+05\n",
      "2022-12-30  6.414191e+05\n",
      "2023-01-03  6.399442e+05\n",
      "\n",
      "[1260 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2018-01-02  1.000000e+06  1.000000e+06\n",
      "2018-01-03  1.005723e+06  1.003975e+06\n",
      "2018-01-04  1.009503e+06  1.010116e+06\n",
      "2018-01-05  1.015255e+06  1.019008e+06\n",
      "2018-01-08  1.016111e+06  1.018490e+06\n",
      "...                  ...           ...\n",
      "2022-12-27  6.271834e+05  1.339089e+06\n",
      "2022-12-28  6.265568e+05  1.324351e+06\n",
      "2022-12-29  6.421291e+05  1.338253e+06\n",
      "2022-12-30  6.414191e+05  1.335290e+06\n",
      "2023-01-03  6.399442e+05           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value, \n",
    "#               baseline_ticker = '^DJI', \n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./pkl_results/DDPGPRED.pkl', 'wb') as file:\n",
    "    pickle.dump(df_result_ensemble, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
