{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb9q2_QZgdNk"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXaoZs2lh1hi"
   },
   "source": [
    "# Deep Reinforcement Learning for Stock Trading from Scratch: Multiple Stock Trading Using Ensemble Strategy\n",
    "\n",
    "Tutorials to use OpenAI DRL to trade multiple stocks using ensemble strategy in one Jupyter Notebook | Presented at ICAIF 2020\n",
    "\n",
    "* This notebook is the reimplementation of our paper: Deep Reinforcement Learning for Automated Stock Trading: An Ensemble Strategy, using FinRL.\n",
    "* Check out medium blog for detailed explanations: https://medium.com/@ai4finance/deep-reinforcement-learning-for-automated-stock-trading-f1dad0126a02\n",
    "* Please report any issues to our Github: https://github.com/AI4Finance-LLC/FinRL-Library/issues\n",
    "* **Pytorch Version** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGunVt8oLCVS"
   },
   "source": [
    "# Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HOzAKQ-SLGX6"
   },
   "source": [
    "* [1. Problem Definition](#0)\n",
    "* [2. Getting Started - Load Python packages](#1)\n",
    "    * [2.1. Install Packages](#1.1)    \n",
    "    * [2.2. Check Additional Packages](#1.2)\n",
    "    * [2.3. Import Packages](#1.3)\n",
    "    * [2.4. Create Folders](#1.4)\n",
    "* [3. Download Data](#2)\n",
    "* [4. Preprocess Data](#3)        \n",
    "    * [4.1. Technical Indicators](#3.1)\n",
    "    * [4.2. Perform Feature Engineering](#3.2)\n",
    "* [5.Build Environment](#4)  \n",
    "    * [5.1. Training & Trade Data Split](#4.1)\n",
    "    * [5.2. User-defined Environment](#4.2)   \n",
    "    * [5.3. Initialize Environment](#4.3)    \n",
    "* [6.Implement DRL Algorithms](#5)  \n",
    "* [7.Backtesting Performance](#6)  \n",
    "    * [7.1. BackTestStats](#6.1)\n",
    "    * [7.2. BackTestPlot](#6.2)   \n",
    "    * [7.3. Baseline Stats](#6.3)   \n",
    "    * [7.3. Compare to Stock Market Index](#6.4)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sApkDlD9LIZv"
   },
   "source": [
    "<a id='0'></a>\n",
    "# Part 1. Problem Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjLD2TZSLKZ-"
   },
   "source": [
    "This problem is to design an automated trading solution for single stock trading. We model the stock trading process as a Markov Decision Process (MDP). We then formulate our trading goal as a maximization problem.\n",
    "\n",
    "The algorithm is trained using Deep Reinforcement Learning (DRL) algorithms and the components of the reinforcement learning environment are:\n",
    "\n",
    "\n",
    "* Action: The action space describes the allowed actions that the agent interacts with the\n",
    "environment. Normally, a ∈ A includes three actions: a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying one stock. Also, an action can be carried upon multiple shares. We use\n",
    "an action space {−k, ..., −1, 0, 1, ..., k}, where k denotes the number of shares. For example, \"Buy\n",
    "10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* Reward function: r(s, a, s′) is the incentive mechanism for an agent to learn a better action. The change of the portfolio value when action a is taken at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio\n",
    "values at state s′ and s, respectively\n",
    "\n",
    "* State: The state space describes the observations that the agent receives from the environment. Just as a human trader needs to analyze various information before executing a trade, so\n",
    "our trading agent observes many different features to better learn in an interactive environment.\n",
    "\n",
    "* Environment: Dow 30 consituents\n",
    "\n",
    "\n",
    "The data of the single stock that we will be using for this case study is obtained from Yahoo Finance API. The data contains Open-High-Low-Close price and volume.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ffsre789LY08"
   },
   "source": [
    "<a id='1'></a>\n",
    "# Part 2. Getting Started- Load Python Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy5_PTmOh1hj"
   },
   "source": [
    "<a id='1.1'></a>\n",
    "## 2.1. Install all the packages through FinRL library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mPT0ipYE28wL",
    "outputId": "75fcd958-c29f-44f0-85ea-4b4f6ae180ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wrds in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.26.4)\n",
      "Requirement already satisfied: packaging<23.3 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (23.2)\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.2.2)\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.9.9)\n",
      "Requirement already satisfied: scipy<1.13,>=1.12 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from wrds) (2.0.29)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from pandas<2.3,>=2.2->wrds) (2024.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.11.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from sqlalchemy<2.1,>=2->wrds) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\n",
      "Requirement already satisfied: swig in /home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages (4.2.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\n",
      "E: Unable to lock directory /var/lib/apt/lists/\n",
      "W: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\n",
      "W: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\n"
     ]
    }
   ],
   "source": [
    "# ## install finrl library\n",
    "!pip install wrds\n",
    "!pip install swig\n",
    "!pip install -q condacolab\n",
    "#import condacolab\n",
    "#condacolab.install()\n",
    "!apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
    "#!pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "osBHhVysOEzi"
   },
   "source": [
    "\n",
    "<a id='1.2'></a>\n",
    "## 2.2. Check if the additional packages needed are present, if not install them. \n",
    "* Yahoo Finance API\n",
    "* pandas\n",
    "* numpy\n",
    "* matplotlib\n",
    "* stockstats\n",
    "* OpenAI gym\n",
    "* stable-baselines\n",
    "* tensorflow\n",
    "* pyfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGv01K8Sh1hn"
   },
   "source": [
    "<a id='1.3'></a>\n",
    "## 2.3. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EeMK7Uentj1V"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "lPqeTTwoh1hn"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 00:46:14.254483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-28 00:46:14.284552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 00:46:14.827299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.use('Agg')\n",
    "import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "from finrl.config_tickers import DOW_30_TICKER\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
    "from finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FinRL-Library\")\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOW_5_TICKER = [\n",
    "    \"AXP\",\n",
    "    \"AMGN\",\n",
    "    \"AAPL\",\n",
    "    \"BA\",\n",
    "    \"CAT\",\n",
    "]\n",
    "INDEX_5_TICKER = [\n",
    "    \"^DJI\", \n",
    "    \"^IXIC\", \n",
    "    \"^NYA\", \n",
    "    \"^RUT\", \n",
    "    \"^GSPC\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2owTj985RW4"
   },
   "source": [
    "<a id='1.4'></a>\n",
    "## 2.4. Create Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "w9A8CN5R5PuZ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import (\n",
    "    DATA_SAVE_DIR,\n",
    "    TRAINED_MODEL_DIR,\n",
    "    TENSORBOARD_LOG_DIR,\n",
    "    RESULTS_DIR,\n",
    "    INDICATORS,\n",
    "    TRAIN_START_DATE,\n",
    "    TRAIN_END_DATE,\n",
    "    TEST_START_DATE,\n",
    "    TEST_END_DATE,\n",
    "    TRADE_START_DATE,\n",
    "    TRADE_END_DATE,\n",
    ")\n",
    "\n",
    "check_and_make_directories([DATA_SAVE_DIR, TRAINED_MODEL_DIR, TENSORBOARD_LOG_DIR, RESULTS_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A289rQWMh1hq"
   },
   "source": [
    "<a id='2'></a>\n",
    "# Part 3. Download Data\n",
    "Yahoo Finance is a website that provides stock data, financial news, financial reports, etc. All the data provided by Yahoo Finance is free.\n",
    "* FinRL uses a class **YahooDownloader** to fetch data from Yahoo Finance API\n",
    "* Call Limit: Using the Public API (without authentication), you are limited to 2,000 requests per hour per IP (or up to a total of 48,000 requests a day).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPeQ7iS-LoMm"
   },
   "source": [
    "\n",
    "\n",
    "-----\n",
    "class YahooDownloader:\n",
    "    Provides methods for retrieving daily stock data from\n",
    "    Yahoo Finance API\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "        start_date : str\n",
    "            start date of the data (modified from config.py)\n",
    "        end_date : str\n",
    "            end date of the data (modified from config.py)\n",
    "        ticker_list : list\n",
    "            a list of stock tickers (modified from config.py)\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fetch_data()\n",
    "        Fetches data from yahoo API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzqRRTOX6aFu",
    "outputId": "178c70ab-72e5-4ed7-cfa8-fd6ea7b1e8ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AXP', 'AMGN', 'AAPL', 'BA', 'CAT', 'CSCO', 'CVX', 'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'KO', 'JPM', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'CRM', 'VZ', 'V', 'WBA', 'WMT', 'DIS', 'DOW']\n"
     ]
    }
   ],
   "source": [
    "print(DOW_30_TICKER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################\n",
    "\n",
    "\n",
    "# # TRAIN_START_DATE = '2009-04-01'\n",
    "# # TRAIN_END_DATE = '2021-01-01'\n",
    "# # TEST_START_DATE = '2021-01-01'\n",
    "# # TEST_END_DATE = '2022-06-01'\n",
    "\n",
    "\n",
    "# TRAIN_START_DATE = '2009-06-01'\n",
    "# #TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "# dfexport = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "#                      end_date = TEST_END_DATE,\n",
    "#                      ticker_list = DOW_30_TICKER).fetch_data()\n",
    "\n",
    "\n",
    "##################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(dfexport.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Data export\n",
    "# import pickle\n",
    "# datasetName = \"dailydata\"\n",
    "# datasetDir = \"./datasets\"\n",
    "\n",
    "# os.makedirs(datasetDir, exist_ok=True)\n",
    "# datasetPath = os.path.join(datasetDir, datasetName) + \".pkl\"\n",
    "\n",
    "\n",
    "# with open(datasetPath, 'wb') as file:\n",
    "#     pickle.dump(dfexport, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yCKm4om-s9kE",
    "outputId": "0a5b0405-7c4f-4afd-c3e1-1dabd55c81fb"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Can't determine version for tzdata",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32mtzconversion.pyx:83\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.tzconversion.Localizer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimezones.pyx:81\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timezones.is_utc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mtimezones.pyx:70\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.timezones.is_utc_zoneinfo\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py:150\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[0;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[1;32m    148\u001b[0m minimum_version \u001b[38;5;241m=\u001b[39m min_version \u001b[38;5;28;01mif\u001b[39;00m min_version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m VERSIONS\u001b[38;5;241m.\u001b[39mget(parent)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m minimum_version:\n\u001b[0;32m--> 150\u001b[0m     version \u001b[38;5;241m=\u001b[39m \u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_to_get\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mand\u001b[39;00m Version(version) \u001b[38;5;241m<\u001b[39m Version(minimum_version):\n\u001b[1;32m    152\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    153\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas requires version \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mminimum_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or newer of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(version \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m currently installed).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py:78\u001b[0m, in \u001b[0;36mget_version\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m     75\u001b[0m version \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__version__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt determine version for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpsycopg2\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# psycopg2 appends \" (dt dec pq3 ext lo64)\" to it's version\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     version \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: Can't determine version for tzdata"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'pandas._libs.tslibs.conversion._localize_tso'\n",
      "Traceback (most recent call last):\n",
      "  File \"tzconversion.pyx\", line 83, in pandas._libs.tslibs.tzconversion.Localizer.__cinit__\n",
      "  File \"timezones.pyx\", line 81, in pandas._libs.tslibs.timezones.is_utc\n",
      "  File \"timezones.pyx\", line 70, in pandas._libs.tslibs.timezones.is_utc_zoneinfo\n",
      "  File \"/home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py\", line 150, in import_optional_dependency\n",
      "    version = get_version(module_to_get)\n",
      "  File \"/home/drew/anaconda3/envs/FinRL_2020v2/lib/python3.9/site-packages/pandas/compat/_optional.py\", line 78, in get_version\n",
      "    raise ImportError(f\"Can't determine version for {module.__name__}\")\n",
      "ImportError: Can't determine version for tzdata\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed\n",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (16555, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TRAIN_START_DATE = '2009-04-01'\n",
    "# TRAIN_END_DATE = '2021-01-01'\n",
    "# TEST_START_DATE = '2021-01-01'\n",
    "# TEST_END_DATE = '2022-06-01'\n",
    "#TRAIN_START_DATE = '2000-01-01'\n",
    "# TRAIN_START_DATE = '2010-01-01'\n",
    "# TRAIN_END_DATE = '2021-10-01'\n",
    "# TEST_START_DATE = '2021-10-01'\n",
    "# TEST_END_DATE = '2023-03-01'\n",
    "TRAIN_START_DATE = '2010-01-01'\n",
    "TRAIN_END_DATE = '2017-10-01'\n",
    "TEST_START_DATE = '2017-10-01'\n",
    "TEST_END_DATE = '2023-03-01'\n",
    "\n",
    "\n",
    "df = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TEST_END_DATE,\n",
    "                     ticker_list = INDEX_5_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "# Part 4: Preprocess Data\n",
    "Data preprocessing is a crucial step for training a high quality machine learning model. We need to check for missing data and do feature engineering in order to convert the data into a model-ready state.\n",
    "* Add technical indicators. In practical trading, various information needs to be taken into account, for example the historical stock prices, current holding shares, technical indicators, etc. In this article, we demonstrate two trend-following technical indicators: MACD and RSI.\n",
    "* Add turbulence index. Risk-aversion reflects whether an investor will choose to preserve the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, FinRL employs the financial turbulence index that measures extreme asset price fluctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jgXfBcjxtj1a",
    "outputId": "bd80d5c7-6ab7-4938-e1aa-f60ff642dc02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully added technical indicators\n",
      "Successfully added turbulence index\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df)\n",
    "processed = processed.copy()\n",
    "processed = processed.fillna(0)\n",
    "processed = processed.replace(np.inf,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Andrew Martin - UNCOMMENT BELOW TO ADD PREDICTION INDICATOR\n",
    "import pickle\n",
    "with open(\"./index_5_predictor_4.pkl\", 'rb') as file:\n",
    "  df_prob = pickle.load(file)\n",
    "df6 = df_prob.copy()\n",
    "df6 = df6.loc[:, ~df6.columns.duplicated(keep='first')]\n",
    "df6[\"date\"] = df6[\"date\"].dt.strftime('%Y-%m-%d')\n",
    "df2 = processed.merge(df6[['tic', 'date', 'Probability']], on=['tic', 'date'], how='left')\n",
    "processed = df2.copy()\n",
    "INDICATORS.append(\"Probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date              2010-01-12\n",
      "open            10662.860352\n",
      "high            10663.080078\n",
      "low             10568.839844\n",
      "close           10627.259766\n",
      "volume             256050000\n",
      "tic                     ^DJI\n",
      "day                        1\n",
      "macd                4.463799\n",
      "boll_ub         10673.133918\n",
      "boll_lb           10539.9975\n",
      "rsi_30             64.829273\n",
      "cci_30             76.395882\n",
      "dx_30               8.049358\n",
      "close_30_sma    10606.565709\n",
      "close_60_sma    10606.565709\n",
      "turbulence               0.0\n",
      "Probability         0.993373\n",
      "Name: 30, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed.iloc[30])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QsYaY0Dh1iw"
   },
   "source": [
    "<a id='4'></a>\n",
    "# Part 5. Design Environment\n",
    "Considering the stochastic and interactive nature of the automated stock trading tasks, a financial task is modeled as a **Markov Decision Process (MDP)** problem. The training process involves observing stock price change, taking an action and reward's calculation to have the agent adjusting its strategy accordingly. By interacting with the environment, the trading agent will derive a trading strategy with the maximized rewards as time proceeds.\n",
    "\n",
    "Our trading environments, based on OpenAI Gym framework, simulate live stock markets with real market data according to the principle of time-driven simulation.\n",
    "\n",
    "The action space describes the allowed actions that the agent interacts with the environment. Normally, action a includes three actions: {-1, 0, 1}, where -1, 0, 1 represent selling, holding, and buying one share. Also, an action can be carried upon multiple shares. We use an action space {-k,…,-1, 0, 1, …, k}, where k denotes the number of shares to buy and -k denotes the number of shares to sell. For example, \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or -10, respectively. The continuous action space needs to be normalized to [-1, 1], since the policy is defined on a Gaussian distribution, which needs to be normalized and symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2zqII8rMIqn",
    "outputId": "e16902dc-86b3-488e-ec15-234a3d6039c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 5, State Space: 56\n"
     ]
    }
   ],
   "source": [
    "stock_dimension = len(processed.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "AWyp84Ltto19"
   },
   "outputs": [],
   "source": [
    "env_kwargs = {\n",
    "    \"hmax\": 100, \n",
    "    \"initial_amount\": 1000000, \n",
    "    \"buy_cost_pct\": 0.001, \n",
    "    \"sell_cost_pct\": 0.001, \n",
    "    \"state_space\": state_space, \n",
    "    \"stock_dim\": stock_dimension, \n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension, \n",
    "    \"reward_scaling\": 1e-4,\n",
    "    \"print_verbosity\":5\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMNR5nHjh1iz"
   },
   "source": [
    "<a id='5'></a>\n",
    "# Part 6: Implement DRL Algorithms\n",
    "* The implementation of the DRL algorithms are based on **OpenAI Baselines** and **Stable Baselines**. Stable Baselines is a fork of OpenAI Baselines, with a major structural refactoring, and code cleanups.\n",
    "* FinRL library includes fine-tuned standard DRL algorithms, such as DQN, DDPG,\n",
    "Multi-Agent DDPG, PPO, SAC, A2C and TD3. We also allow users to\n",
    "design their own DRL algorithms by adapting these DRL algorithms.\n",
    "\n",
    "* In this notebook, we are training and validating 3 agents (A2C, PPO, DDPG) using Rolling-window Ensemble Method ([reference code](https://github.com/AI4Finance-LLC/Deep-Reinforcement-Learning-for-Automated-Stock-Trading-Ensemble-Strategy-ICAIF-2020/blob/80415db8fa7b2179df6bd7e81ce4fe8dbf913806/model/models.py#L92))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "v-gthCxMtj1d"
   },
   "outputs": [],
   "source": [
    "rebalance_window = 63 # rebalance_window is the number of days to retrain the model\n",
    "validation_window = 63 # validation_window is the number of days to do validation and trading (e.g. if validation_window=63, then both validation and trading period will be 63 days)\n",
    "\n",
    "ensemble_agent = DRLEnsembleAgent(df=processed,\n",
    "                 train_period=(TRAIN_START_DATE,TRAIN_END_DATE),\n",
    "                 val_test_period=(TEST_START_DATE,TEST_END_DATE),\n",
    "                 rebalance_window=rebalance_window, \n",
    "                 validation_window=validation_window, \n",
    "                 **env_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "KsfEHa_Etj1d",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "A2C_model_kwargs = {\n",
    "                    'n_steps': 5,\n",
    "                    'ent_coef': 0.005,\n",
    "                    'learning_rate': 0.0007\n",
    "                    }\n",
    "\n",
    "PPO_model_kwargs = {\n",
    "                    \"ent_coef\":0.01,\n",
    "                    \"n_steps\": 2048,\n",
    "                    \"learning_rate\": 0.00025,\n",
    "                    \"batch_size\": 128\n",
    "                    }\n",
    "\n",
    "DDPG_model_kwargs = {\n",
    "                      #\"action_noise\":\"ornstein_uhlenbeck\",\n",
    "                      \"buffer_size\": 10_000,\n",
    "                      \"learning_rate\": 0.0005,\n",
    "                      \"batch_size\": 64\n",
    "                    }\n",
    "\n",
    "timesteps_dict = {'a2c' : 10_000, \n",
    "                 'ppo' : 10_000, \n",
    "                 'ddpg' : 10_000\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch.utils.tensorboard import SummaryWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1lyCECstj1e",
    "outputId": "73e2d3f8-463a-42d5-d49f-c71385a26c92",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Start Ensemble Strategy============\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2017-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_126_17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 283         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.3       |\n",
      "|    reward             | 0.009028808 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 3.05        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 314        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -2.64      |\n",
      "|    reward             | 0.87127095 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.93       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 328        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.26      |\n",
      "|    reward             | -2.8062048 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 4.3        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 335      |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 5        |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.13    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 6.85     |\n",
      "|    reward             | 0.643976 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.51     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 337        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -2.55      |\n",
      "|    reward             | 0.32807958 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.22       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 5.27       |\n",
      "|    reward             | -0.1832905 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.676      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 339        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 1.87       |\n",
      "|    reward             | 0.78114223 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.494      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.2      |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -54.7     |\n",
      "|    reward             | 3.7831924 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 51.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.19     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -18.5     |\n",
      "|    reward             | 2.4817996 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 4.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 23.6      |\n",
      "|    reward             | 1.6657948 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -22.3      |\n",
      "|    reward             | -0.8872321 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 13.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 5.33      |\n",
      "|    reward             | 1.3666681 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.66      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -25.8      |\n",
      "|    reward             | -2.4442255 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 15.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.21      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 8.21       |\n",
      "|    reward             | 0.76216745 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.91       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 338         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.21       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 5.5         |\n",
      "|    reward             | -0.47309753 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 1.26        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.24      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 5.83       |\n",
      "|    reward             | -1.9309291 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 0.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 338       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.24     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 6.39      |\n",
      "|    reward             | -0.084912 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.777     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 338      |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.23    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -17.1    |\n",
      "|    reward             | -3.8266  |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 7.97     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 338         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.25       |\n",
      "|    explained_variance | 0.023       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -1.94       |\n",
      "|    reward             | -0.40524837 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 2.19        |\n",
      "---------------------------------------\n",
      "day: 1949, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2504572.27\n",
      "total_reward: 1504572.27\n",
      "total_cost: 196948.54\n",
      "total_trades: 4296\n",
      "Sharpe: 0.796\n",
      "=================================\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 339         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 29          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.25       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 5.72        |\n",
      "|    reward             | -0.16708294 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 1.02        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2017-10-02 to  2018-01-02\n",
      "A2C Sharpe Ratio:  0.21031538944946526\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_126_16\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 457         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.14646858 |\n",
      "------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 435         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006965187 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.034      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.51        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00864    |\n",
      "|    reward               | 0.7986904   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.02        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 429          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068946322 |\n",
      "|    clip_fraction        | 0.0966       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.00147      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.09         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00762     |\n",
      "|    reward               | -0.5919369   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.97         |\n",
      "------------------------------------------\n",
      "day: 1949, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 594409.32\n",
      "total_reward: -405590.68\n",
      "total_cost: 828804.49\n",
      "total_trades: 7615\n",
      "Sharpe: -0.378\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 429         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005966029 |\n",
      "|    clip_fraction        | 0.0502      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.14       |\n",
      "|    explained_variance   | 0.00561     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00415    |\n",
      "|    reward               | -0.71125466 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 418         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 24          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005177782 |\n",
      "|    clip_fraction        | 0.0557      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.15       |\n",
      "|    explained_variance   | 4.79e-05    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.68        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00464    |\n",
      "|    reward               | -1.514263   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3           |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2017-10-02 to  2018-01-02\n",
      "PPO Sharpe Ratio:  0.08861645591881182\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_126_14\n",
      "day: 1949, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2128598.69\n",
      "total_reward: 1128598.69\n",
      "total_cost: 998.59\n",
      "total_trades: 3898\n",
      "Sharpe: 0.761\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 169        |\n",
      "|    time_elapsed    | 45         |\n",
      "|    total_timesteps | 7800       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -810       |\n",
      "|    critic_loss     | 87.1       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7699       |\n",
      "|    reward          | 0.23197193 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2017-10-02 to  2018-01-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-01-02\n",
      "======Trading from:  2018-01-02 to  2018-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-01-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_189_15\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 283          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | -0.0875      |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -8.13        |\n",
      "|    reward             | -0.038134962 |\n",
      "|    std                | 0.993        |\n",
      "|    value_loss         | 1.43         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 284       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -0.56     |\n",
      "|    reward             | 0.6237842 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 280        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.17      |\n",
      "|    reward             | -2.1428201 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0.0177     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 2.08       |\n",
      "|    reward             | 0.82184416 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 2.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.77      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 48.4       |\n",
      "|    reward             | 0.42242548 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 41.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 280         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 2.73        |\n",
      "|    reward             | -0.22187878 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 0.422       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 13.9       |\n",
      "|    reward             | -0.6986639 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 282        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.109      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -14.5      |\n",
      "|    reward             | -0.1721219 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 284        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0.000923   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -6.91      |\n",
      "|    reward             | -1.1529908 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.88       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 283        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -18.9      |\n",
      "|    reward             | 0.32215592 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 284        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -0.123     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 12.9       |\n",
      "|    reward             | 0.11240362 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.29       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 285       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0.00477   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.0874    |\n",
      "|    reward             | 0.5064072 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.405     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 284        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 19         |\n",
      "|    reward             | -2.2288065 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.38       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 284          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.16        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | 8.54         |\n",
      "|    reward             | -0.008075561 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 3.4          |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 284      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.14    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 4.07     |\n",
      "|    reward             | 2.253746 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.23     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 285         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 28          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0.242       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 0.328       |\n",
      "|    reward             | -0.13085678 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.0621      |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 284      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.09    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -2.65    |\n",
      "|    reward             | 0.942425 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 2.89     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -8.38      |\n",
      "|    reward             | 0.24598682 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 2.02       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 285       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -25.2     |\n",
      "|    reward             | 2.8273728 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 11.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -0.0037    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 0.212      |\n",
      "|    reward             | 0.36668843 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 0.309      |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-01-02 to  2018-04-04\n",
      "A2C Sharpe Ratio:  -0.09504640887541316\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_189_15\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    fps             | 391       |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 5         |\n",
      "|    total_timesteps | 2048      |\n",
      "| train/             |           |\n",
      "|    reward          | 0.7059178 |\n",
      "----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 363          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035218233 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0915      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 0.986        |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00407     |\n",
      "|    reward               | 0.05144565   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.28         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 351         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004916548 |\n",
      "|    clip_fraction        | 0.0682      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.0188      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.14        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00774    |\n",
      "|    reward               | -2.9448898  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.43        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004841103 |\n",
      "|    clip_fraction        | 0.0667      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0171      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00646    |\n",
      "|    reward               | 1.0409427   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.37        |\n",
      "-----------------------------------------\n",
      "day: 2012, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 647899.00\n",
      "total_reward: -352101.00\n",
      "total_cost: 891075.43\n",
      "total_trades: 7781\n",
      "Sharpe: -0.320\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 348          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 29           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076806284 |\n",
      "|    clip_fraction        | 0.0697       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.0205       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.75         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00621     |\n",
      "|    reward               | -0.099414706 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.29         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-01-02 to  2018-04-04\n",
      "PPO Sharpe Ratio:  -0.2874405074019499\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_189_15\n",
      "day: 2012, episode: 15\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2483940.34\n",
      "total_reward: 1483940.34\n",
      "total_cost: 998.55\n",
      "total_trades: 8047\n",
      "Sharpe: 0.877\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 163        |\n",
      "|    time_elapsed    | 49         |\n",
      "|    total_timesteps | 8052       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 107        |\n",
      "|    critic_loss     | 4          |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 7951       |\n",
      "|    reward          | -1.3263464 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-01-02 to  2018-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-04-04\n",
      "======Trading from:  2018-04-04 to  2018-07-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_252_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 329        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | 0.25313443 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 5          |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 317       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -3.93     |\n",
      "|    reward             | 0.8719154 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.91      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 315        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.9      |\n",
      "|    reward             | -2.6945145 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 4.21       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 304        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -6.37      |\n",
      "|    reward             | 0.34547788 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 301        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 16.6       |\n",
      "|    reward             | -0.9792096 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 10         |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 299        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -12.2      |\n",
      "|    reward             | 0.39544362 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 4.11       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -131       |\n",
      "|    reward             | -2.0983074 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 328        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 292         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -20.9       |\n",
      "|    reward             | -0.28674316 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 15          |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 292         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.92       |\n",
      "|    reward             | -0.86473495 |\n",
      "|    std                | 0.995       |\n",
      "|    value_loss         | 1.86        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 291       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 8.97      |\n",
      "|    reward             | 1.4581306 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 1.91      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 289      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.08    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -20.9    |\n",
      "|    reward             | 0.2376   |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 12.5     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 289         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 20          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 1.79e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 14.7        |\n",
      "|    reward             | 0.082096875 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 6.59        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 288      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -4.25    |\n",
      "|    reward             | 2.207946 |\n",
      "|    std                | 0.992    |\n",
      "|    value_loss         | 1.75     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 288          |\n",
      "|    iterations         | 1400         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 7000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.04        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1399         |\n",
      "|    policy_loss        | 3.36         |\n",
      "|    reward             | -0.027589893 |\n",
      "|    std                | 0.989        |\n",
      "|    value_loss         | 0.318        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 286        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -34.6      |\n",
      "|    reward             | 0.32313517 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 28.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 286        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 13.2       |\n",
      "|    reward             | 0.73094064 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 7.04       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 287       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 29        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -0.418    |\n",
      "|    reward             | 1.0093255 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 0.576     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 286        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -9.07      |\n",
      "|    reward             | -0.5745919 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 3.89       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 285       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -11       |\n",
      "|    reward             | 1.9666827 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 4.48      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 15.8       |\n",
      "|    reward             | 0.55596524 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 7.08       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-04-04 to  2018-07-03\n",
      "A2C Sharpe Ratio:  0.2667345453803055\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_252_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 373         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 5           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.07573467 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 349          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050449087 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0566      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.33         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00359     |\n",
      "|    reward               | 0.9939168    |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.38         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 339         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008993065 |\n",
      "|    clip_fraction        | 0.0562      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | 0.0211      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.54        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    reward               | 0.3656372   |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 3.2         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 336          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057084667 |\n",
      "|    clip_fraction        | 0.064        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.000149    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.26         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00662     |\n",
      "|    reward               | 0.3245111    |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.13         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 334          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046516913 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.00064      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.7          |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0036      |\n",
      "|    reward               | 0.109628245  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.47         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-04-04 to  2018-07-03\n",
      "PPO Sharpe Ratio:  -0.12283944049146438\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_252_15\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 166       |\n",
      "|    time_elapsed    | 49        |\n",
      "|    total_timesteps | 8304      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | -625      |\n",
      "|    critic_loss     | 87.7      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 8203      |\n",
      "|    reward          | 2.3221495 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2018-04-04 to  2018-07-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-07-03\n",
      "======Trading from:  2018-07-03 to  2018-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-07-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_315_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 332        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.5      |\n",
      "|    reward             | -0.0642407 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.53       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 320       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -0.271    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -5.67     |\n",
      "|    reward             | 0.5168253 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.52      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 317        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.1       |\n",
      "|    reward             | -1.2256227 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 1.34       |\n",
      "|    reward             | 0.44398472 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.497      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 322        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | 0.74906045 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 3.14       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 317        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -0.891     |\n",
      "|    reward             | 0.82035494 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.136      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 314         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -12.4       |\n",
      "|    reward             | 0.078495935 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 3.85        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 311       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0.059     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -9.21     |\n",
      "|    reward             | 0.4693764 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 2.75      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 308       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -0.0516   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -6.3      |\n",
      "|    reward             | 1.6893284 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 1.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 304        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0.0227     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -17.8      |\n",
      "|    reward             | -1.6315758 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 4.06       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 303        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | -0.00115   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 14.1       |\n",
      "|    reward             | 0.33146083 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 7.12       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 302        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0.0184     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -6.99      |\n",
      "|    reward             | -0.3644799 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -10.6     |\n",
      "|    reward             | -2.116183 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 3.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 298       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | -0.152    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -1.61     |\n",
      "|    reward             | 0.9473256 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 0.728     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 297         |\n",
      "|    iterations         | 1500        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 7500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1499        |\n",
      "|    policy_loss        | 10.6        |\n",
      "|    reward             | -0.05628236 |\n",
      "|    std                | 0.979       |\n",
      "|    value_loss         | 4.32        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 5.78       |\n",
      "|    reward             | 0.44001114 |\n",
      "|    std                | 0.975      |\n",
      "|    value_loss         | 1.13       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | -0.0122    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -13.2      |\n",
      "|    reward             | -1.0933393 |\n",
      "|    std                | 0.975      |\n",
      "|    value_loss         | 9.4        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 294       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 9.47      |\n",
      "|    reward             | 1.7331849 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 5         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 293        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -0.00446   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -6.47      |\n",
      "|    reward             | -1.2939163 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 3.02       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 292        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 34         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -17.7      |\n",
      "|    reward             | 0.44017535 |\n",
      "|    std                | 0.978      |\n",
      "|    value_loss         | 8.34       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2018-07-03 to  2018-10-02\n",
      "A2C Sharpe Ratio:  0.6073941134157957\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_315_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 380        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 5          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.15984146 |\n",
      "-----------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 354          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.006914044  |\n",
      "|    clip_fraction        | 0.0593       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.0259      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.81         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    reward               | -0.013752451 |\n",
      "|    std                  | 0.995        |\n",
      "|    value_loss           | 3.21         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007852277 |\n",
      "|    clip_fraction        | 0.0659      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00404    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00674    |\n",
      "|    reward               | 0.14938359  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.45        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 340          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059767356 |\n",
      "|    clip_fraction        | 0.0502       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.01         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.94         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    reward               | -0.24726865  |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.4          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 338          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072513307 |\n",
      "|    clip_fraction        | 0.058        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | 0.00537      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.15         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00488     |\n",
      "|    reward               | 0.80306005   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 4.53         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-07-03 to  2018-10-02\n",
      "PPO Sharpe Ratio:  -0.037956201196155843\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_315_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 165        |\n",
      "|    time_elapsed    | 51         |\n",
      "|    total_timesteps | 8556       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 790        |\n",
      "|    critic_loss     | 211        |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8455       |\n",
      "|    reward          | 0.72723436 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-07-03 to  2018-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2018-10-02\n",
      "======Trading from:  2018-10-02 to  2019-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2018-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_378_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 334        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | -0.0918873 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 3.01       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 305       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 9.36e-06  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.366     |\n",
      "|    reward             | 0.5254355 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.835     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 312        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.53      |\n",
      "|    reward             | -1.4589832 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.48       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 316       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | 2.41      |\n",
      "|    reward             | 0.6744454 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.12      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 320        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.109      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 10         |\n",
      "|    reward             | -1.9831978 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.34       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 316        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 7.15       |\n",
      "|    reward             | 0.29720998 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.66       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 317       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 6.18      |\n",
      "|    reward             | -0.747417 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 1.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 319        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0.0466     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 6.58       |\n",
      "|    reward             | 0.11901963 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 2.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 320       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.0137    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -7.45     |\n",
      "|    reward             | 1.1198689 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 4.01      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 314        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | -0.5828164 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 6.7        |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 311         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.03       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -11         |\n",
      "|    reward             | 0.074992225 |\n",
      "|    std                | 0.987       |\n",
      "|    value_loss         | 2.53        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 309        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -13.6      |\n",
      "|    reward             | 0.74393225 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 4.67       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 307        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -25.8      |\n",
      "|    reward             | -1.6188539 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 20.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 303        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 3.08       |\n",
      "|    reward             | -2.2679293 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 0.8        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 301       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 0.653     |\n",
      "|    reward             | -0.226074 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 0.293     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 300         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 16.3        |\n",
      "|    reward             | 0.122042544 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 7.13        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -7.11      |\n",
      "|    reward             | 0.18448713 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 297         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 30          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -2.9        |\n",
      "|    reward             | -0.17963608 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 0.593       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -3.46     |\n",
      "|    reward             | 0.7340277 |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 0.211     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 296         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 33          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -2.77       |\n",
      "|    reward             | -0.39491504 |\n",
      "|    std                | 0.976       |\n",
      "|    value_loss         | 0.626       |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2018-10-02 to  2019-01-03\n",
      "A2C Sharpe Ratio:  -0.32062679721701076\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_378_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 384        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 5          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.20313153 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 357         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006735869 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.0107      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.74        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00527    |\n",
      "|    reward               | 0.0970186   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005901846 |\n",
      "|    clip_fraction        | 0.0693      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.0157      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00621    |\n",
      "|    reward               | -0.8056855  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.89        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 342          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055851256 |\n",
      "|    clip_fraction        | 0.0573       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.16        |\n",
      "|    explained_variance   | -0.00481     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.72         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00648     |\n",
      "|    reward               | -0.108291514 |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 3.07         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 339          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054596867 |\n",
      "|    clip_fraction        | 0.0595       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.19        |\n",
      "|    explained_variance   | -0.00443     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.93         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00564     |\n",
      "|    reward               | 0.3173405    |\n",
      "|    std                  | 1.02         |\n",
      "|    value_loss           | 3.7          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2018-10-02 to  2019-01-03\n",
      "PPO Sharpe Ratio:  -0.3899163555397815\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_378_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 163        |\n",
      "|    time_elapsed    | 53         |\n",
      "|    total_timesteps | 8808       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -18.5      |\n",
      "|    critic_loss     | 1.54       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 8707       |\n",
      "|    reward          | 0.07541578 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2018-10-02 to  2019-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-01-03\n",
      "======Trading from:  2019-01-03 to  2019-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_441_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 299        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -12.3      |\n",
      "|    reward             | 0.47477847 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.23       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 307      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.06    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 5.2      |\n",
      "|    reward             | 1.540475 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 3.3      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | -1.9455563 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 9.23       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 306         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -22         |\n",
      "|    reward             | -0.52860785 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 10.7        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 305         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | 11.4        |\n",
      "|    reward             | 0.001958499 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 2.64        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 306         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 9           |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -0.836      |\n",
      "|    reward             | -0.07790426 |\n",
      "|    std                | 0.988       |\n",
      "|    value_loss         | 0.365       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 310          |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 11           |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.04        |\n",
      "|    explained_variance | -0.558       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 1.01         |\n",
      "|    reward             | -0.016895832 |\n",
      "|    std                | 0.99         |\n",
      "|    value_loss         | 0.0511       |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 312        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -0.0349    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 7.58       |\n",
      "|    reward             | 0.19535027 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 1.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 314        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 2.07       |\n",
      "|    reward             | 0.36518636 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 1.3        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 315       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | -0.0582   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 4.48      |\n",
      "|    reward             | 0.5902729 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 0.901     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 315       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -2.54     |\n",
      "|    reward             | 0.9748114 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 0.219     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 315        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 7.11       |\n",
      "|    reward             | -0.3651055 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 0.954      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 315        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 3.65       |\n",
      "|    reward             | -0.3511658 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 0.517      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 312         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 22          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -6.28       |\n",
      "|    reward             | 0.048756447 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 1.36        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 310          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 24           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.97        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | -11          |\n",
      "|    reward             | -0.096076064 |\n",
      "|    std                | 0.977        |\n",
      "|    value_loss         | 4.6          |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 310         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -14.4       |\n",
      "|    reward             | -0.06625332 |\n",
      "|    std                | 0.976       |\n",
      "|    value_loss         | 7.32        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 310        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -8.22      |\n",
      "|    reward             | -2.0453043 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 1.64       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 308        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -3.64      |\n",
      "|    reward             | -1.4356403 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 1.37       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 307        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -34.9      |\n",
      "|    reward             | -2.2966244 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 27.7       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 306        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -5.99      |\n",
      "|    reward             | -0.6619388 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 1.87       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-01-03 to  2019-04-04\n",
      "A2C Sharpe Ratio:  0.6267174053414465\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_441_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 383         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 5           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.071974225 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 350          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046291016 |\n",
      "|    clip_fraction        | 0.0501       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.111       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.27         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00425     |\n",
      "|    reward               | -0.22977528  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.59         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 342          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 17           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073061106 |\n",
      "|    clip_fraction        | 0.0689       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.0057      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00855     |\n",
      "|    reward               | 0.10756534   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.71         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 338          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073265927 |\n",
      "|    clip_fraction        | 0.0637       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.000201    |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.11         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00526     |\n",
      "|    reward               | -0.54205024  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.38         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 336          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005656137  |\n",
      "|    clip_fraction        | 0.0655       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.14        |\n",
      "|    explained_variance   | -0.00365     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.85         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00486     |\n",
      "|    reward               | -0.099671505 |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.28         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-01-03 to  2019-04-04\n",
      "PPO Sharpe Ratio:  0.5243058182201216\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_441_15\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 175      |\n",
      "|    time_elapsed    | 51       |\n",
      "|    total_timesteps | 9060     |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 3.59e+03 |\n",
      "|    critic_loss     | 9.25e+04 |\n",
      "|    learning_rate   | 0.0005   |\n",
      "|    n_updates       | 8959     |\n",
      "|    reward          | 0.0      |\n",
      "---------------------------------\n",
      "======DDPG Validation from:  2019-01-03 to  2019-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-04-04\n",
      "======Trading from:  2019-04-04 to  2019-07-05\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_504_15\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 394          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.07        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -9.05        |\n",
      "|    reward             | -0.030023424 |\n",
      "|    std                | 0.994        |\n",
      "|    value_loss         | 1.64         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 395       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.42      |\n",
      "|    reward             | 0.5738884 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 0.807     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 388        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0.0969     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -3.09      |\n",
      "|    reward             | -1.5446857 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 0.803      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 384        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0.213      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | 0.265      |\n",
      "|    reward             | 0.17969686 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 0.127      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 382        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 16.7       |\n",
      "|    reward             | 0.46111065 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 6.69       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 384       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -4.7      |\n",
      "|    reward             | 0.6229504 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 0.545     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 382      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 9        |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.95    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 1.88     |\n",
      "|    reward             | 1.058889 |\n",
      "|    std                | 0.971    |\n",
      "|    value_loss         | 0.91     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 384        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -3.73      |\n",
      "|    reward             | -1.4230945 |\n",
      "|    std                | 0.975      |\n",
      "|    value_loss         | 0.854      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 386        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -9.91      |\n",
      "|    reward             | 0.71864676 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 2.85       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 387        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -2.6       |\n",
      "|    reward             | -0.9390452 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 2.07       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 384         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.99       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 12.3        |\n",
      "|    reward             | -0.14531967 |\n",
      "|    std                | 0.98        |\n",
      "|    value_loss         | 4.4         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 385        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -36.7      |\n",
      "|    reward             | 0.73906076 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 21.6       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 386      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.98    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 25.1     |\n",
      "|    reward             | -1.0792  |\n",
      "|    std                | 0.978    |\n",
      "|    value_loss         | 19.4     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 386        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -9.1       |\n",
      "|    reward             | 0.20317306 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 383        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 5.22       |\n",
      "|    reward             | 0.16160977 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 0.909      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 379         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 0.558       |\n",
      "|    reward             | -0.07680161 |\n",
      "|    std                | 0.976       |\n",
      "|    value_loss         | 0.0864      |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 376        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.98      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -17.2      |\n",
      "|    reward             | 0.63839114 |\n",
      "|    std                | 0.977      |\n",
      "|    value_loss         | 9.44       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 374       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 3.97      |\n",
      "|    reward             | 0.9770408 |\n",
      "|    std                | 0.982     |\n",
      "|    value_loss         | 0.354     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | 7.29      |\n",
      "|    reward             | 0.7224155 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 1.86      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 370        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 2.83       |\n",
      "|    reward             | -1.1980135 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2019-04-04 to  2019-07-05\n",
      "A2C Sharpe Ratio:  0.14026414632694922\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_504_15\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 462          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.024077354 |\n",
      "-------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 429         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007343798 |\n",
      "|    clip_fraction        | 0.0785      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0138     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.15        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00628    |\n",
      "|    reward               | 0.20909804  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 417         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007803304 |\n",
      "|    clip_fraction        | 0.0665      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00228    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.88        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00664    |\n",
      "|    reward               | 0.07845266  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.04        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 411          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047787214 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.00913      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.34         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00361     |\n",
      "|    reward               | 1.1574616    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 411          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068450742 |\n",
      "|    clip_fraction        | 0.0713       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.00485     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.19         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00659     |\n",
      "|    reward               | 0.30661196   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.54         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-04-04 to  2019-07-05\n",
      "PPO Sharpe Ratio:  0.03521036359869225\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_504_15\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 180       |\n",
      "|    time_elapsed    | 51        |\n",
      "|    total_timesteps | 9312      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 192       |\n",
      "|    critic_loss     | 765       |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9211      |\n",
      "|    reward          | 1.1908848 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-04-04 to  2019-07-05\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-07-05\n",
      "======Trading from:  2019-07-05 to  2019-10-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-07-05\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_567_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 435        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -8.75      |\n",
      "|    reward             | 0.11009762 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.99       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 436       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | -0.0253   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -1.12     |\n",
      "|    reward             | 0.7505325 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.471     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 422         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 3           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.09       |\n",
      "|    explained_variance | -0.00997    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -0.965      |\n",
      "|    reward             | -0.88917387 |\n",
      "|    std                | 0.999       |\n",
      "|    value_loss         | 0.212       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 430         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 4           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -4.87       |\n",
      "|    reward             | -0.18713059 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 1.01        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 428       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 5         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0.0465    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 15.9      |\n",
      "|    reward             | 2.5798545 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 6.45      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 422        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | -0.046     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 1.92       |\n",
      "|    reward             | 0.49106336 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 1.06       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 419         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.06       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | 13.8        |\n",
      "|    reward             | -0.19116597 |\n",
      "|    std                | 0.994       |\n",
      "|    value_loss         | 4.79        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 418       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.66     |\n",
      "|    reward             | 1.1502227 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 0.76      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 413       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 18.2      |\n",
      "|    reward             | 1.6732936 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 8.79      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 413        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 12.4       |\n",
      "|    reward             | -1.6080636 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 3.86       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 405        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0.442      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 7.94       |\n",
      "|    reward             | -1.4106607 |\n",
      "|    std                | 0.981      |\n",
      "|    value_loss         | 1.43       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 406         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 14          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | -0.0583     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 13.1        |\n",
      "|    reward             | -0.42734224 |\n",
      "|    std                | 0.981       |\n",
      "|    value_loss         | 5.06        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 404        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 0.148      |\n",
      "|    reward             | -0.4402403 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 0.12       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 402       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 15.7      |\n",
      "|    reward             | -5.548047 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 8.81      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 401       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0.172     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -12.1     |\n",
      "|    reward             | 1.5646671 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 401        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 13.5       |\n",
      "|    reward             | -0.8303502 |\n",
      "|    std                | 0.976      |\n",
      "|    value_loss         | 5.21       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 403          |\n",
      "|    iterations         | 1700         |\n",
      "|    time_elapsed       | 21           |\n",
      "|    total_timesteps    | 8500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.96        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1699         |\n",
      "|    policy_loss        | -4.69        |\n",
      "|    reward             | -0.035758544 |\n",
      "|    std                | 0.973        |\n",
      "|    value_loss         | 0.645        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 401       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 21.4      |\n",
      "|    reward             | 1.8312159 |\n",
      "|    std                | 0.975     |\n",
      "|    value_loss         | 8.91      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 396       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -9.7      |\n",
      "|    reward             | 1.1309232 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 12.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 392       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.96     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -0.385    |\n",
      "|    reward             | 2.0418124 |\n",
      "|    std                | 0.973     |\n",
      "|    value_loss         | 3.7       |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2019-07-05 to  2019-10-03\n",
      "A2C Sharpe Ratio:  -0.14608340503895184\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_567_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 416         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | 0.107726514 |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 396        |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00545664 |\n",
      "|    clip_fraction        | 0.0501     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.11      |\n",
      "|    explained_variance   | -0.121     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.71       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.00371   |\n",
      "|    reward               | -0.7589529 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 4.27       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 396         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005370029 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0096      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    reward               | 0.33062467  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.33        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 389          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 21           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051157135 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.0118       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.42         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00389     |\n",
      "|    reward               | 0.04791778   |\n",
      "|    std                  | 0.999        |\n",
      "|    value_loss           | 3.16         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006248771 |\n",
      "|    clip_fraction        | 0.0614      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | 0.0139      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.22        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00538    |\n",
      "|    reward               | 0.4065892   |\n",
      "|    std                  | 0.994       |\n",
      "|    value_loss           | 3.81        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2019-07-05 to  2019-10-03\n",
      "PPO Sharpe Ratio:  -0.1491578390264053\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_567_15\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 176       |\n",
      "|    time_elapsed    | 54        |\n",
      "|    total_timesteps | 9564      |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 465       |\n",
      "|    critic_loss     | 3.46      |\n",
      "|    learning_rate   | 0.0005    |\n",
      "|    n_updates       | 9463      |\n",
      "|    reward          | 1.2724007 |\n",
      "----------------------------------\n",
      "======DDPG Validation from:  2019-07-05 to  2019-10-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2019-10-03\n",
      "======Trading from:  2019-10-03 to  2020-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2019-10-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_630_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 356        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -11.6      |\n",
      "|    reward             | 0.48451748 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 4.37       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 367       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 5.62      |\n",
      "|    reward             | 1.4688494 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 3.2       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -18.8     |\n",
      "|    reward             | -1.939423 |\n",
      "|    std                | 0.984     |\n",
      "|    value_loss         | 9.16      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 377        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -22.5      |\n",
      "|    reward             | -0.5300958 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 382        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | 8.29       |\n",
      "|    reward             | 0.37745404 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 1.96       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 382       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.205    |\n",
      "|    reward             | 0.1867203 |\n",
      "|    std                | 0.985     |\n",
      "|    value_loss         | 0.516     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 382        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 16.2       |\n",
      "|    reward             | -1.0125127 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 6.21       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 385       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 27.1      |\n",
      "|    reward             | 1.5049105 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 22        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 379       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -2.44     |\n",
      "|    reward             | -2.112058 |\n",
      "|    std                | 0.987     |\n",
      "|    value_loss         | 8.03      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 380         |\n",
      "|    iterations         | 1000        |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 5000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | -0.000508   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 999         |\n",
      "|    policy_loss        | -0.587      |\n",
      "|    reward             | 0.054162286 |\n",
      "|    std                | 0.983       |\n",
      "|    value_loss         | 7.81        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 374        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 2.08       |\n",
      "|    reward             | -0.9141546 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 0.34       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 376       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 15        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 4.58      |\n",
      "|    reward             | 1.1048996 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 0.873     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -6.37     |\n",
      "|    reward             | 1.1043444 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 0.781     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 379      |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.07    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 5        |\n",
      "|    reward             | 2.243476 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 2.1      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 4.1       |\n",
      "|    reward             | 1.9103376 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 0.984     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 378        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -11.4      |\n",
      "|    reward             | -0.4710359 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.76       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 378       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 3.22      |\n",
      "|    reward             | 1.0485481 |\n",
      "|    std                | 0.99      |\n",
      "|    value_loss         | 0.763     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 378        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 7.1        |\n",
      "|    reward             | -0.2120388 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 10.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 377       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -29.6     |\n",
      "|    reward             | 0.3292881 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 20.5      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 377         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | -19         |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 17.7        |\n",
      "|    reward             | -0.43432146 |\n",
      "|    std                | 0.993       |\n",
      "|    value_loss         | 3.87        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2019-10-03 to  2020-01-03\n",
      "A2C Sharpe Ratio:  0.5634932120543863\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_630_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 492         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.07961935 |\n",
      "------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 427          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060648047 |\n",
      "|    clip_fraction        | 0.0649       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0878      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.71         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00739     |\n",
      "|    reward               | 0.010673437  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.58         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 409          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056769936 |\n",
      "|    clip_fraction        | 0.0521       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.00264     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.19         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00491     |\n",
      "|    reward               | -1.0685066   |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.51         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 398          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 20           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051173093 |\n",
      "|    clip_fraction        | 0.0445       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.00803      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.27         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00577     |\n",
      "|    reward               | 0.98970956   |\n",
      "|    std                  | 0.996        |\n",
      "|    value_loss           | 3.94         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 396          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 25           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055743875 |\n",
      "|    clip_fraction        | 0.0511       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.06        |\n",
      "|    explained_variance   | 0.02         |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.02         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00463     |\n",
      "|    reward               | -0.96149564  |\n",
      "|    std                  | 0.991        |\n",
      "|    value_loss           | 3.2          |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2019-10-03 to  2020-01-03\n",
      "PPO Sharpe Ratio:  0.44030212795715523\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_630_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 165        |\n",
      "|    time_elapsed    | 59         |\n",
      "|    total_timesteps | 9816       |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -1.02e+03  |\n",
      "|    critic_loss     | 53.5       |\n",
      "|    learning_rate   | 0.0005     |\n",
      "|    n_updates       | 9715       |\n",
      "|    reward          | -3.0861108 |\n",
      "-----------------------------------\n",
      "======DDPG Validation from:  2019-10-03 to  2020-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-01-03\n",
      "======Trading from:  2020-01-03 to  2020-04-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_693_15\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 297          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.1         |\n",
      "|    explained_variance | 5.96e-08     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -9.53        |\n",
      "|    reward             | -0.015496187 |\n",
      "|    std                | 1            |\n",
      "|    value_loss         | 2.24         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -4.49     |\n",
      "|    reward             | 0.8466432 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.38      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 297        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | -0.0788    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -14.6      |\n",
      "|    reward             | -1.5726187 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 6.38       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 295          |\n",
      "|    iterations         | 400          |\n",
      "|    time_elapsed       | 6            |\n",
      "|    total_timesteps    | 2000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | -1.19e-07    |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 399          |\n",
      "|    policy_loss        | -6.49        |\n",
      "|    reward             | -0.031280268 |\n",
      "|    std                | 0.993        |\n",
      "|    value_loss         | 0.613        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 296         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.07       |\n",
      "|    explained_variance | -0.0818     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -8.43       |\n",
      "|    reward             | -0.49648333 |\n",
      "|    std                | 0.996       |\n",
      "|    value_loss         | 1.96        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 297         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | -0.176      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | -27.3       |\n",
      "|    reward             | -0.13581681 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 19.7        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 299        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0.189      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -8.43      |\n",
      "|    reward             | 0.25135785 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 1.21       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 299         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | -6.01       |\n",
      "|    reward             | -0.18996719 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.14        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -3.69     |\n",
      "|    reward             | 1.1583855 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 0.347     |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 302          |\n",
      "|    iterations         | 1000         |\n",
      "|    time_elapsed       | 16           |\n",
      "|    total_timesteps    | 5000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.12        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 999          |\n",
      "|    policy_loss        | 2.25         |\n",
      "|    reward             | -0.021049537 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.54         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 303        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -2.54      |\n",
      "|    reward             | 0.39585423 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 3.49       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 301       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 3.26      |\n",
      "|    reward             | 0.6314721 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.402     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 302         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 12          |\n",
      "|    reward             | -0.92879164 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 3.64        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 303       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 4.69      |\n",
      "|    reward             | 0.5003792 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.667     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 301       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.15     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 38.5      |\n",
      "|    reward             | 3.1677554 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 29.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.17     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 29.9      |\n",
      "|    reward             | 0.6774284 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 17        |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 301       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -9.12     |\n",
      "|    reward             | 3.6257877 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.88      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 302        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -58.4      |\n",
      "|    reward             | -1.4294457 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 75         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 302       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -7.88     |\n",
      "|    reward             | 1.8453379 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.09      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 303      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.19    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -1.82    |\n",
      "|    reward             | -2.0311  |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.467    |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-01-03 to  2020-04-03\n",
      "A2C Sharpe Ratio:  -0.03325294537566037\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_693_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 381         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 5           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.08392328 |\n",
      "------------------------------------\n",
      "day: 2516, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 452449.95\n",
      "total_reward: -547550.05\n",
      "total_cost: 1031742.49\n",
      "total_trades: 9545\n",
      "Sharpe: -0.491\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 350          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 11           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068562236 |\n",
      "|    clip_fraction        | 0.0507       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.0969      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.41         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00514     |\n",
      "|    reward               | 0.6939998    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.54         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 326          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.004005668  |\n",
      "|    clip_fraction        | 0.0187       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0133      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.65         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    reward               | -0.082005896 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.34         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 315         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008232405 |\n",
      "|    clip_fraction        | 0.0924      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | -0.00716    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00756    |\n",
      "|    reward               | 0.15712261  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.34        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 311        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 32         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01150739 |\n",
      "|    clip_fraction        | 0.106      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.13      |\n",
      "|    explained_variance   | -0.000747  |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 1.42       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00809   |\n",
      "|    reward               | 0.38053483 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 3.34       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2020-01-03 to  2020-04-03\n",
      "PPO Sharpe Ratio:  -0.15870496671473575\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_693_15\n",
      "day: 2516, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2655883.81\n",
      "total_reward: 1655883.81\n",
      "total_cost: 43664.67\n",
      "total_trades: 5265\n",
      "Sharpe: 0.758\n",
      "=================================\n",
      "======DDPG Validation from:  2020-01-03 to  2020-04-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-04-03\n",
      "======Trading from:  2020-04-03 to  2020-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-04-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_756_15\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 299           |\n",
      "|    iterations         | 100           |\n",
      "|    time_elapsed       | 1             |\n",
      "|    total_timesteps    | 500           |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -7            |\n",
      "|    explained_variance | -0.0967       |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 99            |\n",
      "|    policy_loss        | -9.71         |\n",
      "|    reward             | -0.0031359391 |\n",
      "|    std                | 0.982         |\n",
      "|    value_loss         | 3.24          |\n",
      "-----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -3.38     |\n",
      "|    reward             | 0.656435  |\n",
      "|    std                | 0.98      |\n",
      "|    value_loss         | 1.21      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 300        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 0.116      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -8.16      |\n",
      "|    reward             | -1.9297367 |\n",
      "|    std                | 0.98       |\n",
      "|    value_loss         | 2.79       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 301       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.99     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -4.45     |\n",
      "|    reward             | 0.2592513 |\n",
      "|    std                | 0.979     |\n",
      "|    value_loss         | 0.649     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 299         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.97       |\n",
      "|    explained_variance | -0.291      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -8.69       |\n",
      "|    reward             | -0.67439914 |\n",
      "|    std                | 0.975       |\n",
      "|    value_loss         | 2.83        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 300        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | -0.0338    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 8.43       |\n",
      "|    reward             | -1.2566931 |\n",
      "|    std                | 0.971      |\n",
      "|    value_loss         | 4.26       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 299       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | 8.47      |\n",
      "|    reward             | 1.0106232 |\n",
      "|    std                | 0.97      |\n",
      "|    value_loss         | 1.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.92      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 0.911      |\n",
      "|    reward             | -1.0794649 |\n",
      "|    std                | 0.966      |\n",
      "|    value_loss         | 0.399      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.92      |\n",
      "|    explained_variance | -0.199     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -15.2      |\n",
      "|    reward             | -0.2747669 |\n",
      "|    std                | 0.966      |\n",
      "|    value_loss         | 5.9        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 299       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.93     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 18.1      |\n",
      "|    reward             | 0.6043922 |\n",
      "|    std                | 0.968     |\n",
      "|    value_loss         | 20.1      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.92     |\n",
      "|    explained_variance | -0.314    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -4.72     |\n",
      "|    reward             | 0.9500522 |\n",
      "|    std                | 0.966     |\n",
      "|    value_loss         | 0.679     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 296       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | -0.0181   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 7.03      |\n",
      "|    reward             | 0.7398824 |\n",
      "|    std                | 0.971     |\n",
      "|    value_loss         | 0.872     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 297       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | -0.00176  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 3.8       |\n",
      "|    reward             | 1.1939576 |\n",
      "|    std                | 0.971     |\n",
      "|    value_loss         | 0.517     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 298         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.92       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -6.77       |\n",
      "|    reward             | -0.13584854 |\n",
      "|    std                | 0.967       |\n",
      "|    value_loss         | 1.73        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 298       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.92     |\n",
      "|    explained_variance | -0.0989   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | 3.91      |\n",
      "|    reward             | 1.4833332 |\n",
      "|    std                | 0.967     |\n",
      "|    value_loss         | 0.566     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 299         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.91       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 1.21        |\n",
      "|    reward             | -0.18831582 |\n",
      "|    std                | 0.962       |\n",
      "|    value_loss         | 0.348       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.88     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 16        |\n",
      "|    reward             | 0.9490479 |\n",
      "|    std                | 0.959     |\n",
      "|    value_loss         | 8.84      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 301        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.89      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 3.09       |\n",
      "|    reward             | -1.5292445 |\n",
      "|    std                | 0.959      |\n",
      "|    value_loss         | 0.386      |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 301          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 31           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.85        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | -0.0416      |\n",
      "|    reward             | -0.068161674 |\n",
      "|    std                | 0.953        |\n",
      "|    value_loss         | 0.13         |\n",
      "----------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 299      |\n",
      "|    iterations         | 2000     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 10000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -6.86    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -49.9    |\n",
      "|    reward             | 9.689447 |\n",
      "|    std                | 0.954    |\n",
      "|    value_loss         | 49.2     |\n",
      "------------------------------------\n",
      "======A2C Validation from:  2020-04-03 to  2020-07-06\n",
      "A2C Sharpe Ratio:  0.4132700140813879\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_756_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 349         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 5           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.11856449 |\n",
      "------------------------------------\n",
      "day: 2579, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 395029.69\n",
      "total_reward: -604970.31\n",
      "total_cost: 1235592.45\n",
      "total_trades: 10046\n",
      "Sharpe: -0.474\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 323         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004737144 |\n",
      "|    clip_fraction        | 0.0468      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.0225     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.12        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00335    |\n",
      "|    reward               | 0.5095332   |\n",
      "|    std                  | 0.997       |\n",
      "|    value_loss           | 4.08        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 310          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075743673 |\n",
      "|    clip_fraction        | 0.0712       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | -0.00716     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 3.27         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00699     |\n",
      "|    reward               | 0.03572322   |\n",
      "|    std                  | 0.998        |\n",
      "|    value_loss           | 6.25         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 307         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009937139 |\n",
      "|    clip_fraction        | 0.0999      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.08       |\n",
      "|    explained_variance   | -0.00272    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00795    |\n",
      "|    reward               | 2.1635513   |\n",
      "|    std                  | 0.996       |\n",
      "|    value_loss           | 5.7         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 320          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051378976 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.07        |\n",
      "|    explained_variance   | 0.000701     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.82         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00536     |\n",
      "|    reward               | -0.41915047  |\n",
      "|    std                  | 0.994        |\n",
      "|    value_loss           | 5.04         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2020-04-03 to  2020-07-06\n",
      "PPO Sharpe Ratio:  0.2652991289981352\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_756_15\n",
      "day: 2579, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2846406.10\n",
      "total_reward: 1846406.10\n",
      "total_cost: 998.92\n",
      "total_trades: 5158\n",
      "Sharpe: 0.626\n",
      "=================================\n",
      "======DDPG Validation from:  2020-04-03 to  2020-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-07-06\n",
      "======Trading from:  2020-07-06 to  2020-10-02\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_819_15\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 336         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -12.1       |\n",
      "|    reward             | 0.123887986 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.33        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 329       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | -0.0741   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.65      |\n",
      "|    reward             | 1.6868993 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 1.69      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | -2.5580368 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 5.96       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -32.4      |\n",
      "|    reward             | -0.4729934 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 32.5       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 323        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -15.5      |\n",
      "|    reward             | -1.3940287 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 11.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | -3.78      |\n",
      "|    reward             | -1.7534093 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 1.98       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 326        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | -1.5299814 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 2.52       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 320       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 10.1      |\n",
      "|    reward             | 3.1898813 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 4.86      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 314       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -0.143    |\n",
      "|    reward             | 1.2383446 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 0.135     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -41.5     |\n",
      "|    reward             | 3.2739768 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 51.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 306         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -1.32       |\n",
      "|    reward             | 0.046494965 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 0.0617      |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 303       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -18.3     |\n",
      "|    reward             | -1.781475 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 11.6      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 302         |\n",
      "|    iterations         | 1300        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 6500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.14       |\n",
      "|    explained_variance | 0.00269     |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1299        |\n",
      "|    policy_loss        | 7.05        |\n",
      "|    reward             | -0.49076024 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.63        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 300       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.14     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -21.3     |\n",
      "|    reward             | 1.2094827 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 13.3      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -82.1      |\n",
      "|    reward             | -4.3736563 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 151        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 4.23       |\n",
      "|    reward             | -1.4483495 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.985      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 0.794      |\n",
      "|    reward             | -2.7695396 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 296        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 6.37       |\n",
      "|    reward             | -1.9782373 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.83       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 295        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -11.5      |\n",
      "|    reward             | 0.40478772 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 294        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.21      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -14.4      |\n",
      "|    reward             | -3.2326572 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 3.28       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2020-07-06 to  2020-10-02\n",
      "A2C Sharpe Ratio:  0.11891516745058593\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_819_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 403         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 5           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.05192759 |\n",
      "------------------------------------\n",
      "day: 2642, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 667246.20\n",
      "total_reward: -332753.80\n",
      "total_cost: 1268099.16\n",
      "total_trades: 10180\n",
      "Sharpe: -0.148\n",
      "=================================\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 377           |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 10            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0053683566  |\n",
      "|    clip_fraction        | 0.0551        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -7.09         |\n",
      "|    explained_variance   | -0.0311       |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.94          |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00386      |\n",
      "|    reward               | -0.0073536504 |\n",
      "|    std                  | 0.999         |\n",
      "|    value_loss           | 3.76          |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 373         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007234557 |\n",
      "|    clip_fraction        | 0.0778      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.00451    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.32        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00817    |\n",
      "|    reward               | 0.25243223  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.2         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 356          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074570267 |\n",
      "|    clip_fraction        | 0.0904       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.00818      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.84         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00654     |\n",
      "|    reward               | -1.2062511   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.83         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008412204 |\n",
      "|    clip_fraction        | 0.0938      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0175      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.46        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00594    |\n",
      "|    reward               | 0.253807    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.52        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-07-06 to  2020-10-02\n",
      "PPO Sharpe Ratio:  0.0860601575455361\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_819_15\n",
      "day: 2642, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1848561.53\n",
      "total_reward: 848561.53\n",
      "total_cost: 998.41\n",
      "total_trades: 7926\n",
      "Sharpe: 0.422\n",
      "=================================\n",
      "======DDPG Validation from:  2020-07-06 to  2020-10-02\n",
      "======Best Model Retraining from:  2010-01-01 to  2020-10-02\n",
      "======Trading from:  2020-10-02 to  2021-01-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2020-10-02\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_882_15\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 267       |\n",
      "|    iterations         | 100       |\n",
      "|    time_elapsed       | 1         |\n",
      "|    total_timesteps    | 500       |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.16     |\n",
      "|    explained_variance | -0.322    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 99        |\n",
      "|    policy_loss        | -11.1     |\n",
      "|    reward             | 0.2367623 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 4.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 269       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 3         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.18     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.142     |\n",
      "|    reward             | 1.5586544 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.39      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 269        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -9.75      |\n",
      "|    reward             | -2.3528566 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 4.65       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 268        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -21.3      |\n",
      "|    reward             | -0.3373532 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.97       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 269        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 9          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.18      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -20.6      |\n",
      "|    reward             | -1.1277564 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 9.23       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 272         |\n",
      "|    iterations         | 600         |\n",
      "|    time_elapsed       | 11          |\n",
      "|    total_timesteps    | 3000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.19       |\n",
      "|    explained_variance | -0.000188   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 599         |\n",
      "|    policy_loss        | 0.268       |\n",
      "|    reward             | -0.60860693 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 4.74        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 274        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.2       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | -6.49      |\n",
      "|    reward             | 0.39705104 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.81       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 276        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 12.3       |\n",
      "|    reward             | 0.16706128 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 277        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 15         |\n",
      "|    reward             | 0.39745834 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.63       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 278        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -11.1      |\n",
      "|    reward             | 0.22272758 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 6.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 279        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -20.7      |\n",
      "|    reward             | 0.21162155 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 280        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 14         |\n",
      "|    reward             | -1.5878352 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 5.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 5.06       |\n",
      "|    reward             | 0.56192094 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.84       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 282         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.15       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | -1.38       |\n",
      "|    reward             | -0.32756087 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 1.5         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 283        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.15      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 19.9       |\n",
      "|    reward             | -4.0091257 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 20.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 284       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | -0.000374 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 150       |\n",
      "|    reward             | 6.068277  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 526       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 285        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -0.000267  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | 27.5       |\n",
      "|    reward             | -2.4463446 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 16.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 285       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 31        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 8.02      |\n",
      "|    reward             | 0.2334798 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.8       |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 286      |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 9500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.1     |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -3.83    |\n",
      "|    reward             | 1.122306 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 1.98     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 287         |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | 14          |\n",
      "|    reward             | -0.77279717 |\n",
      "|    std                | 0.997       |\n",
      "|    value_loss         | 5.17        |\n",
      "---------------------------------------\n",
      "======A2C Validation from:  2020-10-02 to  2021-01-04\n",
      "A2C Sharpe Ratio:  0.5077101781107471\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_882_15\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 401          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 5            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.086448416 |\n",
      "-------------------------------------\n",
      "day: 2705, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 516800.05\n",
      "total_reward: -483199.95\n",
      "total_cost: 1272443.63\n",
      "total_trades: 10424\n",
      "Sharpe: -0.282\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 379          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055191065 |\n",
      "|    clip_fraction        | 0.054        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0315      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.17         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00568     |\n",
      "|    reward               | 0.29339907   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.36         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 368        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 16         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00601937 |\n",
      "|    clip_fraction        | 0.0538     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.1       |\n",
      "|    explained_variance   | 0.0105     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.42       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00647   |\n",
      "|    reward               | 0.28224298 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 4.03       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 357          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 22           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0056983223 |\n",
      "|    clip_fraction        | 0.0467       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.0162       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.14         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00474     |\n",
      "|    reward               | -0.17759083  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.83         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009189839 |\n",
      "|    clip_fraction        | 0.0799      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.00137    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.81        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00706    |\n",
      "|    reward               | 0.030457163 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.85        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2020-10-02 to  2021-01-04\n",
      "PPO Sharpe Ratio:  0.3133522707265055\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_882_15\n",
      "day: 2705, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2384309.31\n",
      "total_reward: 1384309.31\n",
      "total_cost: 1124.82\n",
      "total_trades: 8117\n",
      "Sharpe: 0.544\n",
      "=================================\n",
      "======DDPG Validation from:  2020-10-02 to  2021-01-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-01-04\n",
      "======Trading from:  2021-01-04 to  2021-04-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-01-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_945_15\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 314         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -10.2       |\n",
      "|    reward             | 0.009034167 |\n",
      "|    std                | 0.984       |\n",
      "|    value_loss         | 2.93        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 304        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.43      |\n",
      "|    reward             | 0.80658966 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 317        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.97      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -11.1      |\n",
      "|    reward             | -2.5635421 |\n",
      "|    std                | 0.975      |\n",
      "|    value_loss         | 3.8        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 326        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 6          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.99      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -4.71      |\n",
      "|    reward             | 0.34196973 |\n",
      "|    std                | 0.979      |\n",
      "|    value_loss         | 0.999      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 333       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 7         |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.97     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | -13.6     |\n",
      "|    reward             | -0.824851 |\n",
      "|    std                | 0.976     |\n",
      "|    value_loss         | 5.32      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.95     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 2.66      |\n",
      "|    reward             | 1.3496653 |\n",
      "|    std                | 0.972     |\n",
      "|    value_loss         | 0.78      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 10         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 10.8       |\n",
      "|    reward             | 0.95613974 |\n",
      "|    std                | 0.971      |\n",
      "|    value_loss         | 3.67       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 339       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.93     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -1.43     |\n",
      "|    reward             | 0.8889324 |\n",
      "|    std                | 0.968     |\n",
      "|    value_loss         | 0.0737    |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 340         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.94       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | 31          |\n",
      "|    reward             | -0.29816687 |\n",
      "|    std                | 0.97        |\n",
      "|    value_loss         | 22.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.95      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 18.3       |\n",
      "|    reward             | -4.7393384 |\n",
      "|    std                | 0.972      |\n",
      "|    value_loss         | 17.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 339         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 16          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.98       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | -60.3       |\n",
      "|    reward             | -0.08726484 |\n",
      "|    std                | 0.977       |\n",
      "|    value_loss         | 111         |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 341        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -1.23      |\n",
      "|    reward             | -3.0292318 |\n",
      "|    std                | 0.975      |\n",
      "|    value_loss         | 1.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 342        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.96      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 12.2       |\n",
      "|    reward             | 0.85006124 |\n",
      "|    std                | 0.975      |\n",
      "|    value_loss         | 5.46       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 343       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 10.2      |\n",
      "|    reward             | 1.9559712 |\n",
      "|    std                | 0.969     |\n",
      "|    value_loss         | 3.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 343        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.94      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 2.62       |\n",
      "|    reward             | 0.16415156 |\n",
      "|    std                | 0.97       |\n",
      "|    value_loss         | 0.303      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -6.94     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -24.5     |\n",
      "|    reward             | 2.5513945 |\n",
      "|    std                | 0.97      |\n",
      "|    value_loss         | 13.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 345        |\n",
      "|    iterations         | 1700       |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 8500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1699       |\n",
      "|    policy_loss        | -2.36      |\n",
      "|    reward             | 0.67853385 |\n",
      "|    std                | 0.968      |\n",
      "|    value_loss         | 0.255      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 347         |\n",
      "|    iterations         | 1800        |\n",
      "|    time_elapsed       | 25          |\n",
      "|    total_timesteps    | 9000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.91       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1799        |\n",
      "|    policy_loss        | -13.2       |\n",
      "|    reward             | 0.120140806 |\n",
      "|    std                | 0.965       |\n",
      "|    value_loss         | 3.65        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 349         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -6.93       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -15.9       |\n",
      "|    reward             | -0.47962883 |\n",
      "|    std                | 0.968       |\n",
      "|    value_loss         | 4.81        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 349        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -6.93      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 2.08       |\n",
      "|    reward             | -1.2831829 |\n",
      "|    std                | 0.967      |\n",
      "|    value_loss         | 1.9        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-01-04 to  2021-04-06\n",
      "A2C Sharpe Ratio:  0.18860060626411068\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_945_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 483         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.07221561 |\n",
      "------------------------------------\n",
      "day: 2768, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 743981.55\n",
      "total_reward: -256018.45\n",
      "total_cost: 1409364.70\n",
      "total_trades: 10728\n",
      "Sharpe: -0.070\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 446         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008324043 |\n",
      "|    clip_fraction        | 0.0664      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.00674    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.24        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00439    |\n",
      "|    reward               | 0.20076077  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 5.11        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 445          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053082933 |\n",
      "|    clip_fraction        | 0.0416       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.0112       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.87         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00348     |\n",
      "|    reward               | -1.8396704   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.54         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 445         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007827686 |\n",
      "|    clip_fraction        | 0.0765      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0435      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2           |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00707    |\n",
      "|    reward               | 0.05045827  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.18        |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| time/                   |               |\n",
      "|    fps                  | 446           |\n",
      "|    iterations           | 5             |\n",
      "|    time_elapsed         | 22            |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.005306205   |\n",
      "|    clip_fraction        | 0.0701        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -7.1          |\n",
      "|    explained_variance   | 0.0474        |\n",
      "|    learning_rate        | 0.00025       |\n",
      "|    loss                 | 1.38          |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00552      |\n",
      "|    reward               | -0.0074389614 |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.11          |\n",
      "-------------------------------------------\n",
      "======PPO Validation from:  2021-01-04 to  2021-04-06\n",
      "PPO Sharpe Ratio:  0.07028143694260121\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_945_15\n",
      "day: 2768, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3510636.21\n",
      "total_reward: 2510636.21\n",
      "total_cost: 998.63\n",
      "total_trades: 8303\n",
      "Sharpe: 0.740\n",
      "=================================\n",
      "======DDPG Validation from:  2021-01-04 to  2021-04-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-04-06\n",
      "======Trading from:  2021-04-06 to  2021-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-04-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1008_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 345        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0.0751     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -6.45      |\n",
      "|    reward             | 0.05907881 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.55       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 350        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.17      |\n",
      "|    explained_variance | -0.0658    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -0.747     |\n",
      "|    reward             | 0.39576745 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.462      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 352        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -4.27      |\n",
      "|    reward             | -1.0478283 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.816      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 354        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.16      |\n",
      "|    explained_variance | -0.143     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.96      |\n",
      "|    reward             | 0.17736328 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.383      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 354        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -3.25      |\n",
      "|    reward             | -0.1769302 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.363      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 354       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.12     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -9.36     |\n",
      "|    reward             | 0.7150826 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.66      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 352          |\n",
      "|    iterations         | 700          |\n",
      "|    time_elapsed       | 9            |\n",
      "|    total_timesteps    | 3500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.08        |\n",
      "|    explained_variance | 1.19e-07     |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 699          |\n",
      "|    policy_loss        | 2.08         |\n",
      "|    reward             | -0.124718465 |\n",
      "|    std                | 0.998        |\n",
      "|    value_loss         | 0.206        |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 353       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.04     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 6.75      |\n",
      "|    reward             | 0.4033774 |\n",
      "|    std                | 0.989     |\n",
      "|    value_loss         | 0.93      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 352        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 1.6        |\n",
      "|    reward             | 0.21230476 |\n",
      "|    std                | 0.982      |\n",
      "|    value_loss         | 0.166      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 352       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.01     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 9.89      |\n",
      "|    reward             | 0.7318765 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 2.6       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 349         |\n",
      "|    iterations         | 1100        |\n",
      "|    time_elapsed       | 15          |\n",
      "|    total_timesteps    | 5500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1099        |\n",
      "|    policy_loss        | 2.7         |\n",
      "|    reward             | -0.44715837 |\n",
      "|    std                | 0.981       |\n",
      "|    value_loss         | 0.569       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 349        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 16         |\n",
      "|    reward             | -0.5770103 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 7.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 348        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7         |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 13.2       |\n",
      "|    reward             | -1.2965109 |\n",
      "|    std                | 0.983      |\n",
      "|    value_loss         | 6.1        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 348       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 20        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 2.68      |\n",
      "|    reward             | -1.310703 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 1.59      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 348       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.00241   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -7.78     |\n",
      "|    reward             | 1.8163221 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 1.24      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 349        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -5.58      |\n",
      "|    reward             | -0.0859231 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 1.41       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 349         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7          |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | 29.9        |\n",
      "|    reward             | 0.061132137 |\n",
      "|    std                | 0.982       |\n",
      "|    value_loss         | 17.2        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 350       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | 10.4      |\n",
      "|    reward             | 1.7950869 |\n",
      "|    std                | 0.983     |\n",
      "|    value_loss         | 3.96      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 350          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 27           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -6.99        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 15.9         |\n",
      "|    reward             | -0.098453514 |\n",
      "|    std                | 0.98         |\n",
      "|    value_loss         | 7.01         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 350       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7        |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 19.5      |\n",
      "|    reward             | 1.8471788 |\n",
      "|    std                | 0.981     |\n",
      "|    value_loss         | 12.3      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-04-06 to  2021-07-06\n",
      "A2C Sharpe Ratio:  0.2299119979149487\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1008_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 477        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.18076077 |\n",
      "-----------------------------------\n",
      "day: 2831, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 693466.56\n",
      "total_reward: -306533.44\n",
      "total_cost: 1340949.80\n",
      "total_trades: 10839\n",
      "Sharpe: -0.112\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 456          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044086236 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0263      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.95         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00455     |\n",
      "|    reward               | -0.73489976  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.09         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 449          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060783215 |\n",
      "|    clip_fraction        | 0.0618       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0188      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.04         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00576     |\n",
      "|    reward               | 2.7767725    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 4.23         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 445         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006226384 |\n",
      "|    clip_fraction        | 0.0446      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.13       |\n",
      "|    explained_variance   | 0.0367      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.39        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00369    |\n",
      "|    reward               | -0.40323612 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.73        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 436          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061304793 |\n",
      "|    clip_fraction        | 0.0909       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.13        |\n",
      "|    explained_variance   | -0.0058      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.56         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.006       |\n",
      "|    reward               | 0.5517258    |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.28         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-04-06 to  2021-07-06\n",
      "PPO Sharpe Ratio:  -0.03440503676016049\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1008_15\n",
      "day: 2831, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3049935.04\n",
      "total_reward: 2049935.04\n",
      "total_cost: 997.55\n",
      "total_trades: 8492\n",
      "Sharpe: 0.647\n",
      "=================================\n",
      "======DDPG Validation from:  2021-04-06 to  2021-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-07-06\n",
      "======Trading from:  2021-07-06 to  2021-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1071_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 358        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0.00788    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | 0.16286103 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 3.29       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 359        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 2          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.08      |\n",
      "|    reward             | 0.87538564 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.62       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 356        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -11        |\n",
      "|    reward             | -1.7891641 |\n",
      "|    std                | 0.992      |\n",
      "|    value_loss         | 4.61       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 355        |\n",
      "|    iterations         | 400        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 2000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | -0.0312    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 399        |\n",
      "|    policy_loss        | -3.63      |\n",
      "|    reward             | 0.36571693 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 0.516      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 355        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 1.79e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -9.05      |\n",
      "|    reward             | -0.5148772 |\n",
      "|    std                | 0.988      |\n",
      "|    value_loss         | 2.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 354        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 14.5       |\n",
      "|    reward             | -3.4325483 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 5.38       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 9         |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -4.97     |\n",
      "|    reward             | 1.1266878 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 2.64      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 355      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.08    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -4.72    |\n",
      "|    reward             | 0.547023 |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 1.19     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 355        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -0.748     |\n",
      "|    reward             | -0.6579389 |\n",
      "|    std                | 0.997      |\n",
      "|    value_loss         | 0.453      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 355        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 14         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 4.07       |\n",
      "|    reward             | -0.9732644 |\n",
      "|    std                | 0.996      |\n",
      "|    value_loss         | 0.822      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 355        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -28.9      |\n",
      "|    reward             | -0.6779226 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 19.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 0.69      |\n",
      "|    reward             | 0.9851194 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 0.22      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 18        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 10.3      |\n",
      "|    reward             | 0.4408912 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 2.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -19       |\n",
      "|    reward             | 1.6813731 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 7.89      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 1500      |\n",
      "|    time_elapsed       | 21        |\n",
      "|    total_timesteps    | 7500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1499      |\n",
      "|    policy_loss        | -12.7     |\n",
      "|    reward             | 1.0471847 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.42      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 356      |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.11    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -14.3    |\n",
      "|    reward             | 1.270083 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.12     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 13.9      |\n",
      "|    reward             | 2.082568  |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.37      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 356        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 0.816      |\n",
      "|    reward             | 0.14531446 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.181      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 358         |\n",
      "|    iterations         | 1900        |\n",
      "|    time_elapsed       | 26          |\n",
      "|    total_timesteps    | 9500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.08       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1899        |\n",
      "|    policy_loss        | -3.81       |\n",
      "|    reward             | -0.20278363 |\n",
      "|    std                | 0.998       |\n",
      "|    value_loss         | 0.516       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 357        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 4.75       |\n",
      "|    reward             | -0.4478488 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.51       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2021-07-06 to  2021-10-04\n",
      "A2C Sharpe Ratio:  -0.05496174988051257\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1071_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 477         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.05787062 |\n",
      "------------------------------------\n",
      "day: 2894, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 524464.84\n",
      "total_reward: -475535.16\n",
      "total_cost: 1198806.71\n",
      "total_trades: 11052\n",
      "Sharpe: -0.248\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 461          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 8            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0025991078 |\n",
      "|    clip_fraction        | 0.0263       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.00288     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.91         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0014      |\n",
      "|    reward               | -1.3878872   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.76         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 454         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009032472 |\n",
      "|    clip_fraction        | 0.0657      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.00503     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.35        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0058     |\n",
      "|    reward               | 0.42646492  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.46        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 440          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 18           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050821416 |\n",
      "|    clip_fraction        | 0.042        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.0171       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.96         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00367     |\n",
      "|    reward               | -0.05102698  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.34         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 426          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069664293 |\n",
      "|    clip_fraction        | 0.075        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.00842      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.54         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00388     |\n",
      "|    reward               | 0.38209313   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.58         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-07-06 to  2021-10-04\n",
      "PPO Sharpe Ratio:  -0.1118684953252714\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1071_15\n",
      "day: 2894, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3999333.25\n",
      "total_reward: 2999333.25\n",
      "total_cost: 1008.85\n",
      "total_trades: 5790\n",
      "Sharpe: 0.783\n",
      "=================================\n",
      "======DDPG Validation from:  2021-07-06 to  2021-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2021-10-04\n",
      "======Trading from:  2021-10-04 to  2022-01-03\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2021-10-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1134_15\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 235          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 2            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.13        |\n",
      "|    explained_variance | 0.105        |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -13          |\n",
      "|    reward             | 0.0033979178 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 2.79         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 264        |\n",
      "|    iterations         | 200        |\n",
      "|    time_elapsed       | 3          |\n",
      "|    total_timesteps    | 1000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -0.112     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 199        |\n",
      "|    policy_loss        | -1.31      |\n",
      "|    reward             | 0.72462803 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.853      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 281        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 5          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -6.05      |\n",
      "|    reward             | -1.6038661 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.14       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 290         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | -0.335      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -8.71       |\n",
      "|    reward             | -0.07104404 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 1.82        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 298        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -9.06      |\n",
      "|    reward             | -0.5619717 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 2.55       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 299       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.11     |\n",
      "|    explained_variance | -3.95     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 0.291     |\n",
      "|    reward             | 1.6668373 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.04      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 304        |\n",
      "|    iterations         | 700        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 3500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 699        |\n",
      "|    policy_loss        | 3.42       |\n",
      "|    reward             | 0.40377358 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.48       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 307         |\n",
      "|    iterations         | 800         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 4000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.11       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 799         |\n",
      "|    policy_loss        | 9.16        |\n",
      "|    reward             | -0.21812314 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 2.92        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 310       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.1      |\n",
      "|    explained_variance | 0.00519   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | -26.1     |\n",
      "|    reward             | 0.2190297 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 18.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 312        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -44.2      |\n",
      "|    reward             | 0.57041425 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 78.3       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 314       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -12.1     |\n",
      "|    reward             | 1.7879429 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 315        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | -0.69      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -10.4      |\n",
      "|    reward             | -1.3450073 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.44       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 317        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -0.0109    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -2.76      |\n",
      "|    reward             | 0.01964594 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.382      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 319        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.13      |\n",
      "|    explained_variance | -0.0202    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 4.66       |\n",
      "|    reward             | -0.8782878 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.758      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 321        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 6.73       |\n",
      "|    reward             | 0.11169719 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 1.19       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 24        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | -0.0749   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | -2.71     |\n",
      "|    reward             | 0.5260697 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 0.852     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 322      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.14    |\n",
      "|    explained_variance | 0.00734  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -9.91    |\n",
      "|    reward             | 1.364022 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.86     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 323       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 27        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -0.011    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -11.6     |\n",
      "|    reward             | 0.6920411 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 3.93      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 324        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.11      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 10.3       |\n",
      "|    reward             | 0.99357617 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.56       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 325       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | 9.84      |\n",
      "|    reward             | 0.5723184 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 1.95      |\n",
      "-------------------------------------\n",
      "======A2C Validation from:  2021-10-04 to  2022-01-03\n",
      "A2C Sharpe Ratio:  0.2153184050927763\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1134_15\n",
      "-------------------------------------\n",
      "| time/              |              |\n",
      "|    fps             | 467          |\n",
      "|    iterations      | 1            |\n",
      "|    time_elapsed    | 4            |\n",
      "|    total_timesteps | 2048         |\n",
      "| train/             |              |\n",
      "|    reward          | -0.039562177 |\n",
      "-------------------------------------\n",
      "day: 2957, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 597782.46\n",
      "total_reward: -402217.54\n",
      "total_cost: 1226778.69\n",
      "total_trades: 11191\n",
      "Sharpe: -0.172\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 432          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060444726 |\n",
      "|    clip_fraction        | 0.0799       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | -0.0361      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.22         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00524     |\n",
      "|    reward               | 0.14440364   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.61         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 14           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0081193475 |\n",
      "|    clip_fraction        | 0.0611       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.11        |\n",
      "|    explained_variance   | -0.0179      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.07         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00843     |\n",
      "|    reward               | -0.26100022  |\n",
      "|    std                  | 1.01         |\n",
      "|    value_loss           | 3.9          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 397         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009513529 |\n",
      "|    clip_fraction        | 0.0891      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.00299     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.25        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00961    |\n",
      "|    reward               | 0.8585645   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 3.46        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 384          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063711214 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | -0.0125      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.65         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00505     |\n",
      "|    reward               | -0.1160165   |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.66         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2021-10-04 to  2022-01-03\n",
      "PPO Sharpe Ratio:  0.2374533602560885\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1134_15\n",
      "day: 2957, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3517593.73\n",
      "total_reward: 2517593.73\n",
      "total_cost: 998.53\n",
      "total_trades: 2957\n",
      "Sharpe: 0.588\n",
      "=================================\n",
      "======DDPG Validation from:  2021-10-04 to  2022-01-03\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-01-03\n",
      "======Trading from:  2022-01-03 to  2022-04-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-01-03\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1197_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 313        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.14      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -12.6      |\n",
      "|    reward             | 0.21371345 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 4.16       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 323      |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.15    |\n",
      "|    explained_variance | -0.426   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.64     |\n",
      "|    reward             | 1.512884 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 2.34     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 327       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -4.33     |\n",
      "|    reward             | -2.540487 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 2.65      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 329         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 6           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.13       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -19.6       |\n",
      "|    reward             | -0.34599742 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 10.6        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 330        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.12      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -14.8      |\n",
      "|    reward             | -1.1672055 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 9.89       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 333      |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.15    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 1.05     |\n",
      "|    reward             | 8.374154 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 3.59     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 333         |\n",
      "|    iterations         | 700         |\n",
      "|    time_elapsed       | 10          |\n",
      "|    total_timesteps    | 3500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 699         |\n",
      "|    policy_loss        | -17.9       |\n",
      "|    reward             | -0.24902551 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 8.21        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 332       |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.13     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | 16.2      |\n",
      "|    reward             | 1.9082797 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.18      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 332         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 13          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.12       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -14.1       |\n",
      "|    reward             | 0.061836816 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 4.98        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 331        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 6.07       |\n",
      "|    reward             | -0.2678692 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 0.912      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 333       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | -1.61     |\n",
      "|    reward             | 2.0424256 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 1.72      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 334       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | 7.6       |\n",
      "|    reward             | 1.5539659 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 2.1       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 336       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -4.7      |\n",
      "|    reward             | 1.5293922 |\n",
      "|    std                | 0.991     |\n",
      "|    value_loss         | 0.972     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 335        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 20         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 12.8       |\n",
      "|    reward             | -0.5569128 |\n",
      "|    std                | 0.993      |\n",
      "|    value_loss         | 6.25       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 336        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 10.1       |\n",
      "|    reward             | 0.13859841 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 3.61       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 336         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 23          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.05       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | 15.9        |\n",
      "|    reward             | -0.30427548 |\n",
      "|    std                | 0.992       |\n",
      "|    value_loss         | 4.38        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 335       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -53.5     |\n",
      "|    reward             | 1.7835853 |\n",
      "|    std                | 0.994     |\n",
      "|    value_loss         | 56        |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 336        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 27         |\n",
      "|    reward             | -3.6227293 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 29.9       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 337       |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 28        |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -0.478    |\n",
      "|    reward             | 1.3234265 |\n",
      "|    std                | 0.995     |\n",
      "|    value_loss         | 1.49      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 337          |\n",
      "|    iterations         | 2000         |\n",
      "|    time_elapsed       | 29           |\n",
      "|    total_timesteps    | 10000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1999         |\n",
      "|    policy_loss        | 5.27         |\n",
      "|    reward             | -0.035901368 |\n",
      "|    std                | 0.993        |\n",
      "|    value_loss         | 0.561        |\n",
      "----------------------------------------\n",
      "======A2C Validation from:  2022-01-03 to  2022-04-04\n",
      "A2C Sharpe Ratio:  -0.1890206381026521\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1197_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 456         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.20517132 |\n",
      "------------------------------------\n",
      "day: 3020, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 522298.13\n",
      "total_reward: -477701.87\n",
      "total_cost: 1295764.60\n",
      "total_trades: 11508\n",
      "Sharpe: -0.228\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 422          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063662212 |\n",
      "|    clip_fraction        | 0.0626       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.09        |\n",
      "|    explained_variance   | -0.131       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.83         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00656     |\n",
      "|    reward               | -1.1842288   |\n",
      "|    std                  | 0.997        |\n",
      "|    value_loss           | 3.79         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 399          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055565755 |\n",
      "|    clip_fraction        | 0.0665       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.08        |\n",
      "|    explained_variance   | 0.0297       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.71         |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0059      |\n",
      "|    reward               | -1.533444    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.72         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 386         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008915275 |\n",
      "|    clip_fraction        | 0.0965      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | 0.0823      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.21        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00935    |\n",
      "|    reward               | -0.1688582  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.81        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 383        |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 26         |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00689122 |\n",
      "|    clip_fraction        | 0.0579     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -7.11      |\n",
      "|    explained_variance   | -0.0097    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 2.4        |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.00206   |\n",
      "|    reward               | -0.5732045 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 4.48       |\n",
      "----------------------------------------\n",
      "======PPO Validation from:  2022-01-03 to  2022-04-04\n",
      "PPO Sharpe Ratio:  -0.3225616563854718\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1197_15\n",
      "day: 3020, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 6774979.99\n",
      "total_reward: 5774979.99\n",
      "total_cost: 998.29\n",
      "total_trades: 3020\n",
      "Sharpe: 0.923\n",
      "=================================\n",
      "======DDPG Validation from:  2022-01-03 to  2022-04-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-04-04\n",
      "======Trading from:  2022-04-04 to  2022-07-06\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-04-04\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1260_15\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 353        |\n",
      "|    iterations         | 100        |\n",
      "|    time_elapsed       | 1          |\n",
      "|    total_timesteps    | 500        |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | 0.0296     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 99         |\n",
      "|    policy_loss        | -9.92      |\n",
      "|    reward             | 0.08002199 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 2.27       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 347       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.07     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | -7.55     |\n",
      "|    reward             | 1.1520501 |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 2.48      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 350        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 0.0448     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -10.7      |\n",
      "|    reward             | -1.2929486 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.74       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 350         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.1        |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -16.5       |\n",
      "|    reward             | -0.35153723 |\n",
      "|    std                | 1           |\n",
      "|    value_loss         | 4.87        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 350        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -7.38      |\n",
      "|    reward             | -0.4182027 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 3.33       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 349       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -0.0351   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -11.6     |\n",
      "|    reward             | 2.2805505 |\n",
      "|    std                | 0.998     |\n",
      "|    value_loss         | 4.58      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 349       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -48.5     |\n",
      "|    reward             | 2.6290753 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 53.7      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 350        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | -20.1      |\n",
      "|    reward             | 0.77593577 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 8.89       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 353        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 12         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.1       |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | -10.2      |\n",
      "|    reward             | -1.7707286 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 5.72       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.09     |\n",
      "|    explained_variance | -0.538    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 6.74      |\n",
      "|    reward             | 3.5643387 |\n",
      "|    std                | 0.999     |\n",
      "|    value_loss         | 1.35      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 353        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0.0576     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -64.6      |\n",
      "|    reward             | -3.3887348 |\n",
      "|    std                | 0.999      |\n",
      "|    value_loss         | 108        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 352        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 17         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.09      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 5.93       |\n",
      "|    reward             | 0.81181294 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 15.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 354        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.08      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 14.3       |\n",
      "|    reward             | 0.11475135 |\n",
      "|    std                | 0.998      |\n",
      "|    value_loss         | 3.39       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 353       |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.08     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | -29.6     |\n",
      "|    reward             | 0.8807258 |\n",
      "|    std                | 0.997     |\n",
      "|    value_loss         | 20.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 354        |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 21         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.06      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | 0.943      |\n",
      "|    reward             | 0.84608996 |\n",
      "|    std                | 0.994      |\n",
      "|    value_loss         | 1.44       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 356        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.07      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | -7.93      |\n",
      "|    reward             | -1.3335359 |\n",
      "|    std                | 0.995      |\n",
      "|    value_loss         | 2.13       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 356       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 23        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 40.1      |\n",
      "|    reward             | 2.3210452 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 45.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 355       |\n",
      "|    iterations         | 1800      |\n",
      "|    time_elapsed       | 25        |\n",
      "|    total_timesteps    | 9000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.06     |\n",
      "|    explained_variance | -0.000855 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1799      |\n",
      "|    policy_loss        | -23.6     |\n",
      "|    reward             | -0.674049 |\n",
      "|    std                | 0.993     |\n",
      "|    value_loss         | 41.7      |\n",
      "-------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 355          |\n",
      "|    iterations         | 1900         |\n",
      "|    time_elapsed       | 26           |\n",
      "|    total_timesteps    | 9500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | -0.268       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 1899         |\n",
      "|    policy_loss        | 1.45         |\n",
      "|    reward             | -0.073048525 |\n",
      "|    std                | 0.993        |\n",
      "|    value_loss         | 0.126        |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 355        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | -9.69      |\n",
      "|    reward             | -1.1252495 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 2.44       |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-04-04 to  2022-07-06\n",
      "A2C Sharpe Ratio:  -0.34707183758455223\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1260_15\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 469        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 4          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.40109316 |\n",
      "-----------------------------------\n",
      "day: 3083, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 470394.35\n",
      "total_reward: -529605.65\n",
      "total_cost: 1321936.46\n",
      "total_trades: 11782\n",
      "Sharpe: -0.276\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 415         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008804841 |\n",
      "|    clip_fraction        | 0.0778      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.1        |\n",
      "|    explained_variance   | -0.0272     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 2.06        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00746    |\n",
      "|    reward               | 0.38102624  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.73        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 400         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.00402693  |\n",
      "|    clip_fraction        | 0.0343      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | 0.0164      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.41        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00405    |\n",
      "|    reward               | -0.68230414 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.9         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 389         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010977866 |\n",
      "|    clip_fraction        | 0.0883      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | 0.00296     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.1         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    reward               | -0.30964372 |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.45        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 383          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076050935 |\n",
      "|    clip_fraction        | 0.0689       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.12        |\n",
      "|    explained_variance   | 0.0156       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 1.76         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00535     |\n",
      "|    reward               | -0.40502572  |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.84         |\n",
      "------------------------------------------\n",
      "======PPO Validation from:  2022-04-04 to  2022-07-06\n",
      "PPO Sharpe Ratio:  -0.38787019925633853\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1260_15\n",
      "day: 3083, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3084103.00\n",
      "total_reward: 2084103.00\n",
      "total_cost: 1127.09\n",
      "total_trades: 9253\n",
      "Sharpe: 0.612\n",
      "=================================\n",
      "======DDPG Validation from:  2022-04-04 to  2022-07-06\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-07-06\n",
      "======Trading from:  2022-07-06 to  2022-10-04\n",
      "============================================\n",
      "turbulence_threshold:  27.90730730085542\n",
      "======Model training from:  2010-01-01 to  2022-07-06\n",
      "======A2C Training========\n",
      "{'n_steps': 5, 'ent_coef': 0.005, 'learning_rate': 0.0007}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/a2c/a2c_1323_15\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 341          |\n",
      "|    iterations         | 100          |\n",
      "|    time_elapsed       | 1            |\n",
      "|    total_timesteps    | 500          |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -7.06        |\n",
      "|    explained_variance | -0.436       |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 99           |\n",
      "|    policy_loss        | -9.36        |\n",
      "|    reward             | -0.070905276 |\n",
      "|    std                | 0.994        |\n",
      "|    value_loss         | 2.46         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 2         |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 0.175     |\n",
      "|    reward             | 0.5920892 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 0.836     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 340        |\n",
      "|    iterations         | 300        |\n",
      "|    time_elapsed       | 4          |\n",
      "|    total_timesteps    | 1500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 299        |\n",
      "|    policy_loss        | -5.54      |\n",
      "|    reward             | -1.6784796 |\n",
      "|    std                | 0.99       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 341         |\n",
      "|    iterations         | 400         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 2000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 399         |\n",
      "|    policy_loss        | -12.3       |\n",
      "|    reward             | -0.22690092 |\n",
      "|    std                | 0.989       |\n",
      "|    value_loss         | 3.43        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 338         |\n",
      "|    iterations         | 500         |\n",
      "|    time_elapsed       | 7           |\n",
      "|    total_timesteps    | 2500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.02       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 499         |\n",
      "|    policy_loss        | -7.26       |\n",
      "|    reward             | -0.36966944 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 2.96        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 342       |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 8         |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -10.7     |\n",
      "|    reward             | 4.8060617 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 5.73      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.02     |\n",
      "|    explained_variance | 0.00179   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -1.98     |\n",
      "|    reward             | 0.8130942 |\n",
      "|    std                | 0.986     |\n",
      "|    value_loss         | 0.984     |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 345      |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.05    |\n",
      "|    explained_variance | 5.96e-08 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 17.5     |\n",
      "|    reward             | 0.0      |\n",
      "|    std                | 0.991    |\n",
      "|    value_loss         | 4.57     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 349       |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 12        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.03     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 0.81      |\n",
      "|    reward             | 0.2779757 |\n",
      "|    std                | 0.988     |\n",
      "|    value_loss         | 0.767     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 352       |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 14        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -7.05     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -3.26     |\n",
      "|    reward             | 1.3594322 |\n",
      "|    std                | 0.992     |\n",
      "|    value_loss         | 0.731     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 350        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.05      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | 22         |\n",
      "|    reward             | 0.42666844 |\n",
      "|    std                | 0.991      |\n",
      "|    value_loss         | 12.8       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 351         |\n",
      "|    iterations         | 1200        |\n",
      "|    time_elapsed       | 17          |\n",
      "|    total_timesteps    | 6000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.04       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1199        |\n",
      "|    policy_loss        | 38          |\n",
      "|    reward             | -0.96948105 |\n",
      "|    std                | 0.991       |\n",
      "|    value_loss         | 36.4        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 351        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 18         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | -4.37      |\n",
      "|    reward             | -0.4060149 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 0.959      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 351        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 19         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.04      |\n",
      "|    explained_variance | -0.0357    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | -10.5      |\n",
      "|    reward             | -2.3962173 |\n",
      "|    std                | 0.989      |\n",
      "|    value_loss         | 4.07       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 352      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -7.04    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -22.2    |\n",
      "|    reward             | 2.698733 |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 15.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 353        |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.03      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 0.052      |\n",
      "|    reward             | 0.15874343 |\n",
      "|    std                | 0.987      |\n",
      "|    value_loss         | 0.728      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 352         |\n",
      "|    iterations         | 1700        |\n",
      "|    time_elapsed       | 24          |\n",
      "|    total_timesteps    | 8500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -7.01       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1699        |\n",
      "|    policy_loss        | -7.82       |\n",
      "|    reward             | -0.26749364 |\n",
      "|    std                | 0.985       |\n",
      "|    value_loss         | 2.19        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 353        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.02      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 1.07       |\n",
      "|    reward             | -1.7167257 |\n",
      "|    std                | 0.986      |\n",
      "|    value_loss         | 13.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 353        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 26         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 6.51       |\n",
      "|    reward             | 0.15963797 |\n",
      "|    std                | 0.984      |\n",
      "|    value_loss         | 1.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 354        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 28         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -7.01      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 15.7       |\n",
      "|    reward             | -1.2549322 |\n",
      "|    std                | 0.985      |\n",
      "|    value_loss         | 3.9        |\n",
      "--------------------------------------\n",
      "======A2C Validation from:  2022-07-06 to  2022-10-04\n",
      "A2C Sharpe Ratio:  -0.08985867545054008\n",
      "======PPO Training========\n",
      "{'ent_coef': 0.01, 'n_steps': 2048, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ppo/ppo_1323_15\n",
      "------------------------------------\n",
      "| time/              |             |\n",
      "|    fps             | 423         |\n",
      "|    iterations      | 1           |\n",
      "|    time_elapsed    | 4           |\n",
      "|    total_timesteps | 2048        |\n",
      "| train/             |             |\n",
      "|    reward          | -0.22161411 |\n",
      "------------------------------------\n",
      "day: 3146, episode: 5\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 365356.49\n",
      "total_reward: -634643.51\n",
      "total_cost: 1357090.47\n",
      "total_trades: 11927\n",
      "Sharpe: -0.376\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 397         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005735909 |\n",
      "|    clip_fraction        | 0.0752      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.09       |\n",
      "|    explained_variance   | -0.0136     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.64        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    reward               | 1.991421    |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.6         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 389          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 15           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005697256  |\n",
      "|    clip_fraction        | 0.0497       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -7.1         |\n",
      "|    explained_variance   | 0.00345      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 2.3          |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00482     |\n",
      "|    reward               | -0.083425626 |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.67         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 384         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008513292 |\n",
      "|    clip_fraction        | 0.0728      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.11       |\n",
      "|    explained_variance   | -0.00176    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.96        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00585    |\n",
      "|    reward               | 0.47501427  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.54        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 383         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006349286 |\n",
      "|    clip_fraction        | 0.0625      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -7.12       |\n",
      "|    explained_variance   | -0.043      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.67        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00675    |\n",
      "|    reward               | 0.5377794   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 4.01        |\n",
      "-----------------------------------------\n",
      "======PPO Validation from:  2022-07-06 to  2022-10-04\n",
      "PPO Sharpe Ratio:  -0.017263540890070317\n",
      "======DDPG Training========\n",
      "{'buffer_size': 10000, 'learning_rate': 0.0005, 'batch_size': 64}\n",
      "Using cuda device\n",
      "Logging to tensorboard_log/ddpg/ddpg_1323_15\n",
      "day: 3146, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1971953.24\n",
      "total_reward: 971953.24\n",
      "total_cost: 1467.93\n",
      "total_trades: 3150\n",
      "Sharpe: 0.399\n",
      "=================================\n",
      "======DDPG Validation from:  2022-07-06 to  2022-10-04\n",
      "======Best Model Retraining from:  2010-01-01 to  2022-10-04\n",
      "======Trading from:  2022-10-04 to  2023-01-04\n",
      "Ensemble Strategy took:  39.7498540242513  minutes\n"
     ]
    }
   ],
   "source": [
    "df_summary = ensemble_agent.run_ensemble_strategy(A2C_model_kwargs,\n",
    "                                                 PPO_model_kwargs,\n",
    "                                                 DDPG_model_kwargs,\n",
    "                                                 timesteps_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "-0qd8acMtj1f",
    "outputId": "9f0cbf89-5f4b-4691-9e43-daa093ebceae"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iter</th>\n",
       "      <th>Val Start</th>\n",
       "      <th>Val End</th>\n",
       "      <th>Model Used</th>\n",
       "      <th>A2C Sharpe</th>\n",
       "      <th>PPO Sharpe</th>\n",
       "      <th>DDPG Sharpe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126</td>\n",
       "      <td>2017-10-02</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.210315</td>\n",
       "      <td>0.088616</td>\n",
       "      <td>0.64196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>189</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.095046</td>\n",
       "      <td>-0.287441</td>\n",
       "      <td>-0.012245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>252</td>\n",
       "      <td>2018-04-04</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.266735</td>\n",
       "      <td>-0.122839</td>\n",
       "      <td>0.074028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>315</td>\n",
       "      <td>2018-07-03</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.607394</td>\n",
       "      <td>-0.037956</td>\n",
       "      <td>0.505944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>378</td>\n",
       "      <td>2018-10-02</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.320627</td>\n",
       "      <td>-0.389916</td>\n",
       "      <td>-0.362581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>441</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.626717</td>\n",
       "      <td>0.524306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>504</td>\n",
       "      <td>2019-04-04</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.140264</td>\n",
       "      <td>0.03521</td>\n",
       "      <td>-0.000011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>567</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.146083</td>\n",
       "      <td>-0.149158</td>\n",
       "      <td>-0.170053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>630</td>\n",
       "      <td>2019-10-03</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.563493</td>\n",
       "      <td>0.440302</td>\n",
       "      <td>0.668005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>693</td>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>A2C</td>\n",
       "      <td>-0.033253</td>\n",
       "      <td>-0.158705</td>\n",
       "      <td>-0.096929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>756</td>\n",
       "      <td>2020-04-03</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.41327</td>\n",
       "      <td>0.265299</td>\n",
       "      <td>0.504677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>819</td>\n",
       "      <td>2020-07-06</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.118915</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.204729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>882</td>\n",
       "      <td>2020-10-02</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.50771</td>\n",
       "      <td>0.313352</td>\n",
       "      <td>0.415908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>945</td>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>A2C</td>\n",
       "      <td>0.188601</td>\n",
       "      <td>0.070281</td>\n",
       "      <td>0.175926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1008</td>\n",
       "      <td>2021-04-06</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>0.229912</td>\n",
       "      <td>-0.034405</td>\n",
       "      <td>0.243034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1071</td>\n",
       "      <td>2021-07-06</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.054962</td>\n",
       "      <td>-0.111868</td>\n",
       "      <td>-0.022333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1134</td>\n",
       "      <td>2021-10-04</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>PPO</td>\n",
       "      <td>0.215318</td>\n",
       "      <td>0.237453</td>\n",
       "      <td>0.029115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1197</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.189021</td>\n",
       "      <td>-0.322562</td>\n",
       "      <td>-0.183984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1260</td>\n",
       "      <td>2022-04-04</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>DDPG</td>\n",
       "      <td>-0.347072</td>\n",
       "      <td>-0.38787</td>\n",
       "      <td>-0.312772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1323</td>\n",
       "      <td>2022-07-06</td>\n",
       "      <td>2022-10-04</td>\n",
       "      <td>PPO</td>\n",
       "      <td>-0.089859</td>\n",
       "      <td>-0.017264</td>\n",
       "      <td>-0.102126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Iter   Val Start     Val End Model Used A2C Sharpe PPO Sharpe DDPG Sharpe\n",
       "0    126  2017-10-02  2018-01-02       DDPG   0.210315   0.088616     0.64196\n",
       "1    189  2018-01-02  2018-04-04       DDPG  -0.095046  -0.287441   -0.012245\n",
       "2    252  2018-04-04  2018-07-03        A2C   0.266735  -0.122839    0.074028\n",
       "3    315  2018-07-03  2018-10-02        A2C   0.607394  -0.037956    0.505944\n",
       "4    378  2018-10-02  2019-01-03        A2C  -0.320627  -0.389916   -0.362581\n",
       "5    441  2019-01-03  2019-04-04        A2C   0.626717   0.524306         0.0\n",
       "6    504  2019-04-04  2019-07-05        A2C   0.140264    0.03521   -0.000011\n",
       "7    567  2019-07-05  2019-10-03        A2C  -0.146083  -0.149158   -0.170053\n",
       "8    630  2019-10-03  2020-01-03       DDPG   0.563493   0.440302    0.668005\n",
       "9    693  2020-01-03  2020-04-03        A2C  -0.033253  -0.158705   -0.096929\n",
       "10   756  2020-04-03  2020-07-06       DDPG    0.41327   0.265299    0.504677\n",
       "11   819  2020-07-06  2020-10-02       DDPG   0.118915    0.08606    0.204729\n",
       "12   882  2020-10-02  2021-01-04        A2C    0.50771   0.313352    0.415908\n",
       "13   945  2021-01-04  2021-04-06        A2C   0.188601   0.070281    0.175926\n",
       "14  1008  2021-04-06  2021-07-06       DDPG   0.229912  -0.034405    0.243034\n",
       "15  1071  2021-07-06  2021-10-04       DDPG  -0.054962  -0.111868   -0.022333\n",
       "16  1134  2021-10-04  2022-01-03        PPO   0.215318   0.237453    0.029115\n",
       "17  1197  2022-01-03  2022-04-04       DDPG  -0.189021  -0.322562   -0.183984\n",
       "18  1260  2022-04-04  2022-07-06       DDPG  -0.347072   -0.38787   -0.312772\n",
       "19  1323  2022-07-06  2022-10-04        PPO  -0.089859  -0.017264   -0.102126"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6vvNSC6h1jZ"
   },
   "source": [
    "<a id='6'></a>\n",
    "# Part 7: Backtest Our Strategy\n",
    "Backtesting plays a key role in evaluating the performance of a trading strategy. Automated backtesting tool is preferred because it reduces the human error. We usually use the Quantopian pyfolio package to backtest our trading strategies. It is easy to use and consists of various individual plots that provide a comprehensive image of the performance of a trading strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "X4JKB--8tj1g"
   },
   "outputs": [],
   "source": [
    "unique_trade_date = processed[(processed.date > TEST_START_DATE)&(processed.date <= TEST_END_DATE)].date.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9mKF7GGtj1g",
    "outputId": "99c5e5f8-2e3f-49c3-e5a6-4e66ed92e40a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sharpe Ratio:  0.4023496449234007\n"
     ]
    }
   ],
   "source": [
    "df_trade_date = pd.DataFrame({'datadate':unique_trade_date})\n",
    "\n",
    "df_account_value=pd.DataFrame()\n",
    "for i in range(rebalance_window+validation_window, len(unique_trade_date)+1,rebalance_window):\n",
    "    temp = pd.read_csv('results/account_value_trade_{}_{}.csv'.format('ensemble',i))\n",
    "    df_account_value = pd.concat([df_account_value,temp],ignore_index=True)\n",
    "sharpe=(252**0.5)*df_account_value.account_value.pct_change(1).mean()/df_account_value.account_value.pct_change(1).std()\n",
    "print('Sharpe Ratio: ',sharpe)\n",
    "df_account_value=df_account_value.join(df_trade_date[validation_window:].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "oyosyW7_tj1g",
    "outputId": "0e54f2d5-6057-4a14-c94a-5f2af26ad171"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_value</th>\n",
       "      <th>date</th>\n",
       "      <th>daily_return</th>\n",
       "      <th>datadate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e+06</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.003103e+06</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>2018-01-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.008013e+06</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>2018-01-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.013972e+06</td>\n",
       "      <td>2018-01-05</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>2018-01-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.014498e+06</td>\n",
       "      <td>2018-01-08</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>2018-01-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_value        date  daily_return    datadate\n",
       "0   1.000000e+06  2018-01-02           NaN  2018-01-02\n",
       "1   1.003103e+06  2018-01-03      0.003103  2018-01-03\n",
       "2   1.008013e+06  2018-01-04      0.004895  2018-01-04\n",
       "3   1.013972e+06  2018-01-05      0.005912  2018-01-05\n",
       "4   1.014498e+06  2018-01-08      0.000519  2018-01-08"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_account_value.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "wLsRdw2Ctj1h",
    "outputId": "0e2b0bc2-840c-47fd-87d4-01201d8e4e3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "df_account_value.account_value.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lr2zX7ZxNyFQ"
   },
   "source": [
    "<a id='6.1'></a>\n",
    "## 7.1 BackTestStats\n",
    "pass in df_account_value, this information is stored in env class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Nzkr9yv-AdV_",
    "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Backtest Results===========\n",
      "Annual return          0.067668\n",
      "Cumulative returns     0.387332\n",
      "Annual volatility      0.224032\n",
      "Sharpe ratio           0.402350\n",
      "Calmar ratio           0.193273\n",
      "Stability              0.624233\n",
      "Max drawdown          -0.350115\n",
      "Omega ratio            1.081293\n",
      "Sortino ratio          0.611984\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.898478\n",
      "Daily value at risk   -0.027868\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"==============Get Backtest Results===========\")\n",
    "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
    "\n",
    "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
    "perf_stats_all = pd.DataFrame(perf_stats_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DiHhM1YkoCel",
    "outputId": "c233f613-67a3-4882-8710-c1839247590e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============Get Baseline Stats===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of DataFrame:  (1259, 8)\n",
      "Annual return          0.059583\n",
      "Cumulative returns     0.335290\n",
      "Annual volatility      0.217954\n",
      "Sharpe ratio           0.375375\n",
      "Calmar ratio           0.160661\n",
      "Stability              0.685042\n",
      "Max drawdown          -0.370862\n",
      "Omega ratio            1.077914\n",
      "Sortino ratio          0.516270\n",
      "Skew                        NaN\n",
      "Kurtosis                    NaN\n",
      "Tail ratio             0.893140\n",
      "Daily value at risk   -0.027135\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline stats\n",
    "print(\"==============Get Baseline Stats===========\")\n",
    "df_dji_ = get_baseline(\n",
    "        ticker=\"^DJI\", \n",
    "        start = df_account_value.loc[0,'date'],\n",
    "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "\n",
    "stats = backtest_stats(df_dji_, value_col_name = 'close')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RhJ9whD75WTs",
    "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_dji:              date           dji\n",
      "0     2018-01-02  1.000000e+06\n",
      "1     2018-01-03  1.003975e+06\n",
      "2     2018-01-04  1.010116e+06\n",
      "3     2018-01-05  1.019008e+06\n",
      "4     2018-01-08  1.018490e+06\n",
      "...          ...           ...\n",
      "1255  2022-12-27  1.339089e+06\n",
      "1256  2022-12-28  1.324351e+06\n",
      "1257  2022-12-29  1.338253e+06\n",
      "1258  2022-12-30  1.335290e+06\n",
      "1259  2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n",
      "df_dji:                       dji\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.003975e+06\n",
      "2018-01-04  1.010116e+06\n",
      "2018-01-05  1.019008e+06\n",
      "2018-01-08  1.018490e+06\n",
      "...                  ...\n",
      "2022-12-27  1.339089e+06\n",
      "2022-12-28  1.324351e+06\n",
      "2022-12-29  1.338253e+06\n",
      "2022-12-30  1.335290e+06\n",
      "2023-01-03           NaN\n",
      "\n",
      "[1260 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "df_dji = pd.DataFrame()\n",
    "df_dji['date'] = df_account_value['date']\n",
    "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * env_kwargs[\"initial_amount\"]\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji.csv\")\n",
    "df_dji = df_dji.set_index(df_dji.columns[0])\n",
    "print(\"df_dji: \", df_dji)\n",
    "df_dji.to_csv(\"df_dji+.csv\")\n",
    "\n",
    "df_account_value.to_csv('df_account_value.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9U6Suru3h1jc"
   },
   "source": [
    "<a id='6.2'></a>\n",
    "## 7.2 BackTestPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "HggausPRoCem",
    "outputId": "615e8d79-f3d7-47e9-c886-3cd18e4535f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_result_ensemble.columns:  Index(['ensemble'], dtype='object')\n",
      "df_trade_date:          datadate\n",
      "0     2017-10-02\n",
      "1     2017-10-03\n",
      "2     2017-10-04\n",
      "3     2017-10-05\n",
      "4     2017-10-06\n",
      "...          ...\n",
      "1356  2023-02-22\n",
      "1357  2023-02-23\n",
      "1358  2023-02-24\n",
      "1359  2023-02-27\n",
      "1360  2023-02-28\n",
      "\n",
      "[1361 rows x 1 columns]\n",
      "df_result_ensemble:                  ensemble\n",
      "date                    \n",
      "2018-01-02  1.000000e+06\n",
      "2018-01-03  1.003103e+06\n",
      "2018-01-04  1.008013e+06\n",
      "2018-01-05  1.013972e+06\n",
      "2018-01-08  1.014498e+06\n",
      "...                  ...\n",
      "2022-12-27  1.386943e+06\n",
      "2022-12-28  1.367364e+06\n",
      "2022-12-29  1.397675e+06\n",
      "2022-12-30  1.390630e+06\n",
      "2023-01-03  1.387332e+06\n",
      "\n",
      "[1260 rows x 1 columns]\n",
      "==============Compare to DJIA===========\n",
      "result:                  ensemble           dji\n",
      "date                                  \n",
      "2018-01-02  1.000000e+06  1.000000e+06\n",
      "2018-01-03  1.003103e+06  1.003975e+06\n",
      "2018-01-04  1.008013e+06  1.010116e+06\n",
      "2018-01-05  1.013972e+06  1.019008e+06\n",
      "2018-01-08  1.014498e+06  1.018490e+06\n",
      "...                  ...           ...\n",
      "2022-12-27  1.386943e+06  1.339089e+06\n",
      "2022-12-28  1.367364e+06  1.324351e+06\n",
      "2022-12-29  1.397675e+06  1.338253e+06\n",
      "2022-12-30  1.390630e+06  1.335290e+06\n",
      "2023-01-03  1.387332e+06           NaN\n",
      "\n",
      "[1260 rows x 2 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# print(\"==============Compare to DJIA===========\")\n",
    "# %matplotlib inline\n",
    "# # S&P 500: ^GSPC\n",
    "# # Dow Jones Index: ^DJI\n",
    "# # NASDAQ 100: ^NDX\n",
    "# backtest_plot(df_account_value, \n",
    "#               baseline_ticker = '^DJI', \n",
    "#               baseline_start = df_account_value.loc[0,'date'],\n",
    "#               baseline_end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
    "df.to_csv(\"df.csv\")\n",
    "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], 'ensemble': df_account_value['account_value']})\n",
    "df_result_ensemble = df_result_ensemble.set_index('date')\n",
    "\n",
    "print(\"df_result_ensemble.columns: \", df_result_ensemble.columns)\n",
    "\n",
    "# df_result_ensemble.drop(df_result_ensemble.columns[0], axis = 1)\n",
    "print(\"df_trade_date: \", df_trade_date)\n",
    "# df_result_ensemble['date'] = df_trade_date['datadate']\n",
    "# df_result_ensemble['account_value'] = df_account_value['account_value']\n",
    "df_result_ensemble.to_csv(\"df_result_ensemble.csv\")\n",
    "print(\"df_result_ensemble: \", df_result_ensemble)\n",
    "print(\"==============Compare to DJIA===========\")\n",
    "result = pd.DataFrame()\n",
    "# result = pd.merge(result, df_result_ensemble, left_index=True, right_index=True)\n",
    "# result = pd.merge(result, df_dji, left_index=True, right_index=True)\n",
    "result = pd.merge(df_result_ensemble, df_dji, left_index=True, right_index=True)\n",
    "print(\"result: \", result)\n",
    "result.to_csv(\"result.csv\")\n",
    "result.columns = ['ensemble', 'dji']\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure();\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oBQx4bVQFi-a"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./pkl_results/ensemblePRED.pkl', 'wb') as file:\n",
    "    pickle.dump(df_result_ensemble, file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
